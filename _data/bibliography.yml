- key: Abad2018
  kind: inproceedings
  author:
  - Zahra Shakeri Hossein Abad
  - Oliver Karras
  - Kurt Schneider
  - Ken Barker
  - Mike Bauer
  title: Task Interruption in Software Development Projects
  booktitle: Proc. EASE'18
  year: 2018
  doi: 10.1145/3210459.3210471
  abstract: >
    Multitasking has always been an inherent part of software development and is
    known as the primary source of interruptions due to task switching in
    software development teams. Developing software involves a mix of analytical
    and creative work, and requires a significant load on brain functions, such
    as working memory and decision making. Thus, task switching in the context
    of software development imposes a cognitive load that causes software
    developers to lose focus and concentration while working thereby taking a
    toll on productivity. To investigate the disruptiveness of task switching
    and interruptions in software development projects, and to understand the
    reasons for and perceptions of the disruptiveness of task switching we used
    a mixed-methods approach including a longitudinal data analysis on 4,910
    recorded tasks of 17 professional software developers, and a survey of 132
    software developers. We found that, compared to task-specific factors
    (e.g. priority, level, and temporal stage), contextual factors such as
    interruption type (e.g. self/external), time of day, and task type and
    context are a more potent determinant of task switching disruptiveness in
    software development tasks. Furthermore, while most survey respondents
    believe external interruptions are more disruptive than self-interruptions,
    the results of our retrospective analysis reveals otherwise. We found that
    self-interruptions (i.e. voluntary task switchings) are more disruptive than
    external interruptions and have a negative effect on the performance of the
    interrupted tasks. Finally, we use the results of both studies to provide a
    set of comparative vulnerability and interaction patterns which can be used
    as a mean to guide decision-making and forecasting the consequences of task
    switching in software development teams.

- key: Abbate2012
  kind: book
  author:
  - Janet Abbate
  title: "Recoding Gender: Women's Changing Participation in Computing"
  year: 2012
  isbn: 978-0262534536
  publisher: MIT Press

- key: Abbes2011
  kind: inproceedings
  author:
  - Marwen Abbes
  - Foutse Khomh
  - Yann-Gael Gueheneuc
  - Giuliano Antoniol
  title: An Empirical Study of the Impact of Two Antipatterns, Blob and Spaghetti Code, on Program Comprehension
  booktitle: Proc. CSMR'11
  month: '3'
  year: 2011
  doi: 10.1109/csmr.2011.24
  publisher: IEEE
  url: https://doi.org/10.1109/csmr.2011.24
  abstract: >
    Antipatterns are "poor" solutions to recurring design problems which are
    conjectured in the literature to make object-oriented systems harder to
    maintain. However, little quantitative evidence exists to support this
    conjecture. We performed an empirical study to investigate whether the
    occurrence of antipatterns does indeed affect the understandability of
    systems by developers during comprehension and maintenance tasks. We
    designed and conducted three experiments, with 24 subjects each, to collect
    data on the performance of developers on basic tasks related to program
    comprehension and assessed the impact of two antipatterns and of their
    combinations: Blob and Spaghetti Code. We measured the developers’
    performance with: (1) the NASA task load index for their effort, (2) the
    time that they spent performing their tasks, and, (3) their percentages of
    correct answers. Collected data show that the occurrence of one antipattern
    does not significantly decrease developers’ performance while the
    combination of two antipatterns impedes significantly developers. We
    conclude that developers can cope with one antipattern but that combinations
    of antipatterns should be avoided possibly through detection and
    refactorings.

- key: Abdalkareem2017
  kind: inproceedings
  author:
  - Rabe Abdalkareem
  - Olivier Nourry
  - Sultan Wehaibi
  - Suhaib Mujahid
  - Emad Shihab
  title: Why do developers use trivial packages? an empirical case study on npm
  booktitle: Proc. ESEC/FSE'17
  year: 2017
  doi: 10.1145/3106237.3106267
  publisher: ACM Press
  url: https://doi.org/10.1145/3106237.3106267
  abstract: >
    Code reuse is traditionally seen as good practice. Recent trends have pushed
    the concept of code reuse to an extreme, by using packages that implement
    simple and trivial tasks, which we call `trivial packages'. A recent
    incident where a trivial package led to the breakdown of some of the most
    popular web applications such as Facebook and Netflix made it imperative to
    question the growing use of trivial packages. Therefore, in this paper, we
    mine more than 230,000 npm packages and 38,000 JavaScript applications in
    order to study the prevalence of trivial packages. We found that trivial
    packages are common and are increasing in popularity, making up 16.8% of the
    studied npm packages. We performed a survey with 88 Node.js developers who
    use trivial packages to understand the reasons and drawbacks of their
    use. Our survey revealed that trivial packages are used because they are
    perceived to be well implemented and tested pieces of code. However,
    developers are concerned about maintaining and the risks of breakages due to
    the extra dependencies trivial packages introduce. To objectively verify the
    survey results, we empirically validate the most cited reason and drawback
    and find that, contrary to developers' beliefs, only 45.2% of trivial
    packages even have tests. However, trivial packages appear to be `deployment
    tested' and to have similar test, usage and community interest as
    non-trivial packages. On the other hand, we found that 11.5% of the studied
    trivial packages have more than 20 dependencies. Hence, developers should be
    careful about which trivial packages they decide to use.

- key: Abdalkareem2020
  kind: article
  author:
  - Rabe Abdalkareem
  - Vinicius Oda
  - Suhaib Mujahid
  - Emad Shihab
  title: "On the impact of using trivial packages: an empirical case study on npm and PyPI"
  journal: ESE
  month: '1'
  year: 2020
  doi: 10.1007/s10664-019-09792-9
  publisher: Springer Science and Business Media LLC
  url: https://doi.org/10.1007/s10664-019-09792-9
  abstract: >
    Code reuse has traditionally been encouraged since it enables one to avoid
    re-inventing the wheel. Due to the npm left-pad package incident where a
    trivial package led to the breakdown of some of the most popular web
    applications such as Facebook and Netflix, some questioned such reuse. Reuse
    of trivial packages is particularly prevalent in platforms such as npm. To
    date, there is no study that examines the reason why developers reuse
    trivial packages other than in npm. Therefore, in this paper, we study two
    large platforms npm and PyPI. We mine more than 500,000 npm packages and
    38,000 JavaScript applications and more than 63,000 PyPI packages and 14,000
    Python applications to study the prevalence of trivial packages. We found
    that trivial packages are common, making up between 16.0% to 10.5% of the
    studied platforms. We performed surveys with 125 developers who use trivial
    packages to understand the reasons and drawbacks of their use. Our surveys
    revealed that trivial packages are used because they are perceived to be
    well implemented and tested pieces of code. However, developers are
    concerned about maintaining and the risks of breakages due to the extra
    dependencies trivial packages introduce. To objectively verify the survey
    results, we validate the most cited reason and drawback. We find that
    contrary to developers’ beliefs only around 28% of npm and 49% PyPI trivial
    packages have tests. However, trivial packages appear to be ‘deployment
    tested’ and to have similar test, usage and community interest as
    non-trivial packages. On the other hand, we found that 18.4% and 2.9% of the
    studied trivial packages have more than 20 dependencies in npm and PyPI,
    respectively.

- key: Afzal2018
  kind: inproceedings
  author:
  - Afsoon Afzal
  - Claire Le Goues
  title: A study on the use of IDE features for debugging
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196468
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196468
  abstract: >
    Integrated development environments (IDEs) provide features to help
    developers both create and understand code. As maintenance and bug repair
    are time-consuming and costly activities, IDEs have long integrated
    debugging features to simplify these tasks. In this paper we investigate the
    impact of using IDE debugger features on different aspects of programming
    and debugging. Using the data set provided by MSR challenge track, we
    compared debugging tasks performed with or without the IDE debugger. We
    find, on average, that developers spend more time and effort on debugging
    when they use the debugger. Typically, developers start using the debugger
    early, at the beginning of a debugging session, and that their editing
    behavior does not appear to significantly change when they are debugging
    regardless of whether debugging features are in use.

- key: Ahmed2017
  kind: inproceedings
  author:
  - Iftekhar Ahmed
  - Caius Brindescu
  - Umme Ayda Mannan
  - Carlos Jensen
  - Anita Sarma
  title: An Empirical Examination of the Relationship between Code Smells and Merge Conflicts
  booktitle: Proc. ESEM'17
  month: '11'
  year: 2017
  doi: 10.1109/esem.2017.12
  publisher: IEEE
  url: https://doi.org/10.1109/esem.2017.12
  abstract: >
    Background: Merge conflicts are a common occurrence in software
    development. Researchers have shown the negative impact of conflicts on the
    resulting code quality and the development workflow. Thus far, no one has
    investigated the effect of bad design (code smells) on merge
    conflicts. Aims: We posit that entities that exhibit certain types of code
    smells are more likely to be involved in a merge conflict. We also postulate
    that code elements that are both "smelly" and involved in a merge conflict
    are associated with other undesirable effects (more likely to be
    buggy). Method: We mined 143 repositories from GitHub and recreated 6,979
    merge conflicts to obtain metrics about code changes and conflicts. We
    categorized conflicts into semantic or non-semantic, based on whether
    changes affected the Abstract Syntax Tree. For each conflicting change, we
    calculate the number of code smells and the number of future bug-fixes
    associated with the affected lines of code. Results: We found that entities
    that are smelly are three times more likely to be involved in merge
    conflicts. Method-level code smells (Blob Operation and Internal
    Duplication) are highly correlated with semantic conflicts. We also found
    that code that is smelly and experiences merge conflicts is more likely to
    be buggy. Conclusion: Bad code design not only impacts maintainability, it
    also impacts the day to day operations of a project, such as merging
    contributions, and negatively impacts the quality of the resulting code. Our
    findings indicate that research is needed to identify better ways to support
    merge conflict resolution to minimize its effect on code quality.

- key: Aiken1975
  kind: article
  author:
  - Edwin G. Aiken
  - Gary S. Thomas
  - William A. Shennum
  title: "Memory for a Lecture: Effects of Notes, Lecture Rate, and Informational Density"
  journal: Journal of Educational Psychology
  volume: 67
  number: 3
  year: 1975
  doi: 10.1037/h0076613

- key: Aitchison2016
  kind: article
  author:
  - Laurence Aitchison
  - Nicola Corradi
  - Peter E. Latham
  title: Zipf's Law Arises Naturally When There Are Underlying, Unobserved Variables
  editor:
  - Olaf Sporns
  journal: PLoS Comp Bio
  volume: 12
  number: 12
  pages: e1005110
  month: '12'
  year: 2016
  doi: 10.1371/journal.pcbi.1005110
  publisher: Public Library of Science (PLoS)
  url: https://doi.org/10.1371/journal.pcbi.1005110
  abstract: >
    Zipf’s law, which states that the probability of an observation is inversely
    proportional to its rank, has been observed in many domains. While there are
    models that explain Zipf’s law in each of them, those explanations are
    typically domain specific. Recently, methods from statistical physics were
    used to show that a fairly broad class of models does provide a general
    explanation of Zipf’s law. This explanation rests on the observation that
    real world data is often generated from underlying causes, known as latent
    variables. Those latent variables mix together multiple models that do not
    obey Zipf’s law, giving a model that does. Here we extend that work both
    theoretically and empirically. Theoretically, we provide a far simpler and
    more intuitive explanation of Zipf’s law, which at the same time
    considerably extends the class of models to which this explanation can
    apply. Furthermore, we also give methods for verifying whether this
    explanation applies to a particular dataset. Empirically, these advances
    allowed us extend this explanation to important classes of data, including
    word frequencies (the first domain in which Zipf’s law was discovered), data
    with variable sequence length, and multi-neuron spiking activity.

- key: Akerblom2015
  kind: inproceedings
  author:
  - "Beatrice Åkerblom"
  - Tobias Wrigstad
  title: Measuring Polymorphism in Python Programs
  booktitle: Proc. DLS'15
  year: 2015
  doi: 10.1145/2816707.2816717
  abstract: >
    Following the increased popularity of dynamic languages and their increased
    use in critical software, there have been many proposals to retrofit static
    type system to these languages to improve possibilities to catch bugs and
    improve performance. A key question for any type system is whether the types
    should be structural, for more expressiveness, or nominal, to carry more
    meaning for the programmer. For retrofitted type systems, it seems the
    current trend is using structural types. This paper attempts to answer the
    question to what extent this extra expressiveness is needed, and how the
    possible polymorphism in dynamic code is used in practise. We study
    polymorphism in 36 real-world open source Python programs and approximate to
    what extent nominal and structural types could be used to type these
    programs. The study is based on collecting traces from multiple runs of the
    programs and analysing the polymorphic degrees of targets at more than 7
    million call-sites. Our results show that while polymorphism is used in all
    programs, the programs are to a great extent monomorphic. The polymorphism
    found is evenly distributed across libraries and program-specific code and
    occur both during program start-up and normal execution. Most programs
    contain a few ``megamorphic'' call-sites where receiver types vary
    widely. The non-monomorphic parts of the programs can to some extent be
    typed with nominal or structural types, but none of the approaches can type
    entire programs.

- key: Almeida2017
  kind: inproceedings
  author:
  - Daniel A. Almeida
  - Gail C. Murphy
  - Greg Wilson
  - Mike Hoye
  title: Do Software Developers Understand Open Source Licenses?
  booktitle: Proc. ICPC'17
  year: 2017
  doi: 10.1109/ICPC.2017.7
  abstract: >
    Software provided under open source licenses is widely used, from forming
    high-profile stand-alone applications (e.g., Mozilla Firefox) to being
    embedded in commercial offerings (e.g., network routers). Despite the high
    frequency of use of open source licenses, there has been little work about
    whether software developers understand the open source licenses they use. To
    our knowledge, only one survey has been conducted, which focused on which
    licenses developers choose and when they encounter problems with licensing
    open source software. To help fill the gap of whether or not developers
    understand the open source licenses they use, we conducted a survey that
    posed development scenarios involving three popular open source licenses
    (GNU GPL 3.0, GNU LGPL 3.0 and MPL 2.0) both alone and in combination. The
    375 respondents to the survey, who were largely developers, gave answers
    consistent with those of a legal expert's opinion in 62% of 42
    cases. Although developers clearly understood cases involving one license,
    they struggled when multiple licenses were involved. An analysis of the
    quantitative and qualitative results of the study indicate a need for tool
    support to help guide developers in understanding this critical information
    attached to software components.

- key: Altadmri2015
  kind: inproceedings
  author:
  - Amjad Altadmri
  - Neil C.C. Brown
  title: 37 Million Compilations
  booktitle: Proc. SIGCSE'15
  year: 2015
  doi: 10.1145/2676723.2677258
  publisher: ACM Press
  url: https://doi.org/10.1145/2676723.2677258
  abstract: >
    Previous investigations of student errors have typically focused on samples
    of hundreds of students at individual institutions. This work uses a year's
    worth of compilation events from over 250,000 students all over the world,
    taken from the large Blackbox data set. We analyze the frequency,
    time-to-fix, and spread of errors among users, showing how these factors
    inter-relate, in addition to their development over the course of the
    year. These results can inform the design of courses, textbooks and also
    tools to target the most frequent (or hardest to fix) errors.

- key: Amlekar2018
  kind: inproceedings
  author:
  - Rahul Amlekar
  - "Andrés Felipe Rincón Gamboa"
  - Keheliya Gallaba
  - Shane McIntosh
  title: Do software engineers use autocompletion features differently than other developers?
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196471
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196471
  abstract: >
    Autocomplete is a common workspace feature that is used to recommend code
    snippets as developers type in their IDEs. Users of autocomplete features no
    longer need to remember programming syntax and the names and details of the
    API methods that are needed to accomplish tasks. Moreover, autocompletion of
    code snippets may have an accelerating effect, lowering the number of
    keystrokes that are needed to type the code. However, like any tool,
    implicit tendencies of users may emerge. Knowledge of how developers in
    different roles use autocompletion features may help to guide future
    autocompletion development, research, and training material. In this paper,
    we set out to better understand how usage of autocompletion varies among
    software engineers and other developers (i.e., academic researchers,
    industry researchers, hobby programmers, and students). Analysis of
    autocompletion events in the Mining Software Repositories (MSR) challenge
    dataset reveals that: (1) rates of autocompletion usage among software
    engineers and other developers are not significantly different; and (2)
    although several non-negligible effect sizes of autocompletion targets
    (e.g., local variables, method names) are detected between the two groups,
    the rates at which these targets appear do not vary to a significant
    degree. These inconclusive results are likely due to the small sample size
    (n = 35); however, they do provide an interesting insight for future studies
    to build upon.

- key: Anda2009
  kind: article
  author:
  - B.C.D. Anda
  - D.I.K. Sjoberg
  - A. Mockus
  title: "Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System"
  journal: TSE
  volume: 35
  number: 3
  pages: 407--429
  month: '5'
  year: 2009
  doi: 10.1109/tse.2008.89
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  url: https://doi.org/10.1109/tse.2008.89
  abstract: >
    The scientific study of a phenomenon requires it to be reproducible. Mature
    engineering industries are recognized by projects and products that are, to
    some extent, reproducible. Yet, reproducibility in software engineering (SE)
    has not been investigated thoroughly, despite the fact that lack of
    reproducibility has both practical and scientific consequences. We report a
    longitudinal multiple-case study of variations and reproducibility in
    software development, from bidding to deployment, on the basis of the same
    requirement specification. In a call for tender to 81 companies, 35
    responded. Four of them developed the system independently. The firm price,
    planned schedule, and planned development process, had, respectively,
    ldquolow,rdquo ldquolow,rdquo and ldquomediumrdquo reproducibilities. The
    contractor's costs, actual lead time, and schedule overrun of the projects
    had, respectively, ldquomedium,rdquo ldquohigh,rdquo and ldquolowrdquo
    reproducibilities. The quality dimensions of the delivered products,
    reliability, usability, and maintainability had, respectively,
    ldquolow,rdquo "high,rdquo and ldquolowrdquo reproducibilities. Moreover,
    variability for predictable reasons is also included in the notion of
    reproducibility. We found that the observed outcome of the four development
    projects matched our expectations, which were formulated partially on the
    basis of SE folklore. Nevertheless, achieving more reproducibility in SE
    remains a great challenge for SE research, education, and industry.

- key: Aranda2009
  kind: inproceedings
  author:
  - Jorge Aranda
  - Gina Venolia
  title: "The Secret Life of Bugs: Going Past the Errors and Omissions in Software Repositories"
  booktitle: Proc. ICSE'09
  year: 2009
  doi: 10.1109/ICSE.2009.5070530
  abstract: >
    Every bug has a story behind it. The people that discover and resolve it
    need to coordinate, to get information from documents, tools, or other
    people, and to navigate through issues of accountability, ownership, and
    organizational structure. This paper reports on a field study of
    coordination activities around bug fixing that used a combination of case
    study research and a survey of software professionals. Results show that the
    histories of even simple bugs are strongly dependent on social,
    organizational, and technical knowledge that cannot be solely extracted
    through automation of electronic repositories, and that such automation
    provides incomplete and often erroneous accounts of coordination. The paper
    uses rich bug histories and survey results to identify common bug fixing
    coordination patterns and to provide implications for tool designers and
    researchers of coordination in software development.

- key: Armstrong2007
  kind: book
  author:
  - Karen Armstrong
  title: "The Great Transformation: The Beginning of Our Religious Traditions"
  year: 2007
  isbn: 978-0676974669
  publisher: Vintage Canada

- key: Aurora2018
  kind: book
  author:
  - Valerie Aurora
  - Mary Gardiner
  title: How to Respond to Code of Conduct Reports
  year: 2018
  isbn: 978-1386922575
  publisher: Frame Shift Consulting LLC

- key: Auvinen2020
  kind: inproceedings
  author:
  - Tapio Auvinen
  - Nickolas Falkner
  - Arto Hellas
  - Petri Ihantola
  - Ville Karavirta
  - Otto Seppala
  title: Relation of Individual Time Management Practices and Time Management of Teams
  booktitle: FIE'20
  year: 2020
  doi: 10.1109/fie44824.2020.9274203
  abstract: >
    Full research paper—Team configuration, work practices, and communication
    have a considerable impact on the outcomes of student software
    projects. This study observes 150 college students who first individually
    solve exercises and then carry out a class project in teams of three. All
    projects had the same requirements. We analyzed how students’ behavior on
    individual pre-project exercises predict team project outcomes, investigated
    how students’ time management practices affected other team members, and
    analyzed how students divided their work among peers. Our results indicate
    that teams consisting of only low-performing students were the most
    dysfunctional in terms of workload balance, whereas teams with both low-and
    high-performing students performed almost as well as teams consisting of
    only high-performing students. This suggests that teams should combine
    students of varying skill levels rather than allowing teams with only low
    performers or letting students to form teams without constraints. We also
    observed that individual students’ poor time management practices impair
    their teammates’ time management. This underlines the importance of
    encouraging good time management practices. Most teams reported that they
    divided tasks in a way that is beneficial for the acquisition of technical
    skills rather than collaboration and communication skills. Only a few teams
    assigned tasks so that students would have worked only on tasks they already
    knew and thus felt most comfortable to work with.

- key: Bacchelli2013
  kind: inproceedings
  author:
  - Alberto Bacchelli
  - Christian Bird
  title: Expectations, Outcomes, and Challenges of Modern Code Review
  booktitle: Proc. ICSE'13
  year: 2013

- key: Bach2017
  kind: inproceedings
  author:
  - Thomas Bach
  - Artur Andrzejak
  - Ralf Pannemans
  - David Lo
  title: The Impact of Coverage on Bug Density in a Large Industrial Software Project
  booktitle: Proc. ESEM'17
  month: '11'
  year: 2017
  doi: 10.1109/esem.2017.44
  publisher: IEEE
  url: https://doi.org/10.1109/esem.2017.44
  abstract: >
    Measuring quality of test suites is one of the major challenges of software
    testing. Code coverage identifies tested and untested parts of code and is
    frequently used to approximate test suite quality. Multiple previous studies
    have investigated the relationship between coverage ratio and test suite
    quality, without a clear consent in the results. In this work we study
    whether covered code contains a smaller number of future bugs than uncovered
    code (assuming appropriate scaling). If this correlation holds and bug
    density is lower in covered code, coverage can be regarded as a meaningful
    metric to estimate the adequacy of testing. To this end we analyse 16000
    internal bug reports and bug-fixes of SAP HANA, a large industrial software
    project. We found that the above-mentioned relationship indeed holds, and is
    statistically significant. Contrary to most previous works our study uses
    real bugs and real bug-fixes. Furthermore, our data is derived from a
    complex and large industrial project.

- key: Baker2006
  kind: book
  author:
  - Jean H. Baker
  title: "Sisters: The Lives of America''s Suffragists"
  year: 2006
  isbn: 978-0809087037
  publisher: Hill and Wang

- key: Barcomb2019
  kind: inproceedings
  author:
  - Ann Barcomb
  - Klaas-Jan Stol
  - Dirk Riehle
  - Brian Fitzgerald
  title: Why Do Episodic Volunteers Stay in FLOSS Communities?
  booktitle: Proc. ICSE'19
  month: '5'
  year: 2019
  doi: 10.1109/icse.2019.00100
  publisher: IEEE
  url: https://doi.org/10.1109/icse.2019.00100
  abstract: >
    Successful Free/Libre and Open Source Software (FLOSS) projects incorporate
    both habitual and infrequent, or episodic, contributors. Using the concept
    of episodic volunteering (EV) from the general volunteering literature, we
    derive a model consisting of five key constructs that we hypothesize affect
    episodic volunteers' retention in FLOSS communities. To evaluate the model
    we conducted a survey with over 100 FLOSS episodic volunteers. We observe
    that three of our model constructs (social norms, satisfaction and community
    commitment) are all positively associated with volunteers' intention to
    remain, while the two other constructs (psychological sense of community and
    contributor benefit motivations) are not. Furthermore, exploratory
    clustering on unobserved heterogeneity suggests that there are four distinct
    categories of volunteers: satisfied, classic, social and obligated. Based on
    our findings, we offer suggestions for projects to incorporate and manage
    episodic volunteers, so as to better leverage this type of contributors and
    potentially improve projects' sustainability.

- key: Barke2019
  kind: article
  author:
  - Helena Barke
  - Lutz Prechelt
  title: Role Clarity Deficiencies Can Wreck Agile Teams
  journal: PeerJ Computer Science
  volume: 5
  year: 2019
  doi: 10.7717/peerj-cs.241
  abstract: >
    Background One of the twelve agile principles is to build projects around
    motivated individuals and trust them to get the job done. Such agile teams
    must self-organize, but this involves conflict, making self-organization
    difficult. One area of difficulty is agreeing on everybody’s
    role. Background What dynamics arise in a self-organizing team from the
    negotiation of everybody’s role? Method We conceptualize observations from
    five agile teams (work observations, interviews) by Charmazian Grounded
    Theory Methodology. Results We define role as something transient and
    implicit, not fixed and named. The roles are characterized by the
    responsibilities and expectations of each team member. Every team member
    must understand and accept their own roles (Local role clarity) and everbody
    else’s roles (Team-wide role clarity). Role clarity allows a team to work
    smoothly and effectively and to develop its members’ skills fast. Lack of
    role clarity creates friction that not only hampers the day-to-day work, but
    also appears to lead to high employee turnover. Agile coaches are critical
    to create and maintain role clarity. Conclusions Agile teams should pay
    close attention to the levels of Local role clarity of each member and
    Team-wide role clarity overall, because role clarity deficits are highly
    detrimental.

- key: Barr2014
  kind: inproceedings
  author:
  - Earl T. Barr
  - Yuriy Brun
  - Premkumar Devanbu
  - Mark Harman
  - Federica Sarro
  title: The plastic surgery hypothesis
  booktitle: Proc. FSE'14
  year: 2014
  doi: 10.1145/2635868.2635898
  publisher: ACM Press
  url: https://doi.org/10.1145/2635868.2635898
  abstract: >
    Recent work on genetic-programming-based approaches to automatic program
    patching have relied on the insight that the content of new code can often
    be assembled out of fragments of code that already exist in the code
    base. This insight has been dubbed the plastic surgery hypothesis;
    successful, well-known automatic repair tools such as GenProg rest on this
    hypothesis, but it has never been validated. We formalize and validate the
    plastic surgery hypothesis and empirically measure the extent to which raw
    material for changes actually already exists in projects. In this paper, we
    mount a large-scale study of several large Java projects, and examine a
    history of 15,723 commits to determine the extent to which these commits are
    graftable, i.e., can be reconstituted from existing code, and find an
    encouraging degree of graftability, surprisingly independent of commit size
    and type of commit. For example, we find that changes are 43% graftable from
    the exact version of the software being changed. With a view to
    investigating the difficulty of finding these grafts, we study the abundance
    of such grafts in three possible sources: the immediately previous version,
    prior history, and other projects. We also examine the contiguity or
    chunking of these grafts, and the degree to which grafts can be found in the
    same file. Our results are quite promising and suggest an optimistic future
    for automatic program patching methods that search for raw material in
    already extant code in the project being patched.

- key: Basili1987
  kind: article
  author:
  - Victor R. Basili
  - Richard W. Selby
  title: Comparing the Effectiveness of Software Testing Strategies
  journal: TSE
  volume: SE-13
  number: 12
  year: 1987
  doi: 10.1109/tse.1987.232881
  abstract: >
    This study applies an experimentation methodology to compare three
    state-of-the-practice software testing techniques: a) code reading by
    stepwise abstraction, b) functional testing using equivalence partitioning
    and boundary value analysis, and c) structural testing using 100 percent
    statement coverage criteria. The study compares the strategies in three
    aspects of software testing: fault detection effectiveness, fault detection
    cost, and classes of faults detected. Thirty-two professional programmers
    and 42 advanced students applied the three techniques to four unit-sized
    programs in a fractional factorial experimental design. The major results of
    this study are the following. 1) With the professional programmers, code
    reading detected more software faults and had a higher fault detection rate
    than did functional or structural testing, while functional testing detected
    more faults than did structural testing, but functional and structural
    testing were not different in fault detection rate. 2) In one advanced
    student subject group, code reading and functional testing were not
    different in faults found, but were both superior to structural testing,
    while in the other advanced student subject group there was no difference
    among the techniques. 3) With the advanced student subjects, the three
    techniques were not different in fault detection rate. 4) Number of faults
    observed, fault detection rate, and total effort in detection depended on
    the type of software tested. 5) Code reading detected more interface faults
    than did the other methods. 6) Functional testing detected more control
    faults than did the other methods.

- key: Becker2016
  kind: article
  author:
  - Brett A. Becker
  - Graham Glanville
  - Ricardo Iwashima
  - Claire McDonnell
  - Kyle Goslin
  - Catherine Mooney
  title: Effective Compiler Error Message Enhancement for Novice Programming Students
  journal: CSE
  volume: 26
  number: 2-3
  year: 2016
  doi: 10.1080/08993408.2016.1225464
  abstract: >
    Programming is an essential skill that many computing students are expected
    to master. However, programming can be difficult to learn. Successfully
    interpreting compiler error messages (CEMs) is crucial for correcting errors
    and progressing toward success in programming. Yet these messages are often
    difficult to understand and pose a barrier to progress for many novices,
    with struggling students often exhibiting high frequencies of errors,
    particularly repeated errors. This paper presents a control/intervention
    study on the effectiveness of enhancing Java CEMs. Results show that the
    intervention group experienced reductions in the number of overall errors,
    errors per student, and several repeated error metrics. These results are
    important as the effectiveness of CEM enhancement has been recently
    debated. Further, generalizing these results should be possible at least in
    part, as the control group is shown to be comparable to those in several
    studies using Java and other languages.

- key: Begel2014
  kind: inproceedings
  author:
  - Andrew Begel
  - Thomas Zimmermann
  title: Analyze this! 145 questions for data scientists in software engineering
  booktitle: Proc. ICSE'14
  year: 2014
  doi: 10.1145/2568225.2568233
  publisher: ACM Press
  url: https://doi.org/10.1145/2568225.2568233
  abstract: >
    In this paper, we present the results from two surveys related to data
    science applied to software engineering. The first survey solicited
    questions that software engineers would like data scientists to investigate
    about software, about software processes and practices, and about software
    engineers. Our analyses resulted in a list of 145 questions grouped into 12
    categories. The second survey asked a different pool of software engineers
    to rate these 145 questions and identify the most important ones to work on
    first. Respondents favored questions that focus on how customers typically
    use their applications. We also saw opposition to questions that assess the
    performance of individual employees or compare them with one another. Our
    categorization and catalog of 145 questions can help researchers,
    practitioners, and educators to more easily focus their efforts on topics
    that are important to the software industry.

- key: Behroozi2019
  kind: inproceedings
  author:
  - Mahnaz Behroozi
  - Chris Parnin
  - Titus Barik
  title: "Hiring is Broken: What Do Developers Say About Technical Interviews?"
  booktitle: Proc. VL/HCC'19
  year: 2019
  doi: 10.1109/VLHCC.2019.8818836
  abstract: >
    Technical interviews—a problem-solving form of interview in which candidates
    write code—are commonplace in the software industry, and are used by several
    well-known companies including Facebook, Google, and Microsoft. These
    interviews are intended to objectively assess candidates and determine fit
    within the company. But what do developers say about them?To understand
    developer perceptions about technical interviews, we conducted a qualitative
    study using the online social news website, Hacker News—a venue for software
    practitioners. Hacker News posters report several concerns and negative
    perceptions about interviews, including their lack of real-world relevance,
    bias towards younger developers, and demanding time commitment. Posters
    report that these interviews cause unnecessary anxiety and frustration,
    requiring them to learn arbitrary, implicit, and obscure norms. The findings
    from our study inform inclusive hiring guidelines for technical interviews,
    such as collaborative problem-solving sessions.

- key: Behroozi2020a
  kind: inproceedings
  author:
  - Mahnaz Behroozi
  - Shivani Shirolkar
  - Titus Barik
  - Chris Parnin
  title: "Debugging Hiring: What Went Right and What Went Wrong in the Technical Interview Process"
  booktitle: Proc. ICSE'20
  year: 2020
  doi: 10.1145/3377815.3381372
  abstract: >
    The typical hiring pipeline for software engineering occurs over several
    stages—from phone screening and technical on-site interviews, to offer and
    negotiation. When these hiring pipelines are “leaky,” otherwise qualified
    candidates are lost at some stage of the pipeline. These leaky pipelines
    impact companies in several ways, including hindering a company’s ability to
    recruit competitive candidates and build diverse software teams.To
    understand where candidates become disengaged in the hiring pipeline—and
    what companies can do to prevent it—we conducted a qualitative study on over
    10,000 reviews on 19 companies from Glassdoor, a website where candidates
    can leave reviews about their hiring process experiences. We identified
    several poor practices which prematurely sabotage the hiring process—for
    example, not adequately communicating hiring criteria, conducting interviews
    with inexperienced interviewers, and ghosting candidates. Our findings
    provide a set of guidelines to help companies improve their hiring pipeline
    practices—such as being deliberate about phrasing and language during
    initial contact with the candidate, providing candidates with constructive
    feedback after their interviews, and bringing salary transparency and
    long-term career discussions into offers and negotiations. Operationalizing
    these guidelines helps make the hiring pipeline more transparent, fair, and
    inclusive.

- key: Behroozi2020b
  kind: inproceedings
  author:
  - Mahnaz Behroozi
  - Shivani Shirolkar
  - Titus Barik
  - Chris Parnin
  title: Does Stress Impact Technical Interview Performance?
  booktitle: Proc. ESEC/FSE'20
  year: 2020
  doi: 10.1145/3368089.3409712
  abstract: >
    Software engineering candidates commonly participate in whiteboard technical
    interviews as part of a hiring assessment. During these sessions, candidates
    write code while thinking aloud as they work towards a solution, under the
    watchful eye of an interviewer. While technical interviews should allow for
    an unbiased and inclusive assessment of problem-solving ability,
    surprisingly, technical interviews may be instead a procedure for
    identifying candidates who best handle and migrate stress solely caused by
    being examined by an interviewer (performance anxiety). To understand if
    coding interviews—as administered today—can induce stress that significantly
    hinders performance, we conducted a randomized controlled trial with 48
    Computer Science students, comparing them in private and public whiteboard
    settings. We found that performance is reduced by more than half, by simply
    being watched by an interviewer. We also observed that stress and cognitive
    load were significantly higher in a traditional technical interview when
    compared with our private interview. Consequently, interviewers may be
    filtering out qualified candidates by confounding assessment of
    problem-solving ability with unnecessary stress. We propose interview
    modifications to make problem-solving assessment more equitable and
    inclusive, such as through private focus sessions and retrospective
    think-aloud, allowing companies to hire from a larger and diverse pool of
    talent.

- key: Beller2018
  kind: inproceedings
  author:
  - Moritz Beller
  - Niels Spruit
  - Diomidis Spinellis
  - Andy Zaidman
  title: On the Dichotomy of Debugging Behavior Among Programmers
  booktitle: Proc. ICSE'18
  year: 2018
  doi: 10.1145/3180155.3180175
  abstract: >
    Debugging is an inevitable activity in most software projects, often
    difficult and more time-consuming than expected, giving it the nickname the
    "dirty little secret of computer science." Surprisingly, we have little
    knowledge on how software engineers debug software problems in the real
    world, whether they use dedicated debugging tools, and how knowledgeable
    they are about debugging. This study aims to shed light on these aspects by
    following a mixed-methods research approach. We conduct an online survey
    capturing how 176 developers reflect on debugging. We augment this
    subjective survey data with objective observations on how 458 developers use
    the debugger included in their integrated development environments (IDEs) by
    instrumenting the popular Eclipse and IntelliJ IDEs with the purpose-built
    plugin WatchDog 2.0. To clarify the insights and discrepancies observed in
    the previous steps, we followed up by conducting interviews with debugging
    experts and regular debugging users. Our results indicate that IDE-provided
    debuggers are not used as often as expected, as "printf debugging" remains
    a feasible choice for many programmers. Furthermore, both knowledge and use
    of advanced debugging features are low. These results call to strengthen
    hands-on debugging experience in computer science curricula and have already
    refined the implementation of modern IDE debuggers.

- key: Beller2019
  kind: article
  author:
  - Moritz Beller
  - Georgios Gousios
  - Annibale Panichella
  - Sebastian Proksch
  - Sven Amann
  - Andy Zaidman
  title: "Developer Testing in the IDE: Patterns, Beliefs, and Behavior"
  journal: TSE
  volume: 45
  number: 3
  year: 2019
  doi: 10.1109/tse.2017.2776152
  abstract: >
    Software testing is one of the key activities to achieve software quality in
    practice. Despite its importance, however, we have a remarkable lack of
    knowledge on how developers test in real-world projects. In this paper, we
    report on a large-scale field study with 2,443 software engineers whose
    development activities we closely monitored over 2.5 years in four
    integrated development environments (IDEs). Our findings, which largely
    generalized across the studied IDEs and programming languages Java and C#,
    question several commonly shared assumptions and beliefs about developer
    testing: half of the developers in our study do not test; developers rarely
    run their tests in the IDE; most programming sessions end without any test
    execution; only once they start testing, do they do it extensively; a
    quarter of test cases is responsible for three quarters of all test
    failures; 12 percent of tests show flaky behavior; Test-Driven Development
    (TDD) is not widely practiced; and software developers only spend a quarter
    of their time engineering tests, whereas they think they test half of their
    time. We summarize these practices of loosely guiding one's development
    efforts with the help of testing in an initial summary on Test-Guided
    Development (TGD), a behavior we argue to be closer to the development
    reality of most developers than TDD.

- key: Bellman2018
  kind: inproceedings
  author:
  - Christopher Bellman
  - Ahmad Seet
  - Olga Baysal
  title: Studying developer build issues and debugger usage via timeline analysis in visual studio IDE
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196463
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196463
  abstract: >
    Every day, most software developers use development tools to write, build,
    and maintain their code. The most crucial of such tools is the integrated
    development environment (IDE), in which developers create and build
    code. Therefore, it is important to understand how developers perform their
    work and what impact each action has on their workflow to further enhance
    their productivity. In this work, we study the KaVE dataset of developer
    interactions within the Microsoft Visual Studio IDE and analyze a number of
    topics extracted from the data. First, we propose a method for developing
    what we call "timelines" that chronologically map an individual
    development session, and from this, we study build failures, code debugger
    usage, and we propose a metric for measuring developer throughput. We find
    that the timeline analysis may prove to be an invaluable tool for developer
    self-assessment and key to uncovering problem areas regarding build
    failures. Moreover, we find that developers spend a significant amount of
    time debugging their code, utilizing features such as breakpoints to resolve
    issues. Finally, we see that the developer metric can be used for self
    assessment, giving value to the amount of effort, put forth by a developer,
    in a given session.

- key: Beniamini2017
  kind: inproceedings
  author:
  - Gal Beniamini
  - Sarah Gingichashvili
  - Alon Klein Orbach
  - Dror G. Feitelson
  title: "Meaningful Identifier Names: The Case of Single-Letter Variables"
  booktitle: Proc. ICPC'17
  month: '5'
  year: 2017
  doi: 10.1109/icpc.2017.18
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  url: https://doi.org/10.1109/icpc.2017.18
  abstract: >
    It is widely accepted that variable names in computer programs should be
    meaningful, and that this aids program comprehension. "Meaningful" is
    commonly interpreted as favoring long descriptive names. However, there is
    at least some use of short and even single-letter names: using 'i' in loops
    is very common, and we show (by extracting variable names from 1000 popular
    github projects in 5 languages) that some other letters are also widely
    used. In addition, controlled experiments with different versions of the
    same functions (specifically, different variable names) failed to show
    significant differences in ability to modify the code. Finally, an online
    survey showed that certain letters are strongly associated with certain
    types and meanings. This implies that a single letter can in fact convey
    meaning. The conclusion from all this is that single letter variables can
    indeed be used beneficially in certain cases, leading to more concise code.

- key: Benner2000
  kind: book
  author:
  - Patricia Benner
  title: "From Novice to Expert: Excellence and Power in Clinical Nursing Practice"
  year: 2000
  isbn: 978-0130325228
  publisher: Pearson

- key: Berlin2000
  kind: book
  author:
  - Isaiah Berlin
  title: "The Proper Study of Mankind: An Anthology of Essays"
  year: 2000
  isbn: 978-0374527174
  publisher: Farrar Straus and Giroux

- key: Bernstein2018
  kind: article
  author:
  - Ethan S. Bernstein
  - Stephen Turban
  title: The Impact of the 'Open' Workspace on Human Collaboration
  journal: 'Philosophical Transactions of the Royal Society B: Biological Sciences'
  volume: 373
  number: 1753
  year: 2018
  doi: 10.1098/rstb.2017.0239
  abstract: >
    Organizations’ pursuit of increased workplace collaboration has led managers
    to transform traditional office spaces into ‘open’, transparency-enhancing
    architectures with fewer walls, doors and other spatial boundaries, yet
    there is scant direct empirical research on how human interaction patterns
    change as a result of these architectural changes. In two intervention-based
    field studies of corporate headquarters transitioning to more open office
    spaces, we empirically examined—using digital data from advanced wearable
    devices and from electronic communication servers—the effect of open office
    architectures on employees' face-to-face, email and instant messaging (IM)
    interaction patterns. Contrary to common belief, the volume of face-to-face
    interaction decreased significantly (approx. 70%) in both cases, with an
    associated increase in electronic interaction. In short, rather than
    prompting increasingly vibrant face-to-face collaboration, open architecture
    appeared to trigger a natural human response to socially withdraw from
    officemates and interact instead over email and IM. This is the first study
    to empirically measure both face-to-face and electronic interaction before
    and after the adoption of open office architecture. The results inform our
    understanding of the impact on human behaviour of workspaces that trend
    towards fewer spatial boundaries. This article is part of the theme issue
    ‘Interdisciplinary approaches for uncovering the impacts of architecture on
    collective behaviour’.

- key: Bettenburg2008
  kind: inproceedings
  author:
  - Nicolas Bettenburg
  - Sascha Just
  - "Adrian Schröter"
  - Cathrin Weiss
  - Rahul Premraj
  - Thomas Zimmermann
  title: What Makes a Good Bug Report?
  booktitle: Proc. SIGSOFT'08
  year: 2008
  doi: 10.1145/1453101.1453146
  abstract: >
    In software development, bug reports provide crucial information to
    developers. However, these reports widely differ in their quality. We
    conducted a survey among developers and users of APACHE, ECLIPSE, and
    MOZILLA to find out what makes a good bug report. The analysis of the 466
    responses revealed an information mismatch between what developers need and
    what users supply. Most developers consider steps to reproduce, stack
    traces, and test cases as helpful, which are, at the same time, most
    difficult to provide for users. Such insight is helpful for designing new
    bug tracking tools that guide users at collecting and providing more helpful
    information. Our CUEZILLA prototype is such a tool and measures the quality
    of new bug reports; it also recommends which elements should be added to
    improve the quality. We trained CUEZILLA on a sample of 289 bug reports,
    rated by developers as part of the survey. The participants of our survey
    also provided 175 comments on hurdles in reporting and resolving bugs. Based
    on these comments, we discuss several recommendations for better bug
    tracking systems, which should focus on engaging bug reporters, better tool
    support, and improved handling of bug duplicates.

- key: Bielaczyc1995
  kind: article
  author:
  - Katerine Bielaczyc
  - Peter L. Pirolli
  - Ann L. Brown
  title: "Training in Self-Explanation and Self-Regulation Strategies: Investigating the Effects of Knowledge Acquisition Activities on Problem Solving"
  journal: Cognition and Instruction
  volume: 13
  number: 2
  year: 1995
  doi: 10.1207/s1532690xci1302_3
  abstract: >
    Previous research has found positive correlations between particular
    strategies students use while studying to explain instructional materials to
    themselves and student performance on associated problem-solving tasks (Chi,
    Bassok, Lewis, Reimann, & Glaser, 1989; Pirolli & Bielaczyc, 1989; Pirolli &
    Recker, 1994). In the study reported here, we investigate the causal nature
    of this relation. This was accomplished by identifying a set of
    self-explanation and self-regulation strategies used by high-performance
    students in our earlier studies. We used strategy training to manipulate
    students' application of these strategies and examined the impact of their
    use on student explanations and performance. Twenty-four university students
    with no prior programming experience worked through a sequence of
    programming lessons. Following introductory lessons, participants received
    interventions involving explicit training in the strategies (instructional
    group) or received a similar set of interventions but no explicit ...

- key: Binkley2012
  kind: article
  author:
  - Dave Binkley
  - Marcia Davis
  - Dawn Lawrie
  - Jonathan I. Maletic
  - Christopher Morrell
  - Bonita Sharif
  title: The Impact of Identifier Style on Effort and Comprehension
  journal: ESE
  volume: 18
  number: 2
  year: 2012
  doi: 10.1007/s10664-012-9201-4
  abstract: >
    A family of studies investigating the impact of program identifier style on
    human comprehension is presented. Two popular identifier styles are
    examined, namely camel case and underscore. The underlying hypothesis is
    that identifier style affects the speed and accuracy of comprehending source
    code. To investigate this hypothesis, five studies were designed and
    conducted. The first study, which investigates how well humans read
    identifiers in the two different styles, focuses on low-level readability
    issues. The remaining four studies build on the first to focus on the
    semantic implications of identifier style. The studies involve 150
    participants with varied demographics from two different universities. A
    range of experimental methods is used in the studies including timed
    testing, read aloud, and eye tracking. These methods produce a broad set of
    measurements and appropriate statistical methods, such as regression models
    and Generalized Linear Mixed Models (GLMMs), are applied to analyze the
    results. While unexpected, the results demonstrate that the tasks of reading
    and comprehending source code is fundamentally different from those of
    reading and comprehending natural language. Furthermore, as the task becomes
    similar to reading prose, the results become similar to work on reading
    natural language text. For more “source focused” tasks, experienced software
    developers appear to be less affected by identifier style; however,
    beginners benefit from the use of camel casing with respect to accuracy and
    effort.

- key: Bird2011
  kind: inproceedings
  author:
  - Christian Bird
  - Nachiappan Nagappan
  - Brendan Murphy
  - Harald Gall
  - Premkumar Devanbu
  title: Don't touch my code!
  booktitle: Proc. FSE'11
  year: 2011
  doi: 10.1145/2025113.2025119
  publisher: ACM Press
  url: https://doi.org/10.1145/2025113.2025119
  abstract: >
    Ownership is a key aspect of large-scale software development. We examine
    the relationship between different ownership measures and software failures
    in two large software projects: Windows Vista and Windows 7. We find that in
    all cases, measures of ownership such as the number of low-expertise
    developers, and the proportion of ownership for the top owner have a
    relationship with both pre-release faults and post-release failures. We also
    empirically identify reasons that low-expertise developers make changes to
    components and show that the removal of low-expertise contributions
    dramatically decreases the performance of contribution based defect
    prediction. Finally we provide recommendations for source code change
    policies and utilization of resources such as code inspections based on our
    results.

- key: Bluedorn1999
  kind: article
  author:
  - Allen C. Bluedorn
  - Daniel B. Turban
  - Mary Sue Love
  title: The effects of stand-up and sit-down meeting formats on meeting outcomes.
  journal: Journal of Applied Psychology
  volume: 84
  number: 2
  pages: 277--285
  year: 1999
  doi: 10.1037/0021-9010.84.2.277
  publisher: American Psychological Association (APA)
  url: https://doi.org/10.1037/0021-9010.84.2.277
  abstract: >
    The effects of meeting format (standing or sitting) on meeting length and
    the quality of group decision making were investigated by comparing meeting
    outcomes for 56 five-member groups that conducted meetings in a standing
    format with 55 five-member groups that conducted meetings in a seated
    format. Sit-down meetings were 34% longer than stand-up meetings, but they
    produced no better decisions than stand-up meetings. Significant differences
    were also obtained for satisfaction with the meeting and task information
    use during the meeting but not for synergy or commitment to the group's
    decision. The findings were generally congruent with meeting-management
    recommendations in the time-management literature, although the lack of a
    significant difference for decision quality was contrary to theoretical
    expectations. This contrary finding may have been due to differences between
    the temporal context in which this study was conducted and those in which
    other time constraint research has been conducted, thereby revealing a
    potentially important contingency-temporal context.

- key: Boehm1981
  kind: book
  author:
  - Barry Boehm
  title: Software Engineering Economics
  year: 1981
  isbn: 978-0138221225
  publisher: Prentice-Hall

- key: Bohay2011
  kind: article
  author:
  - Mark Bohay
  - Daniel P. Blakely
  - Andrea K. Tamplin
  - Gabriel A. Radvansky
  title: Note Taking, Review, Memory, and Comprehension
  journal: American Journal of Psychology
  volume: 124
  number: 1
  year: 2011
  doi: 10.5406/amerjpsyc.124.1.0063
  abstract: >
    In previous work assessing memory at various levels of representation,
    namely the surface form, textbase, and situation model levels, participants
    read texts but were otherwise not actively engaged with the texts. The
    current study tested the influence of active engagement with the material
    via note taking, along with the opportunity to review such notes, and the
    modality of presentation (text vs. spoken). The influence of these
    manipulations was assessed both immediately and 1 week later. In Experiment
    1 participants read a text, whereas in Experiment 2 participants watched a
    video recording of the material being read as a lecture. For each experiment
    the opportunity to take notes was manipulated within participants, and the
    opportunity to review these notes before the test was manipulated between
    participants. Note taking improved performance at the situation model level
    in both experiments, although there was also some suggestion of benefit for
    the surface form. Thus, active engagement with material, such as note
    taking, appears to have the greatest benefit at the deeper levels of
    understanding.

- key: Bollier2014
  kind: book
  author:
  - David Bollier
  title: "Think Like a Commoner: A Short Introduction to the Life of the Commons"
  year: 2014
  isbn: 978-0865717688
  publisher: New Society Publishers

- key: Borst2015
  kind: inproceedings
  author:
  - Jelmer P. Borst
  - Niels A. Taatgen
  - Hedderik van Rijn
  title: What Makes Interruptions Disruptive?
  booktitle: Proc. CHI'15
  year: 2015
  doi: 10.1145/2702123.2702156
  abstract: >
    In this paper we present a computational cognitive model of task
    interruption and resumption, focusing on the effects of the problem state
    bottleneck. Previous studies have shown that the disruptiveness of
    interruptions is for an important part determined by three factors:
    interruption duration, interrupting-task complexity, and moment of
    interruption. However, an integrated theory of these effects is still
    missing. Based on previous research into multitasking, we propose a first
    step towards such a theory in the form of a process model that attributes
    these effects to problem state requirements of both the interrupted and the
    interrupting task. Subsequently, we tested two predictions of this model in
    two experiments. The experiments confirmed that problem state requirements
    are an important predictor for the disruptiveness of interruptions. This
    suggests that interfaces should be designed to a) interrupt users at
    low-problem state moments and b) maintain the problem state for the user
    when interrupted.

- key: Brand1995
  kind: book
  author:
  - Stewart Brand
  title: "How Buildings Learn: What Happens After They''re Built"
  year: 1995
  isbn: 978-0140139969
  publisher: Penguin USA

- key: Brock2019
  kind: link
  author:
  - Jon Brock
  title: "A love letter to your future self: What scientists need to know about FAIR data"
  year: 2019
  url: https://www.natureindex.com/news-blog/what-scientists-need-to-know-about-fair-data

- key: Brookfield2016
  kind: book
  author:
  - Stephen D. Brookfield
  - Stephen Preskill
  title: "The Discussion Book: 50 Great Ways to Get People Talking"
  year: 2016
  isbn: 978-1119049715
  publisher: Jossey-Bass

- key: Brophy1983
  kind: article
  author:
  - Jere E. Brophy
  title: Research on the Self-Fulfilling Prophecy and Teacher Expectations
  journal: Journal of Educational Psychology
  volume: 75
  number: 5
  year: 1983
  doi: 10.1037/0022-0663.75.5.631

- key: Brown2007
  kind: book
  author:
  - Michael Jacoby Brown
  title: "Building Powerful Community Organizations: A Personal Guide to Creating Groups that Can Solve Problems and Change the World"
  year: 2007
  isbn: 978-0977151806
  publisher: Long Haul Press

- key: Brown2011
  kind: book
  title: "The Architecture of Open Source Applications: Elegance, Evolution, and a Few Fearless Hacks"
  editor:
  - Amy Brown
  - Greg Wilson
  year: 2011
  isbn: 978-1257638017
  publisher: Lulu
  url: http://aosabook.org
  note: Descriptions of the architecture of two dozen open source systems written by the people who created them.

- key: Brown2012
  kind: book
  title: "The Architecture of Open Source Applications: Structure, Scale, and a Few More Fearless Hacks"
  editor:
  - Amy Brown
  - Greg Wilson
  year: 2012
  isbn: 978-0201103427
  publisher: Lulu
  url: http://aosabook.org
  note: More descriptions open source system architectures written by those systems' creators.

- key: Brown2016
  kind: book
  title: "500 Lines or Less: Experienced Programmers Solve Interesting Problems"
  editor:
  - Amy Brown
  - Michael DiBernardo
  year: 2016
  isbn: 978-1329871274
  publisher: Lulu
  url: http://aosabook.org
  note: Scale models of real applications, with commentary from their creators.

- key: Brown2017
  kind: article
  author:
  - Neil C. C. Brown
  - Amjad Altadmri
  title: Novice Java Programming Mistakes
  journal: TCE
  volume: 17
  number: 2
  year: 2017
  doi: 10.1145/2994154
  abstract: >
    Teaching is the process of conveying knowledge and skills to learners. It
    involves preventing misunderstandings or correcting misconceptions that
    learners have acquired. Thus, effective teaching relies on solid knowledge
    of the discipline, but also a good grasp of where learners are likely to
    trip up or misunderstand. In programming, there is much opportunity for
    misunderstanding, and the penalties are harsh: failing to produce the
    correct syntax for a program, for example, can completely prevent any
    progress in learning how to program. Because programming is inherently
    computer-based, we have an opportunity to automatically observe programming
    behaviour -- more closely even than an educator in the room at the time. By
    observing students’ programming behaviour, and surveying educators, we can
    ask: do educators have an accurate understanding of the mistakes that
    students are likely to make? In this study, we combined two years of the
    Blackbox dataset (with more than 900 thousand users and almost 100 million
    compilation events) with a survey of 76 educators to investigate which
    mistakes students make while learning to program Java, and whether the
    educators could make an accurate estimate of which mistakes were most
    common. We find that educators’ estimates do not agree with one another or
    the student data, and discuss the implications of these results.

- key: BuenoDeMesquita2012
  kind: book
  author:
  - Bruce Bueno de Mesquita
  - Alastair Smith
  title: "The Dictator''s Handbook: Why Bad Behavior is Almost Always Good Politics"
  year: 2012
  isbn: 978-1610391849
  publisher: PublicAffairs

- key: Bullock2021
  kind: inproceedings
  author:
  - Beleicia B. Bullock
  - Fernando L. Nascimento
  - Stacy A. Doore
  title: "Computing Ethics Narratives: Teaching Computing Ethics and the Impact of Predictive Algorithms"
  booktitle: Proc. ICSE'21
  year: 2021
  doi: 10.1145/3408877.3432468
  abstract: >
    The prevention of criminal activity has changed dramatically over the past
    two decades, largely due to the increased reliance on systems that provide
    crime data analysis. Created specifically for police, judicial sentencing,
    and prison applications, these systems conduct both predictive and
    retrospective analysis to aid decision making within the criminal justice
    system. Furthermore, these software platforms typically combine spatial
    informatics packages and advanced statistical features behind user-friendly
    interfaces. Recent studies have demonstrated problems with both the flawed
    logic within these systems' algorithms and the inherent biases in the
    underlying data. In this paper, we present a novel repository of computing
    ethics teaching modules across a variety of narrative areas. These modules
    and curated narratives help faculty to establish 'ethical laboratories' that
    can guide computer science students in improving their ethical reasoning
    skills as it relates to the creation of current and future
    technologies. First, we provide an overview of the Computing Ethics
    Narratives (CEN) project, its narrative repository and the module framework
    through a sample module on predictive policing algorithms. Furthermore, we
    share preliminary findings from a pilot of this module, which was
    implemented in an intermediate algorithms course. The preliminary student
    and faculty feedback suggest the predictive policing module was able to help
    students contextualize the ethical issues around the topic, however,
    students recommended devoting more class time to evaluating the technical
    complexities of these critical systems.

- key: Bulmer2018
  kind: inproceedings
  author:
  - Tyson Bulmer
  - Lloyd Montgomery
  - Daniela Damian
  title: Predicting developers' IDE commands with machine learning
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196459
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196459
  abstract: >
    When a developer is writing code they are usually focused and in a
    state-of-mind which some refer to as flow. Breaking out of this flow can
    cause the developer to lose their train of thought and have to start their
    thought process from the beginning. This loss of thought can be caused by
    interruptions and sometimes slow IDE interactions. Predictive functionality
    has been harnessed in user applications to speed up load times, such as in
    Google Chrome's browser which has a feature called "Predicting Network
    Actions". This will pre-load web-pages that the user is most likely to
    click through. This mitigates the interruption that load times can
    introduce. In this paper we seek to make the first step towards predicting
    user commands in the IDE. Using the MSR 2018 Challenge Data of over 3000
    developer session and over 10 million recorded events, we analyze and
    cleanse the data to be parsed into event series, which can then be used to
    train a variety of machine learning models, including a neural network, to
    predict user induced commands. Our highest performing model is able to
    obtain a 5 cross-fold validation prediction accuracy of 64%.

- key: Butterworth2011
  kind: book
  author:
  - Alex Butterworth
  title: "The World That Never Was: A True Story of Dreamers, Schemers, Anarchists and Secret Agents"
  year: 2011
  isbn: 978-0099551928
  publisher: Vintage Books

- key: Casciaro2020
  kind: book
  author:
  - Mario Casciaro
  - Luciano Mammino
  title: Node.js Design Patterns
  edition: 3rd
  year: 2020
  isbn: 978-1839214110
  publisher: Packt
  note: A readable, up-to-date technical exploration of Node's more advanced capabilities.

- key: Cataldo2008
  kind: inproceedings
  author:
  - Marcelo Cataldo
  - James D. Herbsleb
  - Kathleen M. Carley
  title: "Socio-technical Congruence: A Framework for Assessing the Impact of Technical and Work Dependencies on Software Development Productivity"
  booktitle: ESEM'08
  year: 2008
  doi: 10.1145/1414004.1414008
  abstract: >
    The identification and management of work dependencies is a fundamental
    challenge in software development organizations. This paper argues that
    modularization, the traditional technique intended to reduce
    interdependencies among components of a system, has serious limitations in
    the context of software development. We build on the idea of congruence,
    proposed in our prior work, to examine the relationship between the
    structure of technical and work dependencies and the impact of dependencies
    on software development productivity. Our empirical evaluation of the
    congruence framework showed that when developers' coordination patterns are
    congruent with their coordination needs, the resolution time of modification
    requests was significantly reduced. Furthermore, our analysis highlights the
    importance of identifying the "right" set of technical dependencies that
    drive the coordination requirements among software developers. Call and data
    dependencies appear to have far less impact than logical dependencies.

- key: Caulfield2016
  kind: link
  author:
  - Mike Caulfield
  title: Choral Explanations
  year: 2016
  url: https://hapgood.us/2016/05/13/choral-explanations/

- key: Chacon2014
  kind: book
  author:
  - Scott Chacon
  - Ben Straub
  title: Pro Git
  edition: 2nd
  year: 2014
  isbn: 978-1484200773
  publisher: Apress

- key: Chattopadhyay2020
  kind: inproceedings
  author:
  - Souti Chattopadhyay
  - Nicholas Nelson
  - Audrey Au
  - Natalia Morales
  - Christopher Sanchez
  - Rahul Pandita
  - Anita Sarma
  title: "A Tale From the Trenches: Cognitive Biases and Software Development"
  booktitle: Proc. ICSE'20
  year: 2020
  doi: 10.1145/3377811.3380330
  abstract: >
    Cognitive biases are hard-wired behaviors that influence developer actions
    and can set them on an incorrect course of action, necessitating
    backtracking. While researchers have found that cognitive biases occur in
    development tasks in controlled lab studies, we still don't know how these
    biases affect developers' everyday behavior. Without such an understanding,
    development tools and practices remain inadequate. To close this gap, we
    conducted a 2-part field study to examine the extent to which cognitive
    biases occur, the consequences of these biases on developer behavior, and
    the practices and tools that developers use to deal with these biases. About
    70% of observed actions that were reversed were associated with at least one
    cognitive bias. Further, even though developers recognized that biases
    frequently occur, they routinely are forced to deal with such issues with ad
    hoc processes and sub-optimal tool support. As one participant (IP12)
    lamented: There is no salvation!

- key: Cherubini2007
  kind: inproceedings
  author:
  - Mauro Cherubini
  - Gina Venolia
  - Rob DeLine
  - Amy J. Ko
  title: "Let''s Go to the Whiteboard: How and Why Software Developers Use Drawings"
  booktitle: Proc. CHI'07
  year: 2007
  doi: 10.1145/1240624.1240714
  abstract: >
    Software developers are rooted in the written form of their code, yet they
    often draw diagrams representing their code. Unfortunately, we still know
    little about how and why they create these diagrams, and so there is little
    research to inform the design of visual tools to support developers'
    work. This paper presents findings from semi-structured interviews that have
    been validated with a structured survey. Results show that most of the
    diagrams had a transient nature because of the high cost of changing
    whiteboard sketches to electronic renderings. Diagrams that documented
    design decisions were often externalized in these temporary drawings and
    then subsequently lost. Current visualization tools and the software
    development practices that we observed do not solve these issues, but these
    results suggest several directions for future research.

- key: Cheryan2009
  kind: article
  author:
  - Sapna Cheryan
  - Victoria C. Plaut
  - Paul G. Davies
  - Claude M. Steele
  title: "Ambient Belonging: How Stereotypical Cues Impact Gender Participation in Computer Science"
  journal: Journal of Personality and Social Psychology
  volume: 97
  number: 6
  year: 2009
  doi: 10.1037/a0016239
  abstract: >
    People can make decisions to join a group based solely on exposure to that
    group's physical environment. Four studies demonstrate that the gender
    difference in interest in computer science is influenced by exposure to
    environments associated with computer scientists. In Study 1, simply
    changing the objects in a computer science classroom from those considered
    stereotypical of computer science (e.g., Star Trek poster, video games) to
    objects not considered stereotypical of computer science (e.g., nature
    poster, phone books) was sufficient to boost female undergraduates' interest
    in computer science to the level of their male peers. Further investigation
    revealed that the stereotypical broadcast a masculine stereotype that
    discouraged women's sense of ambient belonging and subsequent interest in
    the environment (Studies 2, 3, and 4) but had no similar effect on men
    (Studies 3, 4). This masculine stereotype prevented women's interest from
    developing even in environments entirely populated by other women (Study
    2). Objects can thus come to broadcast stereotypes of a group, which in turn
    can deter people who do not identify with these stereotypes from joining
    that group.

- key: Chi1989
  kind: article
  author:
  - Michelene T. H. Chi
  - Miriam Bassok
  - Matthew W. Lewis
  - Peter Reimann
  - Robert Glaser
  title: "Self-Explanations: How Students Study and Use Examples in Learning to Solve Problems"
  journal: Cognitive Science
  volume: 13
  number: 2
  year: 1989
  doi: 10.1207/s15516709cog1302_1
  abstract: >
    The present paper analyzes the self-generated explanations (from talk-aloud
    protocols) that “Good” ond “Poor” students produce while studying worked-out
    exomples of mechanics problems, and their subsequent reliance on examples
    during problem solving. We find that “Good” students learn with
    understanding: They generate many explanations which refine and expand the
    conditions for the action ports of the exomple solutions, ond relate these
    actions to principles in the text. These self-explanations are guided by
    accurate monitoring of their own understanding and misunderstanding. Such
    learning results in example-independent knowledge and in a better
    understanding of the principles presented in the text. “Poor” students do
    not generate sufficient self-explonations, monitor their learning
    inaccurately, and subsequently rely heovily an examples. We then discuss the
    role of self-explanations in facilitating problem solving, as well OS the
    adequacy of current Al models of explanation-based learning to account for
    these psychological findings.

- key: Ciborowska2018
  kind: inproceedings
  author:
  - Agnieszka Ciborowska
  - Nicholas A. Kraft
  - Kostadin Damevski
  title: Detecting and characterizing developer behavior following opportunistic reuse of code snippets from the web
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196467
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196467
  abstract: >
    Modern software development is social and relies on many online resources
    and tools. In this paper, we study opportunistic code reuse from the Web,
    e.g. when developers copy code snippets from popular Q&A sites and paste
    them into their projects. Our focus is the behavior of developers following
    opportunistic code reuse, which reveals the success or failure of the
    action. We study developer behavior via a large, representative dataset of
    micro-interactions in the IDE. Our analysis of developer behavior exhibited
    in this dataset confirms laboratory study observations that code reuse from
    the Web is followed by heavy editing, in some cases by a rapid undo, and
    rarely by the execution of tests.

- key: Coelho2018
  kind: inproceedings
  author:
  - Jailton Coelho
  - Marco Tulio Valente
  - Luciana L. Silva
  - Emad Shihab
  title: Identifying unmaintained projects in github
  booktitle: Proc. ESEM'18
  year: 2018
  doi: 10.1145/3239235.3240501
  publisher: ACM Press
  url: https://doi.org/10.1145/3239235.3240501
  abstract: >
    Background: Open source software has an increasing importance in modern
    software development. However, there is also a growing concern on the
    sustainability of such projects, which are usually managed by a small number
    of developers, frequently working as volunteers. Aims: In this paper, we
    propose an approach to identify GitHub projects that are not actively
    maintained. Our goal is to alert users about the risks of using these
    projects and possibly motivate other developers to assume the maintenance of
    the projects. Method: We train machine learning models to identify
    unmaintained or sparsely maintained projects, based on a set of features
    about project activity (commits, forks, issues, etc). We empirically
    validate the model with the best performance with the principal developers
    of 129 GitHub projects. Results: The proposed machine learning approach has
    a precision of 80%, based on the feedback of real open source developers;
    and a recall of 96%. We also show that our approach can be used to assess
    the risks of projects becoming unmaintained. Conclusions: The model proposed
    in this paper can be used by open source users and developers to identify
    GitHub projects that are not actively maintained anymore.

- key: Cohen2010
  kind: incollection
  author:
  - Jason Cohen
  title: Modern Code Review
  booktitle: Making Software
  editor:
  - Andy Oram
  - Greg Wilson
  year: 2010
  isbn: 978-0596808327
  publisher: O'Reilly

- key: Cohen2018
  kind: link
  author:
  - Noam Cohen
  title: After Years of Abusive E-mails, the Creator of Linux Steps Aside
  year: 2018
  url: https://www.newyorker.com/science/elements/after-years-of-abusive-e-mails-the-creator-of-linux-steps-aside

- key: Cohen2021
  kind: inproceedings
  author:
  - Lena Cohen
  - Heila Precel
  - Harold Triedman
  - Kathi Fisler
  title: A New Model for Weaving Responsible Computing Into Courses Across the CS Curriculum
  booktitle: Proc. ICSE'21
  year: 2021
  doi: 10.1145/3408877.3432456
  abstract: >
    CS departments in the USA have used various models for teaching about
    ethics, including standalone ethics courses and expert-designed
    assignments. At Brown University, we are trying a different approach: a
    group of undergraduate teaching assistants dedicated to socially-responsible
    practices in computing work with faculty to integrate content into multiple
    assignments both across a course and across the curriculum. This
    "responsible computing" initiative has resulted in a variety of
    assignments added to 13 different courses in the past year. This paper
    describes the program's design, sample assignments, results of an internal
    evaluation, and refinements we are making to the model. We contrast our
    model to others from the literature in hopes of expanding the collection of
    curricular ideas for designing education in responsible computing.

- key: Conery2021
  kind: book
  author:
  - Rob Conery
  title: "The Imposter''s Handbook: A CS Primer for Self-Taught Developers"
  year: 2021
  isbn: 979-8708185266
  publisher: Independently published
  note: An overview of computer science theory for people who didn't do computer science in school.

- key: Conway1968
  kind: article
  author:
  - Melvin E. Conway
  title: How do Committees Invent
  journal: Datamation
  volume: 14
  number: 4
  year: 1968

- key: Cook2019
  kind: link
  author:
  - Mary Rose Cook
  title: Gitlet
  year: 2016
  url: http://gitlet.maryrosecook.com/

- key: CriadoPerez2019
  kind: book
  author:
  - Caroline Criado Perez
  title: "Invisible Women: Data Bias in a World Designed for Men"
  year: 2019
  isbn: 978-1419729072
  publisher: Harry N. Abrams

- key: Csikszentmihalyi1991
  kind: book
  author:
  - Mihaly Csikszentmihalyi
  title: "Flow: The Psychology of Optimal Experience"
  year: 1991
  isbn: 978-0060920432
  publisher: Harper

- key: Dacosta2017
  kind: article
  author:
  - Daniel Alencar da Costa
  - Shane McIntosh
  - "Uirá Kulesza"
  - Ahmed E. Hassan
  - Surafel Lemma Abebe
  title: An empirical study of the integration time of fixed issues
  journal: ESE
  volume: 23
  number: 1
  pages: 334--383
  month: '5'
  year: 2017
  doi: 10.1007/s10664-017-9520-6
  publisher: Springer Science and Business Media LLC
  url: https://doi.org/10.1007/s10664-017-9520-6
  abstract: >
    Predicting the required time to fix an issue (i.e., a new feature, bug fix,
    or enhancement) has long been the goal of many software engineering
    researchers. However, after an issue has been fixed, it must be integrated
    into an official release to become visible to users. In theory, issues
    should be quickly integrated into releases after they are fixed. However, in
    practice, the integration of a fixed issue might be prevented in one or more
    releases before reaching users. For example, a fixed issue might be
    prevented from integration in order to assess the impact that this fixed
    issue may have on the system as a whole. While one can often speculate, it
    is not always clear why some fixed issues are integrated immediately, while
    others are prevented from integration. In this paper, we empirically study
    the integration of 20,995 fixed issues from the ArgoUML, Eclipse, and
    Firefox projects. Our results indicate that: (i) despite being fixed well
    before the release date, the integration of 34% to 60% of fixed issues in
    projects with traditional release cycle (the Eclipse and ArgoUML projects),
    and 98% of fixed issues in a project with a rapid release cycle (the Firefox
    project) was prevented in one or more releases; (ii) using information that
    we derive from fixed issues, our models are able to accurately predict the
    release in which a fixed issue will be integrated, achieving Areas Under the
    Curve (AUC) values of 0.62 to 0.93; and (iii) heuristics that estimate the
    effort that the team invests to fix issues is one of the most influential
    factors in our models. Furthermore, we fit models to study fixed issues that
    suffer from a long integration time. Such models, (iv) obtain AUC values of
    0.82 to 0.96 and (v) derive much of their explanatory power from metrics
    that are related to the release cycle. Finally, we train regression models
    to study integration time in terms of number of days. Our models achieve R2
    values of 0.39 to 0.65, and indicate that the time at which an issue is
    fixed and the resolver of the issue have a large impact on the number of
    days that a fixed issue requires for integration. Our results indicate that,
    in addition to the backlog of issues that need to be fixed, the backlog of
    issues that need to be released introduces a software development overhead,
    which may lead to a longer integration time. Therefore, in addition to
    studying the triaging and fixing stages of the issue lifecycle, the
    integration stage should also be the target of future research and tooling
    efforts in order to reduce the time-to-delivery of fixed issues.

- key: Davidson2009
  kind: book
  author:
  - Neil Davidson
  title: Don't Just Roll the Dice - A Usefully Short Guide to Software Pricing
  year: 2009
  isbn: 978-1906434380
  publisher: Red Gate Books

- key: Davis2018
  kind: book
  author:
  - Ashley Davis
  title: Data Wrangling with JavaScript
  year: 2018
  isbn: 978-1617294846
  publisher: Manning
  url: https://www.manning.com/books/data-wrangling-with-javascript
  note: A step-by-step guide to managing data with JavaScript.

- key: Dawson2010
  kind: article
  author:
  - Maurice Dawson
  - Darrell Burrell
  - Emad Rahim
  - Stephen Brewster
  title: Integrating Software Assurance into the Software Development Life Cycle
  journal: Information Systems Technology and Planning
  volume: 3
  number: 6
  year: 2010

- key: DeBruyckere2015
  kind: book
  author:
  - Pedro De Bruyckere
  - Paul A. Kirschner
  - Casper D. Hulshof
  title: Urban Myths about Learning and Education
  year: 2015
  isbn: 978-0128015377
  publisher: Academic Press

- key: DeOliveiraNeto2019
  kind: article
  author:
  - Francisco Gomes de Oliveira Neto
  - Richard Torkar
  - Robert Feldt
  - Lucas Gren
  - Carlo A. Furia
  - Ziwei Huang
  title: "Evolution of Statistical Analysis in Empirical Software Engineering Research: Current State and Steps Forward"
  journal: JSS
  volume: 156
  year: 2019
  doi: 10.1016/j.jss.2019.07.002
  abstract: >
    Software engineering research is evolving and papers are increasingly based
    on empirical data from a multitude of sources, using statistical tests to
    determine if and to what degree empirical evidence supports their
    hypotheses. To investigate the practices and trends of statistical analysis
    in empirical software engineering (ESE), this paper presents a review of a
    large pool of papers from top-ranked software engineering journals. First,
    we manually reviewed 161 papers and in the second phase of our method, we
    conducted a more extensive semi-automatic classification of papers spanning
    the years 2001--2015 and 5,196 papers. Results from both review steps was
    used to: i) identify and analyze the predominant practices in ESE (e.g.,
    using t-test or ANOVA), as well as relevant trends in usage of specific
    statistical methods (e.g., nonparametric tests and effect size measures)
    and, ii) develop a conceptual model for a statistical analysis workflow with
    suggestions on how to apply different statistical methods as well as
    guidelines to avoid pitfalls. Lastly, we confirm existing claims that
    current ESE practices lack a standard to report practical significance of
    results. We illustrate how practical significance can be discussed in terms
    of both the statistical analysis and in the practitioner's context.

- key: DiBella2013
  kind: article
  author:
  - Enrico di Bella
  - Ilenia Fronza
  - Nattakarn Phaphoom
  - Alberto Sillitti
  - Giancarlo Succi
  - Jelena Vlasenko
  title: "Pair Programming and Software Defects: A Large, Industrial Case Study"
  journal: TSE
  volume: 39
  number: 7
  year: 2013
  doi: 10.1109/TSE.2012.68
  abstract: >
    In the last decade, there has been increasing interest in pair programming
    (PP). However, despite the existing work, there is still a lack of
    substantial evidence of the effects of PP in industrial environments. To
    address this issue, we have analyzed the work of a team of 17 industrial
    developers for 14 months. The team is part of the IT department of a large
    Italian manufacturing company; it adopts a customized version of extreme
    programming (XP). We have investigated the effects of PP on software quality
    in five different scenarios. The results show that PP appears to provide a
    perceivable but small effect on the reduction of defects in these settings.

- key: Didau2016
  kind: book
  author:
  - David Didau
  - Nick Rose
  title: What Every Teacher Needs to Know About Psychology
  year: 2016
  isbn: 978-1909717855
  publisher: John Catt Educational

- key: Diehm2018
  kind: link
  author:
  - Jan Diehm
  - Amber Thomas
  title: Pockets
  year: 2019
  url: https://pudding.cool/2018/08/pockets/

- key: Dobbin2019
  kind: article
  author:
  - Frank Dobbin
  - Alexandra Kalev
  title: The Promise and Peril of Sexual Harassment Programs
  journal: PNAS
  volume: 116
  number: 25
  year: 2019
  doi: 10.1073/pnas.1818477116
  url: https://www.pnas.org/content/116/25/12255.abstract
  abstract: >
    Significance Do corporate sexual harassment programs reduce harassment?
    Those that do should boost the share of women in management, because
    harassment causes women to quit. Sexual harassment grievance procedures
    incite retaliation, according to surveys, and our analyses show that they
    are followed by reductions in women managers. Sexual harassment training for
    managers, which treats managers as victims’ allies and gives them tools to
    intervene, are followed by increases in women managers. Training for
    employees, which treats trainees as suspects, can backfire. Programs work
    better in workplaces with more women managers, who are less likely than men
    to respond negatively to harassment complaints and training. Employers
    should select managers—men and women—committed to eradicating
    harassment. Two decades ago, the Supreme Court vetted the workplace
    harassment programs popular at the time: sexual harassment grievance
    procedures and training. However, harassment at work remains common. Do
    these programs reduce harassment? Program effects have been difficult to
    measure, but, because women frequently quit their jobs after being harassed,
    programs that reduce harassment should help firms retain current and
    aspiring women managers. Thus, effective programs should be followed by
    increases in women managers. We analyze data from 805 companies over 32 y to
    explore how new sexual harassment programs affect the representation of
    white, black, Hispanic, and Asian-American women in management. We find
    support for several propositions. First, sexual harassment grievance
    procedures, shown in surveys to incite retaliation without satisfying
    complainants, are followed by decreases in women managers. Second, training
    for managers, which encourages managers to look for signs of trouble and
    intervene, is followed by increases in women managers. Third, employee
    training, which proscribes specific behaviors and signals that male trainees
    are potential perpetrators, is followed by decreases in women managers. Two
    propositions specify how management composition moderates program
    effects. One, because women are more likely to believe harassment complaints
    and less likely to respond negatively to training, in firms with more women
    managers, programs work better. Two, in firms with more women managers,
    harassment programs may activate group threat and backlash against some
    groups of women. Positive and negative program effects are found in
    different sorts of workplaces.

- key: Dobbin2020
  kind: link
  author:
  - Frank Dobbin
  - Alexandra Kalev
  title: Why Sexual Harassment Programs Backfire
  year: 2020
  url: https://hbr.org/2020/05/why-sexual-harassment-programs-backfire

- key: Donovan2018
  kind: article
  author:
  - Deborah A. Donovan
  - Georgianne L. Connell
  - Daniel Z. Grunspan
  title: Student Learning Outcomes and Attitudes Using Three Methods of Group Formation in a Nonmajors Biology Class
  journal: CBE---Life Sciences Education
  volume: 17
  number: 4
  year: 2018
  doi: 10.1187/cbe.17-12-0283
  abstract: >
    Group work is often a key component of student-centered pedagogies, but
    there is conflicting evidence about what types of groups provide the most
    benefit for undergraduate students. We investigated student learning
    outcomes and attitudes toward working in groups when students were assigned
    to groups using different methods in a large-enrollment, student-centered
    class. We were particularly interested in how students entering the class
    with different levels of competence in biology performed in homogeneous or
    heterogeneous groups, and what types of group compositions were formed using
    different methods of group formation. We found that low-competence students
    had higher learning outcomes when they were in heterogeneous groups, while
    mid- and high-competence students performed equally well in both group
    types. Students of all competence types had better attitudes toward group
    work in heterogeneous groups. The use of student demographic variables to
    preemptively form groups and allowing students to self-select their group
    mates both yielded heterogeneous competence groups. Students in the
    instructor-formed, demographic groups had higher learning outcomes compared
    with students allowed to self-select. Thus, heterogeneous groupings provided
    the most benefit for students in our nonmajors, large-enrollment class.

- key: Easttom2019
  kind: book
  author:
  - Chuck Easttom
  title: Computer Security Fundamentals
  year: 2019
  isbn: 978-0135774779
  publisher: Pearson

- key: Edwards2009
  kind: inproceedings
  author:
  - Stephen H. Edwards
  - Jason Snyder
  - "Manuel A. Pérez-Quiñones"
  - Anthony Allevato
  - Dongkwan Kim
  - Betsy Tretola
  title: Comparing Effective and Ineffective Behaviors of Student Programmers
  booktitle: Proc. ICER'09
  year: 2009
  doi: 10.1145/1584322.1584325
  abstract: >
    This paper reports on a quantitative evaluation of five years of data
    collected in the first three programming courses at Virginia Tech. The
    dataset involves a total of 89,879 assignment submissions by 1,101 different
    students. Assignment results were partitioned into two groups: scores above
    80% (A/B) and scores below 80% (C/D/F). To investigate student behaviors
    that result in differing levels of achievement, all students who
    consistently received A/B scores and all students who consistently received
    C/D/F scores were removed from the dataset. A within-subjects comparison of
    the scores received by the remaining individuals was performed. Further,
    time and code-size data that is difficult to compare directly between
    different courses was normalized.
    This study revealed several significant results. When students received A/B
    scores, they started earlier and finished earlier than when the same
    students received C/D/F scores. They also wrote slightly more program
    code. They did not appear to spend any more time on their work,
    however. Approximately two-thirds of the A/B scores were received by
    individuals who started more than a day in advance of the deadline, while
    approximately two-thirds of the C/D/F scores were received by individuals
    who started on the last day or later. One possible explanation is that
    students who start earlier simply have more time to seek assistance when
    they get stuck.

- key: Eichberg2015
  kind: inproceedings
  author:
  - Michael Eichberg
  - Ben Hermann
  - Mira Mezini
  - Leonid Glanz
  title: Hidden Truths in Dead Software Paths
  booktitle: Proc. FSE'15
  year: 2015
  doi: 10.1145/2786805.2786865
  abstract: >
    Approaches and techniques for statically finding a multitude of issues in
    source code have been developed in the past. A core property of these
    approaches is that they are usually targeted towards finding only a very
    specific kind of issue and that the effort to develop such an analysis is
    significant. This strictly limits the number of kinds of issues that can be
    detected. In this paper, we discuss a generic approach based on the
    detection of infeasible paths in code that can discover a wide range of code
    smells ranging from useless code that hinders comprehension to real
    bugs. Code issues are identified by calculating the difference between the
    control-flow graph that contains all technically possible edges and the
    corresponding graph recorded while performing a more precise analysis using
    abstract interpretation. We have evaluated the approach using the Java
    Development Kit as well as the Qualitas Corpus (a curated collection of over
    100 Java Applications) and were able to find thousands of issues across a
    wide range of categories.

- key: Elbaum1998
  kind: inproceedings
  author:
  - Sebastian G. Elbaum
  - John C. Munson
  title: "Code churn: A measure for estimating the impact of code change"
  booktitle: Proc. ICSM'98
  year: 1998
  url: http://dl.acm.org/citation.cfm?id=850947.853326

- key: ElEmam2001
  kind: article
  author:
  - Khaled El Emam
  - "Saïda Benlarbi"
  - Nishith Goel
  - Shesh N. Rai
  title: The Confounding Effect of Class Size on the Validity of Object-Oriented Metrics
  journal: TSE
  volume: 27
  number: 7
  year: 2001
  doi: 10.1109/32.935855
  abstract: >
    Much effort has been devoted to the development and empirical validation of
    object-oriented metrics. The empirical validations performed thus far would
    suggest that a core set of validated metrics is close to being
    identified. However, none of these studies allow for the potentially
    confounding effect of class size. We demonstrate a strong size confounding
    effect and question the results of previous object-oriented metrics
    validation studies. We first investigated whether there is a confounding
    effect of class size in validation studies of object-oriented metrics and
    show that, based on previous work, there is reason to believe that such an
    effect exists. We then describe a detailed empirical methodology for
    identifying those effects. Finally, we perform a study on a large C++
    telecommunications framework to examine if size is really a confounder. This
    study considered the Chidamber and Kemerer metrics and a subset of the
    Lorenz and Kidd metrics. The dependent variable was the incidence of a fault
    attributable to a field failure (fault-proneness of a class). Our findings
    indicate that, before controlling for size, the results are very similar to
    previous studies. The metrics that are expected to be validated are indeed
    associated with fault-proneness.

- key: Eley1995
  kind: book
  author:
  - Joanna Eley
  - Alexi Marmot
  title: "Understanding Offices: What Every Manager Needs to Know about Office Buildings"
  year: 1995
  isbn: 978-0140169126
  publisher: Penguin

- key: Endrikat2014
  kind: inproceedings
  author:
  - Stefan Endrikat
  - Stefan Hanenberg
  - Romain Robbes
  - Andreas Stefik
  title: How do API documentation and static typing affect API usability?
  booktitle: Proc. ICSE'14
  year: 2014
  doi: 10.1145/2568225.2568299
  publisher: ACM Press
  url: https://doi.org/10.1145/2568225.2568299
  abstract: >
    When developers use Application Programming Interfaces (APIs), they often
    rely on documentation to assist their tasks. In previous studies, we
    reported evidence indicating that static type systems acted as a form of
    implicit documentation, benefiting developer productivity. Such implicit
    documentation is easier to maintain, given it is enforced by the compiler,
    but previous experiments tested users without any explicit documentation. In
    this paper, we report on a controlled experiment and an exploratory study
    comparing the impact of using documentation and a static or dynamic type
    system on a development task. Results of our study both confirm previous
    findings and show that the benefits of static typing are strengthened with
    explicit documentation, but that this was not as strongly felt with
    dynamically typed languages.

- key: Ensmenger2012
  kind: book
  author:
  - Nathan L. Ensmenger
  title: "The Computer Boys Take Over: Computers, Programmers, and the Politics of Technical Expertise"
  year: 2012
  isbn: 978-0262517966
  publisher: MIT Press

- key: Erdogmus2005
  kind: article
  author:
  - Hakan Erdogmus
  - Maurizio Morisio
  - Marco Torchiano
  title: On the Effectiveness of Test-First Approach to Programming
  journal: TSE
  volume: 31
  number: 3
  year: 2005
  doi: 10.1109/TSE.2005.37
  abstract: >
    Test-driven development (TDD) is based on formalizing a piece of
    functionality as a test, implementing the functionality such that the test
    passes, and iterating the process. This paper describes a controlled
    experiment for evaluating an important aspect of TDD: in TDD, programmers
    write functional tests before the corresponding implementation code. The
    experiment was conducted with undergraduate students. While the experiment
    group applied a test-first strategy, the control group applied a more
    conventional development technique, writing tests after the
    implementation. Both groups followed an incremental process, adding new
    features one at a time and regression testing them. We found that test-first
    students on average wrote more tests and, in turn, students who wrote more
    tests tended to be more productive. We also observed that the minimum
    quality increased linearly with the number of programmer tests, independent
    of the development strategy employed.

- key: Eubanks2019
  kind: book
  author:
  - Virginia Eubanks
  title: "Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor"
  year: 2019
  isbn: 978-1250215789
  publisher: Picador

- key: Fagerholm2017
  kind: article
  author:
  - Fabian Fagerholm
  - Marco Kuhrmann
  - "Jürgen Münch"
  title: Guidelines for Using Empirical Studies in Software Engineering Education
  journal: PeerJ Computer Science
  volume: 3
  year: 2017
  doi: 10.7717/peerj-cs.131
  abstract: >
    Software engineering education is under constant pressure to provide
    students with industry-relevant knowledge and skills. Educators must address
    issues beyond exercises and theories that can be directly rehearsed in small
    settings. Industry training has similar requirements of relevance as
    companies seek to keep their workforce up to date with technological
    advances. Real-life software development often deals with large,
    software-intensive systems and is influenced by the complex effects of
    teamwork and distributed software development, which are hard to demonstrate
    in an educational environment. A way to experience such effects and to
    increase the relevance of software engineering education is to apply
    empirical studies in teaching. In this paper, we show howdifferent types of
    empirical studies can be used for educational purposes in software
    engineering. We give examples illustrating how to utilize empirical studies,
    discuss challenges, and derive an initial guideline that supports teachers
    to include empirical studies in software engineering courses. Furthermore,
    we give examples that show how empirical studies contribute to high-quality
    learning outcomes, to student motivation, and to the awareness of the
    advantages of applying software engineering principles. Having awareness,
    experience, and understanding of the actions required, students are more
    likely to apply such principles under real-life constraints in their working
    life. Subjects Computer Education, Software Engineering

- key: Farland2019
  kind: article
  author:
  - Michelle Z. Farland
  - Xiaoying Feng
  - Linda S. Behar-Horenstein
  - Diane E. Beck
  title: Impact of Team Formation Method on Student Team Performance Across Multiple Courses Incorporating Team-based Learning
  journal: American Journal of Pharmaceutical Education
  volume: 83
  number: 6
  year: 2019
  doi: 10.5688/ajpe7030
  abstract: >
    Objective. To assess the impact of forming student learning teams based on
    problem solving styles on team performance and student perceptions of team
    quality. Methods. This was a prospective observational study involving
    students in the first year of a Doctor of Pharmacy degree
    program. Collaborative learning teams (balanced, implementer, optimizer, and
    random assignment) were created based on students’ results on the Basadur
    Creative Problem Solving Profile Inventory. The teams remained in place
    across all courses for the first academic year, and those courses that
    incorporated team-based learning (TBL) were included in the study. Team
    performance was assessed by administering team readiness assurance
    tests. The quality of team interactions was assessed using the team
    satisfaction domain in the Comprehensive Assessment of Team Member
    Effectiveness (CATME) Smarter Teamwork system and the Team Performance
    Scale. Results. Each of the 237 first-year pharmacy students enrolled was
    assigned to one of 41 teams. All teams participated in the study. A
    significant difference in team performance was observed in the Principles of
    Patient Centered Care course but not in any of the other courses. No
    significant differences were found in quality of team
    interactions. Conclusion. Neither team performance, nor team satisfaction,
    nor quality of team interactions was impacted by the method of team
    formation that was used. Given the existing evidence and the results of this
    study, team formation process, regardless of method used, may have
    negligible influence on the performance of collaborative learning teams in
    courses taught using TBL.

- key: Farmer2006
  kind: article
  author:
  - Eugene Farmer
  title: The Gatekeeper's Guide,  or How to Kill a Tool
  journal: IEEE Software
  volume: 23
  number: 6
  year: 2006
  doi: 10.1109/ms.2006.174
  abstract: >
    Over the past few decades, new tools that facilitate the cost-effective
    production of high-quality software have slowly gained ground. The following
    guidelines show how you can push any tool to the side, perpetuating the
    manual methods that have always worked in the past

- key: Feathers2004
  kind: book
  author:
  - Michael C. Feathers
  title: Working Effectively with Legacy Code
  year: 2004
  isbn: 978-0131177055
  publisher: Prentice-Hall

- key: Felidre2019
  kind: inproceedings
  author:
  - Wagner Felidre
  - Leonardo Furtado
  - Daniel A. da Costa
  - Bruno Cartaxo
  - Gustavo Pinto
  title: Continuous Integration Theater
  booktitle: Proc. ESEM'19
  month: '9'
  year: 2019
  doi: 10.1109/esem.2019.8870152
  publisher: IEEE
  url: https://doi.org/10.1109/esem.2019.8870152
  abstract: >
    Background: Continuous Integration (CI) systems are now the bedrock of
    several software development practices. Several tools such as TravisCI,
    CircleCI, and Hudson, that implement CI practices, are commonly adopted by
    software engineers. However, the way that software engineers use these tools
    could lead to what we call “Continuous Integration Theater”, a situation in
    which software engineers do not employ these tools effectively, leading to
    unhealthy CI practices. Aims: The goal of this paper is to make sense of how
    commonplace are these unhealthy continuous integration practices being
    employed in practice. Method: By inspecting 1,270 open-source projects that
    use TravisCI, the most used CI service, we quantitatively studied how common
    is to use CI (1) with infrequent commits, (2) in a software project with
    poor test coverage, (3) with builds that stay broken for long periods, and
    (4) with builds that take too long to run. Results: We observed that 748
    (~60%) projects face infrequent commits, which essentially makes the merging
    process harder. Moreover, we were able to find code coverage information for
    51 projects. The average code coverage was 78%, although Ruby projects have
    a higher code coverage than Java projects (86% and 63%, respectively).
    However, some projects with very small coverage (~4%) were found. Still, we
    observed that 85% of the studied projects have at least one broken build
    that take more than four days to be fixed. Interestingly, very small
    projects (up to 1,000 lines of code) are the ones that take the longest to
    fix broken builds. Finally, we noted that, for the majority of the studied
    projects, the build is executed under the 10 minutes rule of
    thumb. Conclusions: Our results are important to an increasing community of
    software engineers that employ CI practices on daily basis but may not be
    aware of bad practices that are eventually employed.

- key: Ferreira2021
  kind: inproceedings
  author:
  - Rodrigo Ferreira
  - Moshe Y. Vardi
  title: "Deep Tech Ethics: An Approach to Teaching Social Justice in Computer Science"
  booktitle: Proc. ICSE'21
  year: 2021
  doi: 10.1145/3408877.3432449
  abstract: >
    As ethical questions around the development of contemporary computer
    technologies have become an increasing point of public and political
    concern, computer science departments in universities around the world have
    placed renewed emphasis on tech ethics undergraduate classes as a means to
    educate students on the large-scale social implications of their
    actions. Committed to the idea that tech ethics is an essential part of the
    undergraduate computer science educational curriculum, at Rice University
    this year we piloted a redesigned version of our Ethics and Accountability
    in Computer Science class. This effort represents our first attempt at
    implementing a "deep" tech ethics approach to the course. Incorporating
    elements from philosophy of technology, critical media theory, and science
    and technology studies, we encouraged students to learn not only ethics in a
    "shallow" sense, examining abstract principles or values to determine
    right and wrong, but rather looking at a series of "deeper" questions more
    closely related to present issues of social justice and relying on a
    structural understanding of these problems to develop potential
    sociotechnical solutions. In this article, we report on our implementation
    of this redesigned approach. We describe in detail the rationale and
    strategy for implementing this approach, present key elements of the
    redesigned syllabus, and discuss final student reflections and course
    evaluations. To conclude, we examine course achievements, limitations, and
    lessons learned toward the future, particularly in regard to the number
    escalating social protests and issues involving Covid-19.

- key: Fincher2001
  kind: book
  author:
  - Sally Fincher
  - Marian Petre
  - Martyn Clarke
  title: "Computer Science Project Work: Principles and Pragmatics"
  year: 2001
  isbn: 978-1852333577
  publisher: Springer

- key: Fischer2015
  kind: inproceedings
  author:
  - Lars Fischer
  - Stefan Hanenberg
  title: An Empirical Investigation of the Effects of Type Systems and Code Completion on API Usability Using TypeScript and JavaScript in MS Visual Studio
  booktitle: Proc. DLS'15
  year: 2015
  doi: 10.1145/2816707.2816720
  abstract: >
    Recent empirical studies that compared static and dynamic type systems on
    API usability showed a positive impact of static type systems on developer
    productivity in most cases. Nevertheless, it is unclear how large this
    effect is in comparison to other factors. One obvious factor in programming
    is tooling: It is commonly accepted that modern IDEs have a large positive
    impact on developers, although it is not clear which parts of modern IDEs
    are responsible for that. One possible---and for most developers obvious
    candidate---is code completion. This paper describes a 2x2 randomized trial
    that compares JavaScript and Microsoft's statically typed alternative
    TypeScript with and without code completion in MS Visual Studio. While the
    experiment shows (in correspondence to previous experiments) a large
    positive effect of the statically typed language TypeScript, the code
    completion effect is not only marginal, but also just approaching
    statistical significance. This seems to be an indicator that the effect of
    static type systems is larger than often assumed, at least in comparison to
    code completion.

- key: Floyd2017
  kind: inproceedings
  author:
  - Benjamin Floyd
  - Tyler Santander
  - Westley Weimer
  title: "Decoding the Representation of Code in the Brain: An fMRI Study of Code Review and Expertise"
  booktitle: Proc. ICSE'17
  year: 2017
  doi: 10.1109/ICSE.2017.24
  abstract: >
    Subjective judgments in software engineering tasks are of critical
    importance but can be difficult to study with conventional means. Medical
    imaging techniques hold the promise of relating cognition to physical
    activities and brain structures. In a controlled experiment involving 29
    participants, we examine code comprehension, code review and prose review
    using functional magnetic resonance imaging. We find that the neural
    representations of programming languages vs. natural languages are
    distinct. We can classify which task a participant is undertaking based
    solely on brain activity (balanced accuracy 79%, p

- key: Fogel2005
  kind: book
  author:
  - Karl Fogel
  title: Producing Open Source Software
  year: 2005
  isbn: 978-0596007591
  publisher: O'Reilly
  url: https://producingoss.com/

- key: Fogel2020
  kind: link
  author:
  - Karl Fogel
  title: Producing Open Source Software (updated)
  year: 2020
  url: https://producingoss.com/

- key: Ford2016
  kind: inproceedings
  author:
  - Denae Ford
  - Justin Smith
  - Philip J. Guo
  - Chris Parnin
  title: "Paradise unplugged: identifying barriers for female participation on stack overflow"
  booktitle: Proc. FSE'16
  year: 2016
  doi: 10.1145/2950290.2950331
  publisher: ACM Press
  url: https://doi.org/10.1145/2950290.2950331
  abstract: >
    It is no secret that females engage less in programming fields than
    males. However, in online communities, such as Stack Overflow, this gender
    gap is even more extreme: only 5.8% of contributors are female. In this
    paper, we use a mixed-methods approach to identify contribution barriers
    females face in online communities. Through 22 semi-structured interviews
    with a spectrum of female users ranging from non-contributors to a top 100
    ranked user of all time, we identified 14 barriers preventing them from
    contributing to Stack Overflow. We then conducted a survey with 1470 female
    and male developers to confirm which barriers are gender related or general
    problems for everyone. Females ranked five barriers significantly higher
    than males. A few of these include doubts in the level of expertise needed
    to contribute, feeling overwhelmed when competing with a large number of
    users, and limited awareness of site features. Still, there were other
    barriers that equally impacted all Stack Overflow users or affected
    particular groups, such as industry programmers. Finally, we describe
    several implications that may encourage increased participation in the Stack
    Overflow community across genders and other demographics.

- key: Forsgren2018
  kind: book
  author:
  - Nicole Forsgren
  - Jez Humble
  - Gene Kim
  title: "Accelerate: Building and Scaling High Performing Technology Organizations"
  year: 2018
  isbn: 978-1942788331
  publisher: IT Revolution Press

- key: Forsgren2021
  kind: article
  author:
  - Nicole Forsgren
  - Margaret-Anne Storey
  - Chandra Maddila
  - Thomas Zimmermann
  - Brian Houck
  - Jenna Butler
  title: The SPACE of Developer Productivity
  journal: ACM Queue
  volume: 19
  number: 1
  year: 2021
  doi: 10.1145/3454122.3454124
  abstract: >
    Developer productivity is about more than an individual's activity levels or
    the efficiency of the engineering systems relied on to ship software, and it
    cannot be measured by a single metric or dimension. The SPACE framework
    captures different dimensions of productivity, and here we demonstrate how
    this framework can be used to understand productivity in practice and why
    using it will help teams better understand developer productivity and create
    better measures to inform their work and teams.

- key: Fowler2018
  kind: book
  author:
  - Martin Fowler
  title: Refactoring
  edition: 2nd
  year: 2018
  isbn: 978-0134757599
  publisher: Addison-Wesley Professional

- key: France2016
  kind: book
  author:
  - David France
  title: "How to Survive a Plague: The Inside Story of How Citizens and Science Tamed AIDS"
  year: 2016
  isbn: 978-0307700636
  publisher: Knopf

- key: Frankfurt2005
  kind: book
  author:
  - Harry G. Frankfurt
  title: On Bullshit
  year: 2005
  isbn: 978-0691122946
  publisher: Princeton University Press

- key: Frase2016
  kind: book
  author:
  - Peter Frase
  title: "Four Futures: Life After Capitalism"
  year: 2016
  isbn: 978-1781688137
  publisher: Verso

- key: Freeman1972
  kind: article
  author:
  - Jo Freeman
  title: The Tyranny of Structurelessness
  journal: The Second Wave
  volume: 2
  number: 1
  year: 1972
  doi: 10.1353/wsq.2013.0072
  abstract: >
    During the years in which the women's liberation movement has been taking
    shape, a great emphasis has been placed on what are called leader less,
    structureless groups as the main—if not sole—organizational form of the
    movement. The source of this idea was a natural reaction against the
    over-structured society in which most of us found ourselves, and the
    inevitable control this gave others over our lives, and the continual
    elitism of the Left and similar groups among those who were supposedly
    fighting this overstructuredness.

- key: Fucci2013
  kind: inproceedings
  author:
  - Davide Fucci
  - Burak Turhan
  title: A Replicated Experiment on the Effectiveness of Test-First Development
  booktitle: Proc. ESEM'13
  month: '10'
  year: 2013
  doi: 10.1109/esem.2013.15
  url: https://doi.org/10.1109/esem.2013.15
  abstract: >
    Background: Test-first development (TF) is regarded as a development
    practice that can lead to better quality of software products, as well as
    improved developer productivity. By implementing unit tests before the
    corresponding production code, the tests themselves are the main driver to
    such improvements. The role of tests on the effectiveness of TF has been
    studied in a controlled experiment by Erdogmus et al. (i.e. original
    study). Aim: Our goal is to examine the impact of test-first (TF)
    development on product quality and developer productivity, specifically the
    role that tests play in it. Method: We replicated the original study's
    controlled experiment by comparing an experimental group applying TF to a
    control group applying a test-last approach. We then carried out a
    correlation study in order to understand whether the number of tests is a
    good predictor for external quality and/or productivity. Results:
    Mann-Whitney tests did not show any significant difference between the two
    groups in terms of number of tests written (W=114.5, p=0.38), developers'
    productivity (W=90, p=0.82) and external quality (W=81.55, p=0.53). In
    addition, while a significant correlation exists between the number of tests
    and productivity (Spearman's ρ = 0.57, p<;0.001), none was found in the case
    of external quality (Spearman's ρ = 0.17, p=0.18). Conclusions: We conclude
    that TF neither improves nor deteriorates the external quality or the
    productivity when compared to the test-last approach, leaving room for other
    variables to impact the effects of TF. This replication has partially
    confirmed the findings of the original study.

- key: Fucci2016
  kind: inproceedings
  author:
  - Davide Fucci
  - Giuseppe Scanniello
  - Simone Romano
  - Martin Shepperd
  - Boyce Sigweni
  - Fernando Uyaguari
  - Burak Turhan
  - Natalia Juristo
  - Markku Oivo
  title: An External Replication on the Effects of Test-driven Development Using a Multi-site Blind Analysis Approach
  booktitle: Proc. ESEM'16
  year: 2016
  doi: 10.1145/2961111.2962592
  publisher: ACM Press
  url: https://doi.org/10.1145/2961111.2962592
  abstract: >
    Context: Test-driven development (TDD) is an agile practice claimed to
    improve the quality of a software product, as well as the productivity of
    its developers. A previous study (i.e., baseline experiment) at the
    University of Oulu (Finland) compared TDD to a test-last development (TLD)
    approach through a randomized controlled trial. The results failed to
    support the claims. Goal: We want to validate the original study results by
    replicating it at the University of Basilicata (Italy), using a different
    design. Method: We replicated the baseline experiment, using a crossover
    design, with 21 graduate students. We kept the settings and context as close
    as possible to the baseline experiment. In order to limit researchers bias,
    we involved two other sites (UPM, Spain, and Brunel, UK) to conduct blind
    analysis of the data. Results: The Kruskal-Wallis tests did not show any
    significant difference between TDD and TLD in terms of testing effort
    (p-value = .27), external code quality (p-value = .82), and developers'
    productivity (p-value = .83). Nevertheless, our data revealed a difference
    based on the order in which TDD and TLD were applied, though no carry over
    effect. Conclusions: We verify the baseline study results, yet our results
    raises concerns regarding the selection of experimental objects,
    particularly with respect to their interaction with the order in which of
    treatments are applied. We recommend future studies to survey the tasks used
    in experiments evaluating TDD. Finally, to lower the cost of replication
    studies and reduce researchers' bias, we encourage other research groups to
    adopt similar multi-site blind analysis approach described in this paper.

- key: Fucci2017
  kind: article
  author:
  - Davide Fucci
  - Hakan Erdogmus
  - Burak Turhan
  - Markku Oivo
  - Natalia Juristo
  title: "A Dissection of the Test-Driven Development Process: Does It Really Matter to Test-First or to Test-Last?"
  journal: TSE
  volume: 43
  number: 7
  pages: 597--614
  month: '7'
  year: 2017
  doi: 10.1109/tse.2016.2616877
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  url: https://doi.org/10.1109/tse.2016.2616877
  abstract: >
    Background: Test-driven development (TDD) is a technique that repeats short
    coding cycles interleaved with testing. The developer first writes a unit
    test for the desired functionality, followed by the necessary production
    code, and refactors the code. Many empirical studies neglect unique process
    characteristics related to TDD iterative nature. Aim: We formulate four
    process characteristic: sequencing, granularity, uniformity, and refactoring
    effort. We investigate how these characteristics impact quality and
    productivity in TDD and related variations. Method: We analyzed 82 data
    points collected from 39 professionals, each capturing the process used
    while performing a specific development task. We built regression models to
    assess the impact of process characteristics on quality and
    productivity. Quality was measured by functional correctness. Result:
    Quality and productivity improvements were primarily positively associated
    with the granularity and uniformity. Sequencing, the order in which test and
    production code are written, had no important influence. Refactoring effort
    was negatively associated with both outcomes. We explain the unexpected
    negative correlation with quality by possible prevalence of mixed
    refactoring. Conclusion: The claimed benefits of TDD may not be due to its
    distinctive test-first dynamic, but rather due to the fact that TDD-like
    processes encourage fine-grained, steady steps that improve focus and flow.

- key: Fucci2020
  kind: article
  author:
  - Davide Fucci
  - Giuseppe Scanniello
  - Simone Romano
  - Natalia Juristo
  title: "Need for Sleep: The Impact of a Night of Sleep Deprivation on Novice Developers'' Performance"
  journal: TSE
  volume: 46
  number: 1
  year: 2020
  doi: 10.1109/tse.2018.2834900
  abstract: >
    We present a quasi-experiment to investigate whether, and to what extent,
    sleep deprivation impacts the performance of novice software developers
    using the agile practice of test-first development (TFD). We recruited 45
    undergraduates, and asked them to tackle a programming task. Among the
    participants, 23 agreed to stay awake the night before carrying out the
    task, while 22 slept normally. We analyzed the quality (i.e., the functional
    correctness) of the implementations delivered by the participants in both
    groups, their engagement in writing source code (i.e., the amount of
    activities performed in the IDE while tackling the programming task) and
    ability to apply TFD (i.e., the extent to which a participant is able to
    apply this practice). By comparing the two groups of participants, we found
    that a single night of sleep deprivation leads to a reduction of 50 percent
    in the quality of the implementations. There is notable evidence that the
    developers’ engagement and their prowess to apply TFD are negatively
    impacted. Our results also show that sleep-deprived developers make more
    fixes to syntactic mistakes in the source code. We conclude that sleep
    deprivation has possibly disruptive effects on software development
    activities. The results open opportunities for improving developers’
    performance by integrating the study of sleep with other
    psycho-physiological factors in which the software engineering research
    community has recently taken an interest in.

- key: Gao2017
  kind: inproceedings
  author:
  - Zheng Gao
  - Christian Bird
  - Earl T. Barr
  title: "To Type or Not to Type: Quantifying Detectable Bugs in JavaScript"
  booktitle: ICSE'17
  year: 2017
  doi: 10.1109/ICSE.2017.75
  abstract: >
    JavaScript is growing explosively and is now used in large mature projects
    even outside the web domain. JavaScript is also a dynamically typed language
    for which static type systems, notably Facebook's Flow and Microsoft's
    TypeScript, have been written. What benefits do these static type systems
    provide? Leveraging JavaScript project histories, we select a fixed bug and
    check out the code just prior to the fix. We manually add type annotations
    to the buggy code and test whether Flow and TypeScript report an error on
    the buggy code, thereby possibly prompting a developer to fix the bug before
    its public release. We then report the proportion of bugs on which these
    type systems reported an error. Evaluating static type systems against
    public bugs, which have survived testing and review, is conservative: it
    understates their effectiveness at detecting bugs during private
    development, not to mention their other benefits such as facilitating code
    search/completion and serving as documentation. Despite this uneven playing
    field, our central finding is that both static type systems find an
    important percentage of public bugs: both Flow 0.30 and TypeScript 2.0
    successfully detect 15%!.

- key: Gaucher2011
  kind: article
  author:
  - Danielle Gaucher
  - Justin Friesen
  - Aaron C. Kay
  title: Evidence that Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality
  journal: Journal of Personality and Social Psychology
  volume: 101
  number: 1
  year: 2011
  doi: 10.1037/a0022530
  abstract: >
    Social dominance theory (Sidanius & Pratto, 1999) contends that
    institutional-level mechanisms exist that reinforce and perpetuate existing
    group-based inequalities, but very few such mechanisms have been empirically
    demonstrated. We propose that gendered wording (i.e., masculine- and
    feminine-themed words, such as those associated with gender stereotypes) may
    be a heretofore unacknowledged, institutional-level mechanism of inequality
    maintenance. Employing both archival and experimental analyses, the present
    research demonstrates that gendered wording commonly employed in job
    recruitment materials can maintain gender inequality in traditionally
    male-dominated occupations. Studies 1 and 2 demonstrated the existence of
    subtle but systematic wording differences within a randomly sampled set of
    job advertisements. Results indicated that job advertisements for
    male-dominated areas employed greater masculine wording (i.e., words
    associated with male stereotypes, such as leader, competitive, dominant)
    than advertisements within female-dominated areas. No difference in the
    presence of feminine wording (i.e., words associated with female
    stereotypes, such as support, understand, interpersonal) emerged across
    male- and female-dominated areas. Next, the consequences of highly masculine
    wording were tested across 3 experimental studies. When job advertisements
    were constructed to include more masculine than feminine wording,
    participants perceived more men within these occupations (Study 3), and
    importantly, women found these jobs less appealing (Studies 4 and
    5). Results confirmed that perceptions of belongingness (but not perceived
    skills) mediated the effect of gendered wording on job appeal (Study 5). The
    function of gendered wording in maintaining traditional gender divisions,
    implications for gender parity, and theoretical models of inequality are
    discussed.

- key: Gavett2017
  kind: link
  author:
  - Gretchen Gavett
  title: What Research Tells Us About How Women Are Treated at Work
  year: 2017
  url: https://hbr.org/2017/12/what-research-tells-us-about-how-women-are-treated-at-work

- key: Gawande2011
  kind: book
  author:
  - Atul Gawande
  title: "The Checklist Manifesto: How to Get Things Right"
  year: 2011
  isbn: 978-0312430009
  publisher: Picador

- key: Ghattas2020
  kind: book
  author:
  - Kim Ghattas
  title: "Black Wave: Saudi Arabia, Iran, and the Forty-Year Rivalry That Unraveled Culture, Religion, and Collective Memory in the Middle East"
  year: 2020
  isbn: 978-1250131201
  publisher: Henry Holt & Co.

- key: Gitinabard2020
  kind: inproceedings
  author:
  - Niki Gitinabard
  - Ruth Okoilu
  - Yiqao Xu
  - Sarah Heckman
  - Tiffany Barnes
  - Collin Lynch
  title: "Student Teamwork on Programming Projects: What Can GitHub Logs Show Us?"
  booktitle: Proc. EDM'2020
  year: 2020

- key: Glass2002
  kind: book
  author:
  - Robert L. Glass
  title: Facts and Fallacies of Software Engineering
  year: 2002
  isbn: 978-0321117427
  publisher: Addison-Wesley Professional

- key: Glerum2009
  kind: inproceedings
  author:
  - Kirk Glerum
  - Kinshuman Kinshumann
  - Steve Greenberg
  - Gabriel Aul
  - Vince Orgovan
  - Greg Nichols
  - David Grant
  - Gretchen Loihle
  - Galen Hunt
  title: "Debugging in the (Very) Large: Ten Years of Implementation and Experience"
  booktitle: Proc. SOSP'09
  year: 2009
  doi: 10.1145/1629575.1629586
  abstract: >
    Windows Error Reporting (WER) is a distributed system that automates the
    processing of error reports coming from an installed base of a billion
    machines. WER has collected billions of error reports in ten years of
    operation. It collects error data automatically and classifies errors into
    buckets, which are used to prioritize developer effort and report fixes to
    users. WER uses a progressive approach to data collection, which minimizes
    overhead for most reports yet allows developers to collect detailed
    information when needed. WER takes advantage of its scale to use error
    statistics as a tool in debugging; this allows developers to isolate bugs
    that could not be found at smaller scale. WER has been designed for large
    scale: one pair of database servers can record all the errors that occur on
    all Windows computers worldwide.

- key: Gordon2021
  kind: inproceedings
  author:
  - Nikolas Gordon
  - Omar Alam
  title: "The Role of Race and Gender in Teaching Evaluation of Computer Science Professors: A Large Scale Analysis on RateMyProfessor Data"
  booktitle: Proc. ICSE'21
  year: 2021
  doi: 10.1145/3408877.3432369
  abstract: >
    Recently, Computer Science (CS) education has experienced a renewed
    interest, driven by the demand in the fast-changing job market. This renewed
    interest created an uptick of enrollment in computer science
    courses. Increased number of students search for information about CS
    courses and professors. Often times, students turn to a professor's profile
    on online sites, e.g. RateMyProfessor.com (RMP), to read feedback and
    assessments made by other students. Student Evaluations of Teaching (SETs),
    conducted online or on paper, are widely used to assess and improve the
    teaching quality of professors, and to provide critical assessment of the
    teaching material and content. This paper studies the role of race and
    gender of computer science professors on their teaching evaluation by
    analyzing the publicly available data of over 39,000 CS professors on
    RateMyProfessor. We found that women are generally rated lower then men in
    overall teaching quality. They are also perceived lower in
    personality-related student feedback ratings, i.e. they perceived less
    humorous, and less inspirational. We also found that Asian professors are
    perceived to be tough graders and lecture heavy. They are also perceived to
    be more difficult in general.

- key: Graeber2019
  kind: book
  author:
  - David Graeber
  title: "Bullshit Jobs: A Theory"
  year: 2019
  isbn: 978-1501143335
  publisher: Simon & Schuster

- key: Graziotin2014
  kind: article
  author:
  - Daniel Graziotin
  - Xiaofeng Wang
  - Pekka Abrahamsson
  title: "Happy software developers solve problems better: psychological measurements in empirical software engineering"
  journal: PeerJ
  volume: 2
  pages: e289
  month: '3'
  year: 2014
  doi: 10.7717/peerj.289
  publisher: PeerJ
  url: https://doi.org/10.7717/peerj.289
  abstract: >
    For more than thirty years, it has been claimed that a way to improve
    software developers’ productivity and software quality is to focus on people
    and to provide incentives to make developers satisfied and happy. This claim
    has rarely been verified in software engineering research, which faces an
    additional challenge in comparison to more traditional engineering fields:
    software development is an intellectual activity and is dominated by
    often-neglected human factors (called human aspects in software engineering
    research). Among the many skills required for software development,
    developers must possess high analytical problem-solving skills and
    creativity for the software construction process. According to psychology
    research, affective states—emotions and moods—deeply influence the cognitive
    processing abilities and performance of workers, including creativity and
    analytical problem solving. Nonetheless, little research has investigated
    the correlation between the affective states, creativity, and analytical
    problem-solving performance of programmers. This article echoes the call to
    employ psychological measurements in software engineering research. We
    report a study with 42 participants to investigate the relationship between
    the affective states, creativity, and analytical problem-solving skills of
    software developers. The results offer support for the claim that happy
    developers are indeed better problem solvers in terms of their analytical
    abilities. The following contributions are made by this study: (1) providing
    a better understanding of the impact of affective states on the creativity
    and analytical problem-solving capacities of developers, (2) introducing and
    validating psychological measurements, theories, and concepts of affective
    states, creativity, and analytical-problem-solving skills in empirical
    software engineering, and (3) raising the need for studying the human
    factors of software engineering by employing a multidisciplinary viewpoint.

- key: Green1996
  kind: article
  author:
  - Thomas R. Green
  - Marian Petre
  title: "Usability Analysis of Visual Programming Environments: A ''Cognitive Dimensions'' Framework"
  journal: Journal of Visual Languages & Computing
  volume: 7
  number: 2
  year: 1996
  doi: 10.1006/jvlc.1996.0009
  abstract: >
    The cognitive dimensions framework is a broad-brush evaluation technique for
    interactive devices and for non-interactive notations. It sets out a small
    vocabulary of terms designed to capture the cognitively-relevant aspects of
    structure, and shows how they can be traded off against each other. The
    purpose of this paper is to propose the framework as an evaluation technique
    for visual programming environments. We apply it to two
    commercially-available dataflow languages (with further examples from other
    systems) and conclude that it is effective and insightful; other HCI-based
    evaluation techniques focus on different aspects and would make good
    complements. Insofar as the examples we used are representative, current
    VPLs are successful in achieving a good ‘closeness of match’, but designers
    need to consider the ‘viscosity ’ (resistance to local change) and the
    ‘secondary notation’ (possibility of conveying extra meaning by choice of
    layout, colour, etc.).

- key: Greenwald1995
  kind: article
  author:
  - Anthony G. Greenwald
  - Mahzarin R. Banaji
  title: "Implicit Social Cognition: Attitudes, Self-esteem, and Stereotypes"
  journal: Psychological Review
  volume: 102
  number: 1
  year: 1995
  doi: 10.1037/0033-295x.102.1.4
  abstract: >
    Social behavior is ordinarily treated as being under conscious (if not
    always thoughtful) control. However, considerable evidence now supports the
    view that social behavior often operates in an implicit or unconscious
    fashion. The identifying feature of implicit cognition is that past
    experience influences judgment in a fashion not introspectively known by the
    actor. The present conclusion--that attitudes, self-esteem, and stereotypes
    have important implicit modes of operation--extends both the construct
    validity and predictive usefulness of these major theoretical constructs of
    social psychology. Methodologically, this review calls for increased use of
    indirect measures--which are imperative in studies of implicit
    cognition. The theorized ordinariness of implicit stereotyping is consistent
    with recent findings of discrimination by people who explicitly disavow
    prejudice. The finding that implicit cognitive effects are often reduced by
    focusing judges' attention on their judgment task provides a basis for
    evaluating applications (such as affirmative action) aimed at reducing such
    unintended discrimination.

- key: Gregg2020
  kind: book
  author:
  - Brendan Gregg
  title: "Systems Performance: Enterprise and the Cloud"
  edition: 2nd
  year: 2020
  isbn: 978-0136820154
  publisher: Pearson
  note: Covers strategies and tools for getting the best performance out of operating systems and applications.

- key: Gruenert2015
  kind: book
  author:
  - Steve Gruenert
  - Todd Whitaker
  title: "School Culture Rewired: How to Define, Assess, and Transform It"
  year: 2015
  isbn: 978-1416619901
  publisher: ASCD

- key: Hanenberg2010
  kind: article
  author:
  - Stefan Hanenberg
  title: An experiment about static and dynamic type systems
  journal: SIGPLAN
  volume: 45
  number: 10
  pages: '22'
  month: '10'
  year: 2010
  doi: 10.1145/1932682.1869462
  publisher: Association for Computing Machinery (ACM)
  url: https://doi.org/10.1145/1932682.1869462

- key: Hanenberg2013
  kind: article
  author:
  - Stefan Hanenberg
  - Sebastian Kleinschmager
  - Romain Robbes
  - Éric Tanter
  - Andreas Stefik
  title: An Empirical Study on the Impact of Static Typing on Software Maintainability
  journal: ESE
  volume: 19
  number: 5
  year: 2013
  doi: 10.1007/s10664-013-9289-1
  abstract: >
    Static type systems play an essential role in contemporary programming
    languages. Despite their importance, whether static type systems impact
    human software development capabilities remains open. One frequently
    mentioned argument in favor of static type systems is that they improve the
    maintainability of software systems—an often-used claim for which there is
    little empirical evidence. This paper describes an experiment that tests
    whether static type systems improve the maintainability of software systems,
    in terms of understanding undocumented code, fixing type errors, and fixing
    semantic errors. The results show rigorous empirical evidence that static
    types are indeed beneficial to these activities, except when fixing semantic
    errors. We further conduct an exploratory analysis of the data in order to
    understand possible reasons for the effect of type systems on the three
    kinds of tasks used in this experiment. From the exploratory analysis, we
    conclude that developers using a dynamic type system tend to look at
    different files more frequently when doing programming tasks—which is a
    potential reason for the observed differences in time.

- key: Hannay2009
  kind: article
  author:
  - Jo E. Hannay
  - Tore Dybå
  - Erik Arisholm
  - Dag I.K. Sjøberg
  title: "The Effectiveness of Pair Programming: A Meta-Analysis"
  journal: Information and Software Technology
  volume: 51
  number: 7
  year: 2009
  doi: 10.1016/j.infsof.2009.02.001
  abstract: >
    Several experiments on the effects of pair versus solo programming have been
    reported in the literature. We present a meta-analysis of these studies. The
    analysis shows a small significant positive overall effect of pair
    programming on quality, a medium significant positive overall effect on
    duration, and a medium significant negative overall effect on
    effort. However, between-study variance is significant, and there are signs
    of publication bias among published studies on pair programming. A more
    detailed examination of the evidence suggests that pair programming is
    faster than solo programming when programming task complexity is low and
    yields code solutions of higher quality when task complexity is high. The
    higher quality for complex tasks comes at a price of considerably greater
    effort, while the reduced completion time for the simpler tasks comes at a
    price of noticeably lower quality. We conclude that greater attention should
    be given to moderating factors on the effects of pair programming.

- key: Hannay2010
  kind: article
  author:
  - J.E. Hannay
  - E. Arisholm
  - H. Engvik
  - D.I.K. Sjøberg
  title: Effects of Personality on Pair Programming
  journal: TSE
  volume: 36
  number: 1
  pages: 61--80
  month: '1'
  year: 2010
  doi: 10.1109/tse.2009.41
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  url: https://doi.org/10.1109/tse.2009.41
  abstract: >
    Personality tests in various guises are commonly used in recruitment and
    career counseling industries. Such tests have also been considered as
    instruments for predicting the job performance of software professionals
    both individually and in teams. However, research suggests that other
    human-related factors such as motivation, general mental ability, expertise,
    and task complexity also affect the performance in general. This paper
    reports on a study of the impact of the Big Five personality traits on the
    performance of pair programmers together with the impact of expertise and
    task complexity. The study involved 196 software professionals in three
    countries forming 98 pairs. The analysis consisted of a confirmatory part
    and an exploratory part. The results show that: (1) Our data do not confirm
    a meta-analysis-based model of the impact of certain personality traits on
    performance and (2) personality traits, in general, have modest predictive
    value on pair programming performance compared with expertise, task
    complexity, and country. We conclude that more effort should be spent on
    investigating other performance-related predictors such as expertise, and
    task complexity, as well as other promising predictors, such as programming
    skill and learning. We also conclude that effort should be spent on
    elaborating on the effects of personality on various measures of
    collaboration, which, in turn, may be used to predict and influence
    performance. Insights into such malleable, rather than static, factors may
    then be used to improve pair programming performance.

- key: Hansen2013
  kind: link
  author:
  - Michael Hansen
  - Robert L. Goldstone
  - Andrew Lumsdaine
  title: What Makes Code Hard to Understand?
  year: 2013
  url: https://arxiv.org/abs/1304.5257

- key: Hao2021
  kind: link
  author:
  - Karen Hao
  title: How Facebook Got Addicted to Spreading Misinformation
  url: https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/

- key: Hata2019
  kind: inproceedings
  author:
  - Hideaki Hata
  - Christoph Treude
  - Raula Gaikovina Kula
  - Takashi Ishio
  title: "9.6 Million Links in Source Code Comments: Purpose,  Evolution,  and Decay"
  booktitle: Proc. ICSE'19
  month: '5'
  year: 2019
  doi: 10.1109/icse.2019.00123
  publisher: IEEE
  url: https://doi.org/10.1109/icse.2019.00123
  abstract: >
    Links are an essential feature of the World Wide Web, and source code
    repositories are no exception. However, despite their many undisputed
    benefits, links can suffer from decay, insufficient versioning, and lack of
    bidirectional traceability. In this paper, we investigate the role of links
    contained in source code comments from these perspectives. We conducted a
    large-scale study of around 9.6 million links to establish their prevalence,
    and we used a mixed-methods approach to identify the links' targets,
    purposes, decay, and evolutionary aspects. We found that links are prevalent
    in source code repositories, that licenses, software homepages, and
    specifications are common types of link targets, and that links are often
    included to provide metadata or attribution. Links are rarely updated, but
    many link targets evolve. Almost 10% of the links included in source code
    comments are dead. We then submitted a batch of link-fixing pull requests to
    open source software repositories, resulting in most of our fixes being
    merged successfully. Our findings indicate that links in source code
    comments can indeed be fragile, and our work opens up avenues for future
    work to address these problems.

- key: Hatton2008
  kind: article
  author:
  - Les Hatton
  title: Testing the Value of Checklists in Code Inspections
  journal: IEEE Software
  volume: 25
  number: 4
  year: 2008
  doi: 10.1109/ms.2008.100
  abstract: >
    Checklists are an important part of code and design inspections. Ideally,
    they aim to increase the number of faults found per inspection hour by
    highlighting known areas of previous failure. In practice, although some
    researchers have quantified checklists' benefits, the conclusions'
    statistical robustness hasn't been as well represented. The author subjects
    checklists' effectiveness to formal statistical testing, using data from 308
    inspections by industrial engineers over a three-year period. The results
    showed no evidence that checklists significantly improved these
    inspections. Further analysis revealed that individual inspection
    performance varied by a factor of 10 in terms of faults found per unit time,
    and individuals found on average about 53 percent of the faults. Two-person
    teams found on average 76 percent of the faults.

- key: Hecht2004
  kind: book
  author:
  - Jennifer Hecht
  title: "Doubt: A History"
  year: 2004
  isbn: 978-0060097950
  publisher: HarperOne

- key: Hellendoorn2019
  kind: inproceedings
  author:
  - Vincent J. Hellendoorn
  - Sebastian Proksch
  - Harald C. Gall
  - Alberto Bacchelli
  title: "When Code Completion Fails: A Case Study on Real-World Completions"
  booktitle: Proc. ICSE'19
  month: '5'
  year: 2019
  doi: 10.1109/icse.2019.00101
  publisher: IEEE
  url: https://doi.org/10.1109/icse.2019.00101
  abstract: >
    Code completion is commonly used by software developers and is integrated
    into all major IDE's. Good completion tools can not only save time and
    effort but may also help avoid incorrect API usage. Many proposed completion
    tools have shown promising results on synthetic benchmarks, but these
    benchmarks make no claims about the realism of the completions they
    test. This lack of grounding in real-world data could hinder our scientific
    understanding of developer needs and of the efficacy of completion
    models. This paper presents a case study on 15,000 code completions that
    were applied by 66 real developers, which we study and contrast with
    artificial completions to inform future research and tools in this area. We
    find that synthetic benchmarks misrepresent many aspects of real-world
    completions; tested completion tools were far less accurate on real-world
    data. Worse, on the few completions that consumed most of the developers'
    time, prediction accuracy was less than 20% -- an effect that is invisible
    in synthetic benchmarks. Our findings have ramifications for future
    benchmarks, tool design and real-world efficacy: Benchmarks must account for
    completions that developers use most, such as intra-project APIs; models
    should be designed to be amenable to intra-project data; and real-world
    developer trials are essential to quantifying performance on the least
    predictable completions, which are both most time-consuming and far more
    typical than artificial data suggests. We publicly release our preprint
    [https://doi.org/10.5281/zenodo.2565673] and replication data and materials
    [https://doi.org/10.5281/zenodo.2562249].

- key: Henrich2010
  kind: article
  author:
  - Joseph Henrich
  - Steven J. Heine
  - Ara Norenzayan
  title: The Weirdest People in the World?
  journal: Behavioral and Brain Sciences
  volume: 33
  number: 2-3
  year: 2010
  doi: 10.1017/s0140525x0999152x
  abstract: >
    Behavioral scientists routinely publish broad claims about human psychology
    and behavior in the world's top journals based on samples drawn entirely
    from Western, Educated, Industrialized, Rich, and Democratic (WEIRD)
    societies. Researchers – often implicitly – assume that either there is
    little variation across human populations, or that these “standard subjects”
    are as representative of the species as any other population. Are these
    assumptions justified? Here, our review of the comparative database from
    across the behavioral sciences suggests both that there is substantial
    variability in experimental results across populations and that WEIRD
    subjects are particularly unusual compared with the rest of the species –
    frequent outliers. The domains reviewed include visual perception, fairness,
    cooperation, spatial reasoning, categorization and inferential induction,
    moral reasoning, reasoning styles, self-concepts and related motivations,
    and the heritability of IQ. The findings suggest that members of WEIRD
    societies, including young children, are among the least representative
    populations one could find for generalizing about humans. Many of these
    findings involve domains that are associated with fundamental aspects of
    psychology, motivation, and behavior – hence, there are no obvious a priori
    grounds for claiming that a particular behavioral phenomenon is universal
    based on sampling from a single subpopulation. Overall, these empirical
    patterns suggests that we need to be less cavalier in addressing questions
    of human nature on the basis of data drawn from this particularly thin, and
    rather unusual, slice of humanity. We close by proposing ways to
    structurally re-organize the behavioral sciences to best tackle these
    challenges.

- key: Hermans2016
  kind: inproceedings
  author:
  - Felienne Hermans
  - Bas Jansen
  - Sohon Roy
  - Efthimia Aivaloglou
  - Alaaeddin Swidan
  - David Hoepelman
  title: "Spreadsheets are Code: An Overview of Software Engineering Approaches Applied to Spreadsheets"
  booktitle: Proc. SANER'16
  year: 2016
  doi: 10.1109/saner.2016.86
  abstract: >
    Spreadsheets can be considered to be the world's most successful end-user
    programming language. In fact, one could say spreadsheets are programs. This
    paper starts with a comparison of spreadsheets to software: spreadsheets are
    similar in terms of applications domains, expressive power and
    maintainability problems. We then reflect upon what makes spreadsheets
    successful: liveness, directness and an easy deployment environment seem
    contribute largely to their success. Being a programming language, several
    techniques from software engineering can be applied to spreadsheets. We
    present an overview of such research directions, including spreadsheet
    testing, reverse engineering, smell detection, clone detection and
    refactoring. Finally, open challenges and future plans for the domain of
    spreadsheet software engineering are presented.

- key: Herzig2013
  kind: inproceedings
  author:
  - Kim Herzig
  - Sascha Just
  - Andreas Zeller
  title: "It''s not a Bug, it''s a Feature: How Misclassification Impacts Bug Prediction"
  booktitle: Proc. ICSE'13
  year: 2013

- key: Hicks2018
  kind: book
  author: Marie Hicks
  title: "Programmed Inequality: How Britain Discarded Women Technologists and Lost Its Edge in Computing"
  year: 2018
  isbn: 978-0262535182
  publisher: MIT Press

- key: Hilderbrand2020
  kind: inproceedings
  author:
  - Claudia Hilderbrand
  - Christopher Perdriau
  - Lara Letaw
  - Jillian Emard
  - Zoe Steine-Hanson
  - Margaret Burnett
  - Anita Sarma
  title: "Engineering Gender-Inclusivity into Software: Tales from the Trenches"
  booktitle: Proc. ICSE'20
  year: 2020
  doi: 10.1145/3377811.3380371
  abstract: >
    Although the need for gender-inclusivity in software is gaining attention
    among SE researchers and SE practitioners, and at least one method
    (GenderMag) has been published to help, little has been reported on how to
    make such methods work in real-world settings. Real-world teams are
    ever-mindful of the practicalities of adding new methods on top of their
    existing processes. For example, how can they keep the time costs viable?
    How can they maximize impacts of using it? What about controversies that can
    arise in talking about gender? To find out how software teams “in the
    trenches” handle these and similar questions, we collected the
    GenderMag-based processes of 10 real-world software teams—more than 50
    people—for periods ranging from 5 months to 3.5 years. We present these
    teams' insights and experiences in the form of 9 practices, 2 potential
    pitfalls, and 2 open issues, so as to provide their insights to other
    real-world software teams trying to engineer gender-inclusivity into their
    software products.

- key: Hilton2016
  kind: inproceedings
  author:
  - Michael Hilton
  - Timothy Tunnell
  - Kai Huang
  - Darko Marinov
  - Danny Dig
  title: Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects
  booktitle: Proc. ASE'16
  year: 2016
  doi: 10.1145/2970276.2970358
  abstract: >
    Continuous integration (CI) systems automate the compilation, building, and
    testing of software. Despite CI rising as a big success story in automated
    software engineering, it has received almost no attention from the research
    community. For example, how widely is CI used in practice, and what are some
    costs and benefits associated with CI? Without answering such questions,
    developers, tool builders, and researchers make decisions based on folklore
    instead of data. In this paper, we use three complementary methods to study
    the usage of CI in open-source projects. To understand which CI systems
    developers use, we analyzed 34,544 open-source projects from GitHub. To
    understand how developers use CI, we analyzed 1,529,291 builds from the most
    commonly used CI system. To understand why projects use or do not use CI, we
    surveyed 442 developers. With this data, we answered several key questions
    related to the usage, costs, and benefits of CI. Among our results, we show
    evidence that supports the claim that CI helps projects release more often,
    that CI is widely adopted by the most popular projects, as well as finding
    that the overall percentage of projects using CI continues to grow, making
    it important and timely to focus more research on CI.

- key: Hippel2006
  kind: book
  author:
  - Eric von Hippel
  title: Democratizing Innovation
  year: 2006
  isbn: 978-0262720472
  publisher: MIT Press

- key: Hochschild2006
  kind: book
  author:
  - Adam Hochschild
  title: "Bury the Chains: Prophets and Rebels in the Fight to Free an Empire''s Slaves"
  year: 2006
  isbn: 978-0618619078
  publisher: Mariner Books

- key: Hofmeister2017
  kind: inproceedings
  author:
  - Johannes Hofmeister
  - Janet Siegmund
  - Daniel V. Holt
  title: Shorter Identifier Names Take Longer to Comprehend
  booktitle: Proc. SANER'17
  year: 2017
  doi: 10.1109/saner.2017.7884623
  abstract: >
    Developers spend the majority of their time comprehending code, a process in
    which identifier names play a key role. Although many identifier naming
    styles exist, they often lack an empirical basis and it is not quite clear
    whether short or long identifier names facilitate comprehension. In this
    paper, we investigate the effect of different identifier naming styles
    (letters, abbreviations, words) on program comprehension, and whether these
    effects arise because of their length or their semantics. We conducted an
    experimental study with 72 professional C# developers, who looked for
    defects in source-code snippets. We used a within-subjects design, such that
    each developer saw all three versions of identifier naming styles and we
    measured the time it took them to find a defect. We found that words lead
    to, on average, 19% faster comprehension speed compared to letters and
    abbreviations, but we did not find a significant difference in speed between
    letters and abbreviations. The results of our study suggest that defects in
    code are more difficult to detect when code contains only letters and
    abbreviations. Words as identifier names facilitate program comprehension
    and can help to save costs and improve software quality.

- key: Holmes2014
  kind: inproceedings
  author:
  - Reid Holmes
  - Michelle Craig
  - Karen Reid
  - Eleni Stroulia
  title: Lessons Learned Managing Distributed Software Engineering Courses
  booktitle: ICSE'14
  year: 2014
  doi: 10.1145/2591062.2591160
  abstract: >
    We have run the Undergraduate Capstone Open Source Projects (UCOSP) program
    for ten terms over the past six years providing over 400 Canadian students
    from more than 30 schools the opportunity to be members of distributed
    software teams. UCOSP aims to provide students with real development
    experience enabling them to integrate lessons they have learned in the
    classroom with practical development experience while developing their
    technical communication skills. The UCOSP program has evolved over time as
    we have learned how to effectively manage a diverse set of students working
    on a large number of different projects. The goal of this paper is to
    provide an overview of the roles of the various stakeholders for distributed
    software engineering projects and the various lessons we have learned to
    make UCOSP an effective and positive learning experience.

- key: Holmes2018
  kind: inproceedings
  author:
  - Reid Holmes
  - Meghan Allen
  - Michelle Craig
  title: Dimensions of Experientialism for Software Engineering Education
  booktitle: Proc. ICSE-SEET'18
  year: 2018
  doi: 10.1145/3183377.3183380
  abstract: >
    There is a gap between the abstract concepts taught in the classroom and the
    skills needed for students to succeed once they join the workplace. The
    Undergraduate Capstone Open Source Projects (UCOSP) program was developed to
    narrow this gap by enabling undergraduate computer science students to have
    an experiential software engineering learning opportunity. Over the past 8
    years, 737 students from 30 universities have taken part in this program. In
    this paper, we sought to understand student perceptions of how UCOSP
    complements traditional classwork by providing real-world software
    engineering exposure. We report on a qualitative analysis of 2,203 quotes
    collected from 167 students from 18 universities over six academic terms. We
    analyzed these data using a grounded theory approach based on open coding to
    gain insight into the key benefits of the program from the students'
    perspective. We found that students highly value being able to apply their
    classroom knowledge to real, novel tasks, for real projects with a community
    of users, while receiving real mentorship from a member of the development
    team. Further, we found that contributing to real software systems provides
    greater understanding of software engineering than might otherwise be
    obtained through more traditional means. Our goal is that our analysis can
    help fellow educators add additional experimentalism into their existing
    programs.

- key: Hsing2019
  kind: inproceedings
  author:
  - Courtney Hsing
  - Vanessa Gennarelli
  title: Using GitHub in the Classroom Predicts Student Learning Outcomes and Classroom Experiences
  booktitle: Proc. SIGCSE'19
  year: 2019
  doi: 10.1145/3287324.3287460
  abstract: >
    GitHub is a widely-used software development platform that supports version
    control, collaborative development, and project hosting. Currently, an
    estimated 18,000 educators use GitHub in programming classrooms. Depending
    on how GitHub is implemented in the classroom, students may rely on GitHub
    for activities such as, submitting assignments, collaborating on group
    projects, and receiving feedback. Despite GitHub's growing presence in
    programming classrooms, to date, few studies have explored how GitHub and
    the design of its implementation shape students' learning outcomes and
    classroom experiences. Building on previous research, we investigated how
    students in classrooms that used GitHub (GitHub classrooms), as opposed to
    classrooms that did not use GitHub (non-GitHub classrooms), differed across
    key variables. We surveyed 7530 students and 300 educators from GitHub and
    non-GitHub classrooms. Overall, we found that using GitHub in the classroom
    predicted better learning outcomes and classroom experiences. For example,
    students felt more prepared for the future, and they felt a greater sense of
    belonging in the classroom and in the field. Importantly, the design of
    implementation affected learning outcomes. For example, of the students who
    used GitHub in the classroom and received instructor feedback, those who
    received (versus did not receive) feedback via GitHub benefited more from
    the feedback. We discuss best practices for maximizing benefits to student
    learning when implementing GitHub in the classroom, study limitations, and
    future research directions. Our research is a step towards understanding how
    GitHub, a tool with a growing presence in programming classrooms, impacts
    students' learning experiences.

- key: Hu2019
  kind: inproceedings
  author:
  - Yang Hu
  - Umair Z. Ahmed
  - Sergey Mechtaev
  - Ben Leong
  - Abhik Roychoudhury
  title: Re-Factoring Based Program Repair Applied to Programming Assignments
  booktitle: Proc. ASE'19
  year: 2019

- key: Huijgens2020
  kind: inproceedings
  author:
  - Hennie Huijgens
  - Ayushi Rastogi
  - Ernst Mulders
  - Georgios Gousios
  - Arie van Deursen
  title: "Questions for Data Scientists in Software Engineering: A Replication"
  booktitle: ESEC/FSE'20
  year: 2020
  doi: 10.1145/3368089.3409717
  abstract: >
    In 2014, a Microsoft study investigated the sort of questions that data
    science applied to software engineering should answer. This resulted in 145
    questions that developers considered relevant for data scientists to answer,
    thus providing a research agenda to the community. Fast forward to five
    years, no further studies investigated whether the questions from the
    software engineers at Microsoft hold for other software companies, including
    software-intensive companies with different primary focus (to which we refer
    as software-defined enterprises). Furthermore, it is not evident that the
    problems identified five years ago are still applicable, given the
    technological advances in software engineering. This paper presents a study
    at ING, a software-defined enterprise in banking in which over 15,000 IT
    staff provides in-house software solutions. This paper presents a
    comprehensive guide of questions for data scientists selected from the
    previous study at Microsoft along with our current work at ING. We
    replicated the original Microsoft study at ING, looking for questions that
    impact both software companies and software-defined enterprises and continue
    to impact software engineering. We also add new questions that emerged from
    differences in the context of the two companies and the five years gap in
    between. Our results show that software engineering questions for data
    scientists in the software-defined enterprise are largely similar to the
    software company, albeit with exceptions. We hope that the software
    engineering research community builds on the new list of questions to create
    a useful body of knowledge.

- key: Hunt1999
  kind: book
  author:
  - Andrew Hunt
  - David Thomas
  title: "The Pragmatic Programmer: From Journeyman to Master"
  year: 1999
  isbn: 978-0201616224
  publisher: Addison-Wesley Professional

- key: Inozemtseva2014
  kind: inproceedings
  author:
  - Laura Inozemtseva
  - Reid Holmes
  title: Coverage is not strongly correlated with test suite effectiveness
  booktitle: Proc. ICSE'14
  year: 2014
  doi: 10.1145/2568225.2568271
  publisher: ACM Press
  url: https://doi.org/10.1145/2568225.2568271
  abstract: >
    The coverage of a test suite is often used as a proxy for its ability to
    detect faults. However, previous studies that investigated the correlation
    between code coverage and test suite effectiveness have failed to reach a
    consensus about the nature and strength of the relationship between these
    test suite characteristics. Moreover, many of the studies were done with
    small or synthetic programs, making it unclear whether their results
    generalize to larger programs, and some of the studies did not account for
    the confounding influence of test suite size. In addition, most of the
    studies were done with adequate suites, which are are rare in practice, so
    the results may not generalize to typical test suites. We have extended
    these studies by evaluating the relationship between test suite size,
    coverage, and effectiveness for large Java programs. Our study is the
    largest to date in the literature: we generated 31,000 test suites for five
    systems consisting of up to 724,000 lines of source code. We measured the
    statement coverage, decision coverage, and modified condition coverage of
    these suites and used mutation testing to evaluate their fault detection
    effectiveness. We found that there is a low to moderate correlation between
    coverage and effectiveness when the number of test cases in the suite is
    controlled for. In addition, we found that stronger forms of coverage do not
    provide greater insight into the effectiveness of the suite. Our results
    suggest that coverage, while useful for identifying under-tested parts of a
    program, should not be used as a quality target because it is not a good
    indicator of test suite effectiveness.

- key: Irving2021
  kind: book
  author:
  - Damien Irving
  - Kate Hertweck
  - Luke Johnston
  - Joel Ostblom
  - Charlotte Wickham
  - Greg Wilson
  title: "Research Software Engineering with Python: Building Software that Makes Research Possible"
  year: 2019
  isbn: 978-0367698348
  publisher: Chapman & Hall/CRC Press

- key: Jackson2016
  kind: book
  author:
  - Daniel Jackson
  title: "Software Abstractions: Logic, Language, and Analysis"
  edition: revised
  year: 2016
  isbn: 978-0262528900
  publisher: MIT Press

- key: Jackson2017
  kind: book
  author:
  - Justin Ian Jackson
  title: "Marketing for Developers: Build, Launch, and Get Your First 100 Customers"
  year: 2017
  isbn: 978-0994973009
  publisher: Nerd North

- key: Jaffe2021
  kind: book
  author:
  - Sarah Jaffe
  title: "Work Won''t Love You Back: How Devotion to Our Jobs Keeps Us Exploited, Exhausted, and Alone"
  year: 2021
  isbn: 978-1568589398
  publisher: Bold Type Books

- key: Jarry2020
  kind: link
  author:
  - Jonathan Jarry
  title: The Dunning-Kruger Effect is Probably Not Real
  year: 2020
  url: https://www.mcgill.ca/oss/article/critical-thinking/dunning-kruger-effect-probably-not-real

- key: Jiang2013
  kind: inproceedings
  author:
  - Yujuan Jiang
  - Bram Adams
  - Daniel M. German
  title: Will my patch make it? And how fast? Case study on the Linux kernel
  booktitle: Proc. MSR'13
  month: '5'
  year: 2013
  doi: 10.1109/msr.2013.6624016
  publisher: IEEE
  url: https://doi.org/10.1109/msr.2013.6624016
  abstract: >
    The Linux kernel follows an extremely distributed reviewing and integration
    process supported by 130 developer mailing lists and a hierarchy of dozens
    of Git repositories for version control. Since not every patch can make it
    and of those that do, some patches require a lot more reviewing and
    integration effort than others, developers, reviewers and integrators need
    support for estimating which patches are worthwhile to spend effort on and
    which ones do not stand a chance. This paper crosslinks and analyzes eight
    years of patch reviews from the kernel mailing lists and committed patches
    from the Git repository to understand which patches are accepted and how
    long it takes those patches to get to the end user. We found that 33% of the
    patches makes it into a Linux release, and that most of them need 3 to 6
    months for this. Furthermore, that patches developed by more experienced
    developers are more easily accepted and faster reviewed and
    integrated. Additionally, reviewing time is impacted by submission time, the
    number of affected subsystems by the patch and the number of requested
    reviewers.

- key: Jin2018
  kind: inproceedings
  author:
  - Xianhao Jin
  - Francisco Servant
  title: The hidden cost of code completion
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196474
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196474
  abstract: >
    Automatic code completion is a useful and popular technique that software
    developers use to write code more effectively and efficiently. However,
    while the benefits of code completion are clear, its cost is yet not well
    understood. We hypothesize the existence of a hidden cost of code
    completion, which mostly impacts developers when code completion techniques
    produce long recommendations. We study this hidden cost of code completion
    by evaluating how the length of the recommendation list affects other
    factors that may cause inefficiencies in the process. We study how common
    long recommendations are, whether they often provide low-ranked correct
    items, whether they incur longer time to be assessed, and whether they were
    more prevalent when developers did not select any item in the list. In our
    study, we observe evidence for all these factors, confirming the existence
    of a hidden cost of code completion.

- key: Johnson2017
  kind: book
  author:
  - Jeff Johnson
  - Kate Finn
  title: "Designing User Interfaces for an Aging Population: Towards Universal Design"
  year: 2017
  isbn: 978-0128044674
  publisher: Morgan Kaufmann

- key: Johnson2019
  kind: inproceedings
  author:
  - John Johnson
  - Sergio Lubo
  - Nishitha Yedla
  - Jairo Aponte
  - Bonita Sharif
  title: An Empirical Study Assessing Source Code Readability in Comprehension
  booktitle: Proc. ICSME'19
  year: 2019
  doi: 10.1109/ICSME.2019.00085
  note: Found that reducing nesting in source code improved readability.
  abstract: >
    Software developers spend a significant amount of time reading source
    code. If code is not written with readability in mind, it impacts the time
    required to maintain it. In order to alleviate the time taken to read and
    understand code, it is important to consider how readable the code is. The
    general consensus is that source code should be written to minimize the time
    it takes for others to read and understand it. In this paper, we conduct a
    controlled experiment to assess two code readability rules: nesting and
    looping. We test 32 Java methods in four categories: ones that follow/do not
    follow the readability rule and that are correct/incorrect. The study was
    conducted online with 275 participants. The results indicate that minimizing
    nesting decreases the time a developer spends reading and understanding
    source code, increases confidence about the developer's understanding of the
    code, and also suggests that it improves their ability to find bugs. The
    results also show that avoiding the do-while statement had no significant
    impact on level of understanding, time spent reading and understanding,
    confidence in understanding, or ease of finding bugs. It was also found that
    the better knowledge of English a participant had, the more their
    readability and comprehension confidence ratings were affected by the
    minimize nesting rule. We discuss the implications of these findings for
    code readability and comprehension.

- key: Johnson2020
  kind: book
  author:
  - Jeff Johnson
  title: "Designing with the Mind in Mind: Simple Guide to Understanding User Interface Design Guidelines"
  edition: 3rd
  year: 2020
  isbn: 978-0128182024
  publisher: Morgan Kaufmann

- key: Jones2019
  kind: link
  author:
  - Derek Jones
  title: Evidence-Based Software Engineering Using R
  year: 2019
  url: http://www.knosof.co.uk/ESEUR/

- key: Joonbakhsh2018
  kind: inproceedings
  author:
  - Alireza Joonbakhsh
  - Ashkan Sami
  title: Mining and extraction of personal software process measures through IDE interaction logs
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196462
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196462
  abstract: >
    The Personal Software Process (PSP) is an effective software process
    improvement method that heavily relies on manual collection of software
    development data. This paper describes a semi-automated method that reduces
    the burden of PSP data collection by extracting the required time and size
    of PSP measurements from IDE interaction logs. The tool mines enriched event
    data streams so can be easily generalized to other developing environment
    also. In addition, the proposed method is adaptable to phase definition
    changes and creates activity visualizations and summarizations that are
    helpful for software project management. Tools and processed data used for
    this paper are available on GitHub at:
    https://github.com/unknowngithubuser1/data.

- key: Jorgensen2011
  kind: article
  author:
  - Magne Jorgensen
  - Stein Grimstad
  title: "The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment"
  journal: TSE
  volume: 37
  number: 5
  pages: 695--707
  month: '9'
  year: 2011
  doi: 10.1109/tse.2010.78
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  url: https://doi.org/10.1109/tse.2010.78
  abstract: >
    Studies in laboratory settings report that software development effort
    estimates can be strongly affected by effort-irrelevant and misleading
    information. To increase our knowledge about the importance of these effects
    in field settings, we paid 46 outsourcing companies from various countries
    to estimate the required effort of the same five software development
    projects. The companies were allocated randomly to either the original
    requirement specification or a manipulated version of the original
    requirement specification. The manipulations were as follows: 1) reduced
    length of requirement specification with no change of content, 2)
    information about the low effort spent on the development of the old system
    to be replaced, 3) information about the client's unrealistic expectations
    about low cost, and 4) a restriction of a short development period with
    start up a few months ahead. We found that the effect sizes in the field
    settings were much smaller than those found for similar manipulations in
    laboratory settings. Our findings suggest that we should be careful about
    generalizing to field settings the effect sizes found in laboratory
    settings. While laboratory settings can be useful to demonstrate the
    existence of an effect and better understand it, field studies may be needed
    to study the size and importance of these effects.

- key: Jost2009
  kind: article
  author:
  - John T. Jost
  - Laurie A. Rudman
  - Irene V. Blair
  - Dana R. Carney
  - Nilanjana Dasgupta
  - Jack Glaser
  - Curtis D. Hardin
  title: "The Existence of Implicit Bias is Beyond Reasonable Doubt: A Refutation of Ideological and Methodological Objections and Executive Summary of Ten Studies that no Manager Should Ignore"
  journal: Research in Organizational Behavior
  volume: 29
  year: 2009
  doi: 10.1016/j.riob.2009.10.001
  abstract: >
    In this article, we respond at length to recent critiques of research on
    implicit bias, especially studies using the Implicit Association Test
    (IAT). Tetlock and Mitchell (2009) claim that ‘‘there is no evidence that
    the IAT reliably predicts class-wide discrimination on tangible outcomes in
    any setting,’’ accuse their colleagues of violating ‘‘the injunction to
    separate factual from value judgments,’’ adhering blindly to a ‘‘statist
    interventionist’’ ideology, and of conducting a witch-hunt against implicit
    racists, sexists, and others. These and other charges are specious. Far from
    making ‘‘extraordinary claims’’ that ‘‘require extraordinary evidence,’’
    researchers have identified the existence and consequences of implicit bias
    through well-established methods based upon principles of cognitive
    psychology that have been developed in nearly a century’s worth of work. We
    challenge the blanket skepticism and organizational complacency advocated by
    Tetlock and Mitchell and summarize 10 recent studies that no manager (or
    managerial researcher) should ignore. These studies reveal that students,
    nurses, doctors, police officers, employment recruiters, and many others
    exhibit implicit biases with respect to race, ethnicity, nationality,
    gender, social status, and other distinctions. Furthermore—and contrary to
    the emphatic assertions of the critics—participants’ implicit associations
    do predict socially and

- key: Kahan1997
  kind: link
  author:
  - William Kahan
  title: Lecture Notes on the Status of IEEE Standard 754 for Binary Floating-Point Arithmetic
  year: 1997
  url: https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF

- key: Kalyuga2003
  kind: article
  author:
  - Slava Kalyuga
  - Paul Ayres
  - Paul Chandler
  - John Sweller
  title: The Expertise Reversal Effect
  journal: Educational Psychologist
  volume: 38
  number: 1
  year: 2003
  doi: 10.1207/s15326985ep3801_4
  abstract: >
    When new information is presented to learners, it must be processed in a
    severely limited working memory. Learning reduces working memory limitations
    by enabling the use of schemas, stored in long-term memory, to process
    information more efficiently. Several instructional techniques have been
    designed to facilitate schema construction and automation by reducing
    working memory load. Recently, however, strong evidence has emerged that the
    effectiveness of these techniques depends very much on levels of learner
    expertise. Instructional techniques that are highly effective with
    inexperienced learners can lose their effectiveness and even have negative
    consequences when used with more experienced learners. We call this
    phenomenon the expertise reversal effect. In this article, we review the
    empirical literature on the interaction between instructional techniques and
    levels of learner experience that led to the identification of the expertise
    reversal effect.

- key: Kamei2013
  kind: article
  author:
  - Yasutaka Kamei
  - Emad Shihab
  - Bram Adams
  - Ahmed E. Hassan
  - Audris Mockus
  - Anand Sinha
  - Naoyasu Ubayashi
  title: A large-scale empirical study of just-in-time quality assurance
  journal: TSE
  volume: 39
  number: 6
  pages: 757--773
  month: '6'
  year: 2013
  doi: 10.1109/tse.2012.70
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  url: https://doi.org/10.1109/tse.2012.70
  abstract: >
    Defect prediction models are a well-known technique for identifying
    defect-prone files or packages such that practitioners can allocate their
    quality assurance efforts (e.g., testing and code reviews). However, once
    the critical files or packages have been identified, developers still need
    to spend considerable time drilling down to the functions or even code
    snippets that should be reviewed or tested. This makes the approach too time
    consuming and impractical for large software systems. Instead, we consider
    defect prediction models that focus on identifying defect-prone (“risky”)
    software changes instead of files or packages. We refer to this type of
    quality assurance activity as “Just-In-Time Quality Assurance,” because
    developers can review and test these risky changes while they are still
    fresh in their minds (i.e., at check-in time). To build a change risk model,
    we use a wide range of factors based on the characteristics of a software
    change, such as the number of added lines, and developer experience. A
    large-scale study of six open source and five commercial projects from
    multiple domains shows that our models can predict whether or not a change
    will lead to a defect with an average accuracy of 68 percent and an average
    recall of 64 percent. Furthermore, when considering the effort needed to
    review changes, we find that using only 20 percent of the effort it would
    take to inspect all changes, we can identify 35 percent of all
    defect-inducing changes. Our findings indicate that “Just-In-Time Quality
    Assurance” may provide an effort-reducing way to focus on the most risky
    changes and thus reduce the costs of developing high-quality software.

- key: Kang2016
  kind: article
  author:
  - Sean H. K. Kang
  title: Spaced Repetition Promotes Efficient and Effective Learning
  journal: Policy Insights from the Behavioral and Brain Sciences
  volume: 3
  number: 1
  year: 2016
  doi: 10.1177/2372732215624708
  abstract: >
    Concern that students in the United States are less proficient in
    mathematics, science, and reading than their peers in other countries has
    led some to question whether American students spend enough time in
    school. Instead of debating the amount of time that should be spent in
    school (and on schoolwork), this article addresses how the available
    instructional time might be optimally utilized via the scheduling of review
    or practice. Hundreds of studies in cognitive and educational psychology
    have demonstrated that spacing out repeated encounters with the material
    over time produces superior long-term learning, compared with repetitions
    that are massed together. Also, incorporating tests into spaced practice
    amplifies the benefits. Spaced review or practice enhances diverse forms of
    learning, including memory, problem solving, and generalization to new
    situations. Spaced practice is a feasible and cost-effective way to improve
    the effectiveness and efficiency of learning, and has tremendous potential
    to improve educational outcomes. The article also discusses barriers to
    adopting spaced practice, recent developments, and their possible
    implications.

- key: Kapser2008
  kind: article
  author:
  - Cory J. Kapser
  - Michael W. Godfrey
  title: "''Cloning Considered Harmful'' Considered Harmful: Patterns of Cloning in Software"
  journal: ESE
  volume: 13
  number: 6
  year: 2008
  doi: 10.1007/s10664-008-9076-6
  abstract: >
    Literature on the topic of code cloning often asserts that duplicating code
    within a software system is a bad practice, that it causes harm to the
    system’s design and should be avoided. However, in our studies, we have
    found significant evidence that cloning is often used in a variety of ways
    as a principled engineering tool. For example, one way to evaluate possible
    new features for a system is to clone the affected subsystems and introduce
    the new features there, in a kind of sandbox testbed. As features mature and
    become stable within the experimental subsystems, they can be migrated
    incrementally into the stable code base; in this way, the risk of
    introducing instabilities in the stable version is minimized. This paper
    describes several patterns of cloning that we have observed in our case
    studies and discusses the advantages and disadvantages associated with using
    them. We also examine through a case study the frequencies of these clones
    in two medium-sized open source software systems, the Apache web server and
    the Gnumeric spreadsheet application. In this study, we found that as many
    as 71% of the clones could be considered to have a positive impact on the
    maintainability of the software system.

- key: Karabel2006
  kind: book
  author:
  - Jerome Karabel
  title: "The Chosen: The Hidden History of Admission and Exclusion at Harvard, Yale, and Princeton"
  year: 2006
  isbn: 978-0618773558
  publisher: Mariner Books

- key: Kaufman2000
  kind: article
  author:
  - Deborah B. Kaufman
  - Richard M. Felder
  title: Accounting for Individual Effort in Cooperative Learning Teams
  journal: Engineering Education
  volume: 89
  number: 2
  year: 2000
  doi: 10.1002/j.2168-9830.2000.tb00507.x
  abstract: >
    An “autorating” (peer rating) system designed to account for individual
    performance in team projects was used in two sophomore-level chemical
    engineering courses in which the students did their homework in cooperative
    learning teams. Team members confidentially rated how well they and each of
    their teammates fulfilled their responsibilities, the ratings were converted
    to individual weighting factors, and individual project grades were computed
    as the product of the team project grade and the weighting
    factor. Correlations were computed between ratings and grades, self-ratings
    and ratings from teammates, and ratings received and given by men and women
    and by ethnic minorities and non-minorities. Incidences of “hitchhikers”
    (students whose performance was considered less than satisfactory by their
    teammates), “tutors” (students who received top ratings from all of their
    teammates), dysfunctional teams, and teams agreeing on a common rating were
    also determined. The results suggest that the autorating system works
    exceptionally well as a rule, and the benefits it provides more than
    compensate for the relatively infrequent problems that may occur in its use.

- key: Kemerer2009
  kind: article
  author:
  - Chris F. Kemerer
  - Mark C. Paulk
  title: "The Impact of Design and Code Reviews on Software Quality: An Empirical Study Based on PSP Data"
  journal: TSE
  volume: 35
  number: 4
  year: 2009
  doi: 10.1109/tse.2009.27
  abstract: >
    This research investigates the effect of review rate on defect removal
    effectiveness and the quality of software products, while controlling for a
    number of potential confounding factors. Two data sets of 371 and 246
    programs, respectively, from a personal software process (PSP) approach were
    analyzed using both regression and mixed models. Review activities in the
    PSP process are those steps performed by the developer in a traditional
    inspection process. The results show that the PSP review rate is a
    significant factor affecting defect removal effectiveness, even after
    accounting for developer ability and other significant process
    variables. The recommended review rate of 200 LOC/hour or less was found to
    be an effective rate for individual reviews, identifying nearly two-thirds
    of the defects in design reviews and more than half of the defects in code
    reviews.

- key: Kendzior2018
  kind: book
  author:
  - Sarah Kendzior
  title: "The View from Flyover Country: Dispatches from the Forgotten America"
  year: 2018
  isbn: 978-1250189998
  publisher: Flatiron Books

- key: Kendzior2020
  kind: book
  author:
  - Sarah Kendzior
  title: "Hiding in Plain Sight: The Invention of Donald Trump and the Erosion of America"
  year: 2020
  isbn: 978-1250210715
  publisher: Flatiron Books

- key: Kerievsky2004
  kind: book
  author:
  - Joshua Kerievsky
  title: Refactoring to Patterns
  year: 2004
  isbn: 978-0321213358
  publisher: Addison-Wesley Professional

- key: Kernighan1979
  kind: book
  author:
  - Brian W. Kernighan
  - P. J. Plauger
  title: The Elements of Programming Style
  year: 1979
  isbn: 978-0070342071
  publisher: McGraw-Hill
  note: An early and influential description of the Unix programming philosophy.

- key: Kernighan1981
  kind: book
  author:
  - Brian W. Kernighan
  - P. J. Plauger
  title: Software Tools in Pascal
  year: 1981
  isbn: 978-0201103427
  publisher: Addison-Wesley Professional

- key: Kernighan1983
  kind: book
  author:
  - Brian W. Kernighan
  - Rob Pike
  title: The Unix Programming Environment
  year: 1983
  isbn: 978-0139376818
  publisher: Prentice-Hall
  note: An influential early description of Unix.

- key: Kernighan1988
  kind: book
  author:
  - Brian W. Kernighan
  - Dennis M. Ritchie
  title: The C Programming Language
  year: 1988
  isbn: 978-0131103627
  publisher: Prentice-Hall
  note: The book that made C a popular programming language.

- key: Kernighan1999
  kind: book
  author:
  - Brian W. Kernighan
  - Rob Pike
  title: The Practice of Programming
  year: 1999
  isbn: 978-8177582482
  publisher: Addison-Wesley

- key: Khomh2012
  kind: inproceedings
  author:
  - Foutse Khomh
  - Tejinder Dhaliwal
  - Ying Zou
  - Bram Adams
  title: Do faster releases improve software quality? An empirical case study of Mozilla Firefox
  booktitle: Proc. MSR'12
  month: '6'
  year: 2012
  doi: 10.1109/msr.2012.6224279
  publisher: IEEE
  url: https://doi.org/10.1109/msr.2012.6224279
  abstract: >
    Nowadays, many software companies are shifting from the traditional 18-month
    release cycle to shorter release cycles. For example, Google Chrome and
    Mozilla Firefox release new versions every 6 weeks. These shorter release
    cycles reduce the users' waiting time for a new release and offer better
    marketing opportunities to companies, but it is unclear if the quality of
    the software product improves as well, since shorter release cycles result
    in shorter testing periods. In this paper, we empirically study the
    development process of Mozilla Firefox in 2010 and 2011, a period during
    which the project transitioned to a shorter release cycle. We compare crash
    rates, median uptime, and the proportion of post-release bugs of the
    versions that had a shorter release cycle with those having a traditional
    release cycle, to assess the relation between release cycle length and the
    software quality observed by the end user. We found that (1) with shorter
    release cycles, users do not experience significantly more post-release bugs
    and (2) bugs are fixed faster, yet (3) users experience these bugs earlier
    during software execution (the program crashes earlier).

- key: Kirschner2006
  kind: article
  author:
  - Paul A. Kirschner
  - John Sweller
  - Richard E. Clark
  title: "Why Minimal Guidance During Instruction does not Work: An Analysis of the Failure of Constructivist, Discovery, Problem-Based, Experiential, and Inquiry-Based Teaching"
  journal: Educational Psychologist
  volume: 41
  number: 2
  year: 2006
  doi: 10.1207/s15326985ep4102_1
  abstract: >
    Evidence for the superiority of guided instruction is explained in the
    context of our knowledge of human cognitive architecture, expert–novice
    differences, and cognitive load. Although unguided or minimally guided
    instructional approaches are very popular and intuitively appealing, the
    point is made that these approaches ignore both the structures that
    constitute human cognitive architecture and evidence from empirical studies
    over the past half-century that consistently indicate that minimally guided
    instruction is less effective and less efficient than instructional
    approaches that place a strong emphasis on guidance of the student learning
    process. The advantage of guidance begins to recede only when learners have
    sufficiently high prior knowledge to provide "internal" guidance. Recent
    developments in instructional research and instructional design models that
    support guidance during instruction are briefly described.

- key: Kirschner2018
  kind: article
  author:
  - Paul A. Kirschner
  - John Sweller
  - Femke Kirschner
  - Jimmy Zambrano R.
  title: From Cognitive Load Theory to Collaborative Cognitive Load Theory
  journal: International Journal of Computer-Supported Collaborative Learning
  volume: 13
  year: 2018
  doi: 10.1007/s11412-018-9277-y
  abstract: >
    Cognitive load theory has traditionally been associated with individual
    learning. Based on evolutionary educational psychology and our knowledge of
    human cognition, particularly the relations between working memory and
    long-term memory, the theory has been used to generate a variety of
    instructional effects. Though these instructional effects also influence the
    efficiency and effectiveness of collaborative learning, be it computer
    supported or face-to-face, they are often not considered either when
    designing collaborative learning situations/environments or researching
    collaborative learning. One reason for this omission is that cognitive load
    theory has only sporadically concerned itself with certain particulars of
    collaborative learning such as the concept of a collective working memory
    when collaborating along with issues associated with transactive activities
    and their concomitant costs which are inherent to collaboration. We
    illustrate how and why cognitive load theory, by adding these concepts, can
    throw light on collaborative learning and generate principles specific to
    the design and study of collaborative learning.

- key: Kohavi2020
  kind: book
  author:
  - Ron Kohavi
  - Diane Tang
  - Ya Xu
  title: "Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing"
  year: 2020
  isbn: 978-1108724265
  publisher: Cambridge University Press
  note: A guide to statistics and methodology that draws on the authors' experience.

- key: Krein2016
  kind: article
  author:
  - Jonathan L. Krein
  - Lutz Prechelt
  - Natalia Juristo
  - Aziz Nanthaamornphong
  - Jeffrey C. Carver
  - Sira Vegas
  - Charles D. Knutson
  - Kevin D. Seppi
  - Dennis L. Eggett
  title: A Multi-Site Joint Replication of a Design Patterns Experiment Using Moderator Variables to Generalize across Contexts
  journal: TSE
  volume: 42
  number: 4
  year: 2016
  doi: 10.1109/tse.2015.2488625
  abstract: >
    Context. Several empirical studies have explored the benefits of software
    design patterns, but their collective results are highly
    inconsistent. Resolving the inconsistencies requires investigating
    moderators—i.e., variables that cause an effect to differ across
    contexts. Objectives. Replicate a design patterns experiment at multiple
    sites and identify sufficient moderators to generalize the results across
    prior studies. Methods. We perform a close replication of an experiment
    investigating the impact (in terms of time and quality) of design patterns
    (Decorator and Abstract Factory) on software maintenance. The experiment was
    replicated once previously, with divergent results. We execute our
    replication at four universities—spanning two continents and three
    countries—using a new method for performing distributed replications based
    on closely coordinated, small-scale instances (“joint replication”). We
    perform two analyses: 1) a post-hoc analysis of moderators, based on
    frequentist and Bayesian statistics; 2) an a priori analysis of the original
    hypotheses, based on frequentist statistics. Results. The main effect
    differs across the previous instances of the experiment and across the sites
    in our distributed replication. Our analysis of moderators (including
    developer experience and pattern knowledge) resolves the differences
    sufficiently to allow for cross-context (and cross-study) conclusions. The
    final conclusions represent 126 participants from five universities and 12
    software companies, spanning two continents and at least four
    countries. Conclusions. The Decorator pattern is found to be preferable to a
    simpler solution during maintenance, as long as the developer has at least
    some prior knowledge of the pattern. For Abstract Factory, the simpler
    solution is found to be mostly equivalent to the pattern solution. Abstract
    Factory is shown to require a higher level of knowledge and/or experience
    than Decorator for the pattern to be beneficial.

- key: Krueger2020
  kind: inproceedings
  author:
  - Ryan Krueger
  - Yu Huang
  - Xinyu Liu
  - Tyler Santander
  - Westley Weimer
  - Kevin Leach
  title: "Neurological Divide: An fMRI Study of Prose and Code Writing"
  booktitle: Proc. ICSE'20
  year: 2020
  doi: 10.1145/3377811.3380348
  abstract: >
    Software engineering involves writing new code or editing existing
    code. Recent efforts have investigated the neural processes associated with
    reading and comprehending code — however, we lack a thorough understanding
    of the human cognitive processes underlying code writing. While prose
    reading and writing have been studied thoroughly, that same scrutiny has not
    been applied to code writing. In this paper, we leverage functional brain
    imaging to investigate neural representations of code writing in comparison
    to prose writing. We present the first human study in which participants
    wrote code and prose while undergoing a functional magnetic resonance
    imaging (fMRI) brain scan, making use of a full-sized fMRI-safe QWERTY
    keyboard. We find that code writing and prose writing are significantly
    dissimilar neural tasks. While prose writing entails significant left
    hemisphere activity associated with language, code writing involves more
    activations of the right hemisphere, including regions associated with
    attention control, working memory, planning and spatial cognition. These
    findings are unlike existing work in which code and prose comprehension were
    studied. By contrast, we present the first evidence suggesting that code and
    prose writing are quite dissimilar at the neural level.

- key: Kruger1999
  kind: article
  author:
  - Justin Kruger and David Dunning
  title: "Unskilled and Unaware of It: How Difficulties in Recognizing Incompetence Lead to Inflated Self-Assessment"
  journal: Personality and Social Psychology
  volume: 77
  number: 6
  year: 1999
  doi: 10.1037/0022-3514.77.6.1121
  abstract: >
    People tend to hold overly favorable views of their abilities in many social
    and intellectual domains. The authors suggest that this overestimation
    occurs, in part, because people who are unskilled in these domains suffer a
    dual burden: Not only do these people reach erroneous conclusions and make
    unfortunate choices, but their incompetence robs them of the metacognitive
    ability to realize it. Across 4 studies, the authors found that participants
    scoring in the bottom quartile on tests of humor, grammar, and logic grossly
    overestimated their test performance and ability. Although their test scores
    put them in the 12th percentile, they estimated themselves to be in the
    62nd. Several analyses linked this miscalibration to deficits in
    metacognitive skill, or the capacity to distinguish accuracy from
    error. Paradoxically, improving the skills of participants, and thus
    increasing their metacognitive competence, helped them recognize the
    limitations of their abilities.

- key: Lave1991
  kind: book
  author:
  - Jean Lave
  - Etienne Wenger
  title: "Situated Learning: Legitimate Peripheral Participation"
  year: 1991
  isbn: 978-0521423748
  publisher: Cambridge University Press

- key: LeGoues2019
  kind: article
  author:
  - Claire Le Goues
  - Michael Pradel
  - Abhik Roychoudhury
  title: Automated Program Repair
  journal: CACM
  volume: 62
  number: 12
  year: 2019
  doi: 10.1145/3318162
  abstract: >
    Automated program repair can relieve programmers from the burden of manually
    fixing the ever-increasing number of programming mistakes.

- key: Lee1962
  kind: article
  author:
  - Stan Lee
  title: The Amazing Spider-Man
  journal: Amazing Fantasy
  volume: 15
  year: 1962
  publisher: Marvel

- key: Leitao2019
  kind: article
  author:
  - "Roxanne Leitão"
  title: "Technology-Facilitated Intimate Partner Abuse: A Qualitative Analysis of Data from Online Domestic Abuse Forums"
  journal: Human--Computer Interaction
  year: 2019
  doi: 10.1080/07370024.2019.1685883
  abstract: >
    This article reports on a qualitative analysis of data gathered from three
    online discussion forums for victims and survivors of domestic abuse. The
    analysis focussed on technology-facilitated abuse and the findings cover
    three main themes, namely, 1) forms of technology-facilitated abuse being
    discussed on the forums, 2) the ways in which forum members are using
    technology within the context of intimate partner abuse, and 3) the digital
    privacy and security advice being exchanged between victims/survivors on the
    forums. The article concludes with a discussion on the dual role of digital
    technologies within the context of intimate partner abuse, on the challenges
    and advantages of digital ubiquity, as well as on the issues surrounding
    digital evidence of abuse, and the labor of managing digital privacy and
    security.

- key: Lin2020
  kind: article
  author:
  - Sarah Lin
  - Ibraheem Ali
  - Greg Wilson
  title: Ten Quick Tips for Making Things Findable
  journal: PLOS Computational Biology
  volume: 16
  number: 12
  year: 2020
  doi: 10.1371/journal.pcbi.1008469
  abstract: >
    The distribution of scholarly content today happens in the context of an
    immense deluge of information found on the internet. As a result,
    researchers face serious challenges when archiving and finding information
    that relates to their work. Library science principles provide a framework
    for navigating information ecosystems in order to help researchers improve
    findability of their professional output. Here, we describe the information
    ecosystem which consists of users, context, and content, all 3 of which must
    be addressed to make information findable and usable. We provide a set of
    tips that can help researchers evaluate who their users are, how to archive
    their research outputs to encourage findability, and how to leverage
    structural elements of software to make it easier to find information within
    and beyond their publications. As scholars evaluate their research
    communication strategies, they can use these steps to improve how their
    research is discovered and reused.

- key: Lindberg2008
  kind: book
  author:
  - Van Lindberg
  title: "Intellectual Property and Open Source: A Practical Guide to Protecting Code"
  year: 2008
  isbn: 978-0596517960
  publisher: O'Reilly

- key: Linklater2013
  kind: book
  author:
  - Andro Linklater
  title: "Owning the Earth: The Transforming History Of Land Ownership"
  year: 2013
  isbn: 978-1620402894
  publisher: Bloomsbury USA

- key: Littky2004
  kind: book
  author:
  - Dennis Littky
  title: "The Big Picture: Education Is Everyone''s Business"
  year: 2004
  isbn: 978-0871209719
  publisher: Association for Supervision & Curriculum Development

- key: Mahmoudi2019
  kind: inproceedings
  author:
  - Mehran Mahmoudi
  - Sarah Nadi
  - Nikolaos Tsantalis
  title: Are Refactorings to Blame? An Empirical Study of Refactorings in Merge Conflicts
  booktitle: Proc. SANER'19
  month: '2'
  year: 2019
  doi: 10.1109/saner.2019.8668012
  publisher: IEEE
  url: https://doi.org/10.1109/saner.2019.8668012
  abstract: >
    With the rise of distributed software development, branching has become a
    popular approach that facilitates collaboration between software
    developers. One of the biggest challenges that developers face when using
    multiple development branches is dealing with merge conflicts. Conflicts
    occur when inconsistent changes happen to the code. Resolving these
    conflicts can be a cumbersome task as it requires prior knowledge about the
    changes in each of the development branches. A type of change that could
    potentially lead to complex conflicts is code refactoring. Previous studies
    have proposed techniques for facilitating conflict resolution in the
    presence of refactorings. However, the magnitude of the impact that
    refactorings have on merge conflicts has never been empirically
    evaluated. In this paper, we perform an empirical study on almost 3,000
    well-engineered open-source Java software repositories and investigate the
    relation between merge conflicts and 15 popular refactoring types. Our
    results show that refactoring operations are involved in 22% of merge
    conflicts, which is remarkable taking into account that we investigated a
    relatively small subset of all possible refactoring types. Furthermore,
    certain refactoring types, such as EXTRACT METHOD, tend to be more
    problematic for merge conflicts. Our results also suggest that conflicts
    that involve refactored code are usually more complex, compared to conflicts
    with no refactoring changes.

- key: Majumder2019
  kind: article
  author:
  - Suvodeep Majumder
  - Joymallya Chakraborty
  - Amritanshu Agrawal
  - Tim Menzies
  title: Why Software Projects need Heroes (Lessons Learned from 1100+ Projects)
  journal: CoRR
  volume: abs/1904.09954
  year: 2019
  url: http://arxiv.org/abs/1904.09954

- key: Malloy2017
  kind: inproceedings
  author:
  - Brian A. Malloy
  - James F. Power
  title: "Quantifying the Transition from Python 2 to 3: An Empirical Study of Python Applications"
  booktitle: Proc. ESEM'17
  month: '11'
  year: 2017
  doi: 10.1109/esem.2017.45
  publisher: IEEE
  url: https://doi.org/10.1109/esem.2017.45
  abstract: >
    Background: Python is one of the most popular modern programming
    languages. In 2008 its authors introduced a new version of the language,
    Python 3.0, that was not backward compatible with Python 2, initiating a
    transitional phase for Python software developers. Aims: The study described
    in this paper investigates the degree to which Python software developers
    are making the transition from Python 2 to Python 3. Method: We have
    developed a Python compliance analyser, PyComply, and have assembled a large
    corpus of Python applications. We use PyComply to measure and quantify the
    degree to which Python 3 features are being used, as well as the rate and
    context of their adoption. Results: In fact, Python software developers are
    not exploiting the new features and advantages of Python 3, but rather are
    choosing to retain backward compatibility with Python 2. Conclusions: Python
    developers are confining themselves to a language subset, governed by the
    diminishing intersection of Python 2, which is not under development, and
    Python 3, which is under development with new features being introduced as
    the language continues to evolve.

- key: Manns2015
  kind: book
  author:
  - Mary Lynn Manns
  - Linda Rising
  title: "Fearless Change: Patterns for Introducing New Ideas"
  year: 2015
  isbn: 978-0201741575
  publisher: Addison-Wesley

- key: Margolis2002
  kind: book
  author:
  - Jane Margolis and Allan Fisher
  title: "Unlocking the Clubhouse: Women in Computing"
  year: 2002
  isbn: 978-0262632690
  publisher: MIT Press

- key: Mark2008
  kind: inproceedings
  author:
  - Gloria Mark
  - Daniela Gudith
  - Ulrich Klocke
  title: "The Cost of Interrupted Work: More Speed and Stress"
  booktitle: Proc. CHI'08
  year: 2008
  doi: 10.1145/1357054.1357072
  abstract: >
    We performed an empirical study to investigate whether the context of
    interruptions makes a difference. We found that context does not make a
    difference but surprisingly, people completed interrupted tasks in less time
    with no difference in quality. Our data suggests that people compensate for
    interruptions by working faster, but this comes at a price: experiencing
    more stress, higher frustration, time pressure and effort. Individual
    differences exist in the management of interruptions: personality measures
    of openness to experience and need for personal structure predict disruption
    costs of interruptions. We discuss implications for how system design can
    support interrupted work.

- key: Masood2018
  kind: article
  author:
  - Zainab Masood
  - Rashina Hoda
  - Kelly Blincoe
  title: Adapting Agile Practices in University Contexts
  journal: Journal of Systems and Software
  volume: 144
  year: 2018
  doi: 10.1016/j.jss.2018.07.011
  abstract: >
    Teaching agile practices has found its place in software engineering
    curricula in many universities across the globe. As a result, educators and
    students have embraced different ways to apply agile practices during their
    courses through lectures, games, projects, workshops and more for effective
    theoretical and practical learning. Practicing agile in university contexts
    comes with challenges for students and to counter these challenges, they
    perform some adaptations to standard agile practices making them effective
    and easier to use in university contexts. This study describes the
    constraints the students faced while applying agile practices in a
    university course taught at the University of Auckland, including difficulty
    in setting up common time for all team members to work together, limited
    availability of customer due to busy schedule and the modifications the
    students introduced to adapt agile practices to suit the university context,
    such as daily stand-ups with reduced frequency, combining sprint meetings,
    and rotating scrum master from team. In addition, it summarizes the
    effectiveness of these modifications based on reflection of the
    students. Recommendations for educators and students are also provided. Our
    findings and recommendations will help educators and students better
    coordinate and apply agile practices on industry-based projects in
    university contexts.

- key: May2019
  kind: article
  author:
  - Anna May
  - Johannes Wachs
  - "Anikó Hannák"
  title: Gender Differences in Participation and Reward on Stack Overflow
  journal: ESE
  volume: 24
  number: 4
  year: 2019
  doi: 10.1007/s10664-019-09685-x
  abstract: >
    Programming is a valuable skill in the labor market, making the
    underrepresentation of women in computing an increasingly important
    issue. Online question and answer platforms serve a dual purpose in this
    field: they form a body of knowledge useful as a reference and learning
    tool, and they provide opportunities for individuals to demonstrate
    credible, verifiable expertise. Issues, such as male-oriented site design or
    overrepresentation of men among the site’s elite may therefore compound the
    issue of women’s underrepresentation in IT. In this paper we audit the
    differences in behavior and outcomes between men and women on Stack
    Overflow, the most popular of these Q&A sites. We observe significant
    differences in how men and women participate in the platform and how
    successful they are. For example, the average woman has roughly half of the
    reputation points, the primary measure of success on the site, of the
    average man. Using an Oaxaca-Blinder decomposition, an econometric technique
    commonly applied to analyze differences in wages between groups, we find
    that most of the gap in success between men and women can be explained by
    differences in their activity on the site and differences in how these
    activities are rewarded. Specifically, 1) men give more answers than women
    and 2) are rewarded more for their answers on average, even when controlling
    for possible confounders such as tenure or buy-in to the site. Women ask
    more questions and gain more reward per question. We conclude with a
    hypothetical redesign of the site’s scoring system based on these behavioral
    differences, cutting the reputation gap in half.

- key: Mayer2009
  kind: book
  author:
  - Richard E. Mayer
  title: Multimedia Learning
  edition: 2nd
  year: 2009
  isbn: 978-0521735353
  publisher: Cambridge University Press

- key: McConnell2004
  kind: book
  author:
  - Steve McConnell
  title: "Code Complete: A Practical Handbook of Software Construction"
  year: 2004
  isbn: 978-0735619678
  publisher: Microsoft Press

- key: McDonald2020
  kind: book
  author:
  - Malcolm McDonald
  title: "Web Security for Developers: Real Threats, Practical Defense"
  year: 2020
  isbn: 978-1593279943
  publisher: No Starch Press

- key: McIntosh2011
  kind: inproceedings
  author:
  - Shane McIntosh
  - Bram Adams
  - Thanh H.D. Nguyen
  - Yasutaka Kamei
  - Ahmed E. Hassan
  title: An empirical study of build maintenance effort
  booktitle: Proc. ICSE'11
  year: 2011
  doi: 10.1145/1985793.1985813
  publisher: ACM Press
  url: https://doi.org/10.1145/1985793.1985813
  abstract: >
    The build system of a software project is responsible for transforming
    source code and other development artifacts into executable programs and
    deliverables. Similar to source code, build system specifications require
    maintenance to cope with newly implemented features, changes to imported
    Application Program Interfaces (APIs), and source code restructuring. In
    this paper, we mine the version histories of one proprietary and nine open
    source projects of different sizes and domain to analyze the overhead that
    build maintenance imposes on developers. We split our analysis into two
    dimensions: (1) Build Coupling, i.e., how frequently source code changes
    require build changes, and (2) Build Ownership, i.e., the proportion of
    developers responsible for build maintenance. Our results indicate that,
    despite the difference in scale, the build system churn rate is comparable
    to that of the source code, and build changes induce more relative churn on
    the build system than source code changes induce on the source
    code. Furthermore, build maintenance yields up to a 27% overhead on source
    code development and a 44% overhead on test development. Up to 79% of source
    code developers and 89% of test code developers are significantly impacted
    by build maintenance, yet investment in build experts can reduce the
    proportion of impacted developers to 22% of source code developers and 24%
    of test code developers.

- key: McIntosh2015
  kind: article
  author:
  - Shane McIntosh
  - Yasutaka Kamei
  - Bram Adams
  - Ahmed E. Hassan
  title: An empirical study of the impact of modern code review practices on software quality
  journal: ESE
  volume: 21
  number: 5
  pages: 2146--2189
  month: '4'
  year: 2015
  doi: 10.1007/s10664-015-9381-9
  publisher: Springer Science and Business Media LLC
  url: https://doi.org/10.1007/s10664-015-9381-9
  abstract: >
    Software code review, i.e., the practice of having other team members
    critique changes to a software system, is a well-established best practice
    in both open source and proprietary software domains. Prior work has shown
    that formal code inspections tend to improve the quality of delivered
    software. However, the formal code inspection process mandates strict review
    criteria (e.g., in-person meetings and reviewer checklists) to ensure a base
    level of review quality, while the modern, lightweight code reviewing
    process does not. Although recent work explores the modern code review
    process, little is known about the relationship between modern code review
    practices and long-term software quality. Hence, in this paper, we study the
    relationship between post-release defects (a popular proxy for long-term
    software quality) and: (1) code review coverage, i.e., the proportion of
    changes that have been code reviewed, (2) code review participation, i.e.,
    the degree of reviewer involvement in the code review process, and (3) code
    reviewer expertise, i.e., the level of domain-specific expertise of the code
    reviewers. Through a case study of the Qt, VTK, and ITK projects, we find
    that code review coverage, participation, and expertise share a significant
    link with software quality. Hence, our results empirically confirm the
    intuition that poorly-reviewed code has a negative impact on software
    quality in large systems using modern reviewing tools.

- key: McMillanCottom2018
  kind: book
  author:
  - Tressie McMillan Cottom
  title: "Lower Ed: The Troubling Rise of For-Profit Colleges in the New Economy"
  year: 2018
  isbn: 978-1620974384
  publisher: The New Press

- key: Meneely2011
  kind: inproceedings
  author:
  - Andrew Meneely
  - Pete Rotella
  - Laurie Williams
  title: Does adding manpower also affect quality?
  booktitle: Proc. FSE'11
  year: 2011
  doi: 10.1145/2025113.2025128
  publisher: ACM Press
  url: https://doi.org/10.1145/2025113.2025128
  abstract: >
    With each new developer to a software development team comes a greater
    challenge to manage the communication, coordination, and knowledge transfer
    amongst teammates. Fred Brooks discusses this challenge in The Mythical
    Man-Month by arguing that rapid team expansion can lead to a complex team
    organization structure. While Brooks focuses on productivity loss as the
    negative outcome, poor product quality is also a substantial concern. But if
    team expansion is unavoidable, can any quality impacts be mitigated? Our
    objective is to guide software engineering managers by empirically analyzing
    the effects of team size, expansion, and structure on product quality. We
    performed an empirical, longitudinal case study of a large Cisco networking
    product over a five year history. Over that time, the team underwent periods
    of no expansion, steady expansion, and accelerated expansion. Using
    team-level metrics, we quantified characteristics of team expansion,
    including team size, expansion rate, expansion acceleration, and modularity
    with respect to department designations. We examined statistical
    correlations between our monthly team-level metrics and monthly
    product-level metrics. Our results indicate that increased team size and
    linear growth are correlated with later periods of better product
    quality. However, periods of accelerated team expansion are correlated with
    later periods of reduced software quality. Furthermore, our linear
    regression prediction model based on team metrics was able to predict the
    product's post-release failure rate within a 95% prediction interval for 38
    out of 40 months. Our analysis provides insight for project managers into
    how the expansion of development teams can impact product quality.

- key: Meneely2014
  kind: inproceedings
  author:
  - Andrew Meneely
  - Alberto C. Rodriguez Tejeda
  - Brian Spates
  - Shannon Trudeau
  - Danielle Neuberger
  - Katherine Whitlock
  - Christopher Ketant
  - Kayla Davis
  title: An Empirical Investigation of Socio-technical Code Review Metrics and Security Vulnerabilities
  booktitle: Proc. SSE'14
  year: 2014
  doi: 10.1145/2661685.2661687
  abstract: >
    One of the guiding principles of open source software development is to use
    crowds of developers to keep a watchful eye on source code. Eric Raymond
    declared Linus'' Law as "many eyes make all bugs shallow", with the
    socio-technical argument that high quality open source software emerges when
    developers combine together their collective experience and expertise to
    review code collaboratively. Vulnerabilities are a particularly nasty set of
    bugs that can be rare, difficult to reproduce, and require specialized
    skills to recognize. Does Linus' Law apply to vulnerabilities empirically?
    In this study, we analyzed 159,254 code reviews, 185,948 Git commits, and
    667 post-release vulnerabilities in the Chromium browser project. We
    formulated, collected, and analyzed various metrics related to Linus' Law to
    explore the connection between collaborative reviews and vulnerabilities
    that were missed by the review process. Our statistical association results
    showed that source code files reviewed by more developers are,
    counter-intuitively, more likely to be vulnerable (even after accounting for
    file size). However, files are less likely to be vulnerable if they were
    reviewed by developers who had experience participating on prior
    vulnerability-fixing reviews. The results indicate that lack of security
    experience and lack of collaborator familiarity are key risk factors in
    considering Linus’ Law with vulnerabilities.

- key: Menzies2016
  kind: article
  author:
  - Tim Menzies
  - William Nichols
  - Forrest Shull
  - Lucas Layman
  title: Are delayed issues harder to resolve? Revisiting cost-to-fix of defects throughout the lifecycle
  journal: ESE
  volume: 22
  number: 4
  pages: 1903--1935
  month: '11'
  year: 2016
  doi: 10.1007/s10664-016-9469-x
  publisher: Springer Science and Business Media LLC
  url: https://doi.org/10.1007/s10664-016-9469-x
  abstract: >
    Many practitioners and academics believe in a delayed issue effect (DIE);
    i.e. the longer an issue lingers in the system, the more effort it requires
    to resolve. This belief is often used to justify major investments in new
    development processes that promise to retire more issues sooner. This paper
    tests for the delayed issue effect in 171 software projects conducted around
    the world in the period from 2006–2014. To the best of our knowledge, this
    is the largest study yet published on this effect. We found no evidence for
    the delayed issue effect; i.e. the effort to resolve issues in a later phase
    was not consistently or substantially greater than when issues were resolved
    soon after their introduction. This paper documents the above study and
    explores reasons for this mismatch between this common rule of thumb and
    empirical data. In summary, DIE is not some constant across all
    projects. Rather, DIE might be an historical relic that occurs
    intermittently only in certain kinds of projects. This is a significant
    result since it predicts that new development processes that promise to
    faster retire more issues will not have a guaranteed return on investment
    (depending on the context where applied), and that a long-held truth in
    software engineering should not be considered a global truism.

- key: Meszaros2007
  kind: book
  author:
  - Gerard Meszaros
  title: "xUnit Test Patterns: Refactoring Test Code"
  year: 2007
  isbn: 978-0131495050
  publisher: Addison-Wesley
  note: A guide to writing good unit tests within a standard framework.

- key: Metcalfe2016
  kind: article
  author:
  - Janet Metcalfe
  title: Learning from Errors
  journal: Annual Review of Psychology
  volume: 68
  number: 1
  year: 2016
  doi: 10.1146/annurev-psych-010416-044022
  abstract: >
    &NA; Although error avoidance during learning appears to be the rule in
    American classrooms, laboratory studies suggest that it may be a
    counterproductive strategy, at least for neurologically typical
    students. Experimental investigations indicate that errorful learning
    followed by corrective feedback is beneficial to learning. Interestingly,
    the beneficial effects are particularly salient when individuals strongly
    believe that their error is correct: Errors committed with high confidence
    are corrected more readily than low‐confidence errors. Corrective feedback,
    including analysis of the reasoning leading up to the mistake, is
    crucial. Aside from the direct benefit to learners, teachers gain valuable
    information from errors, and error tolerance encourages students’ active,
    exploratory, generative engagement. If the goal is optimal performance in
    high‐stakes situations, it may be worthwhile to allow and even encourage
    students to commit and correct errors while they are in low‐stakes learning
    situations rather than to assiduously avoid errors at all costs.

- key: Meyer2014
  kind: inproceedings
  author:
  - "André N. Meyer"
  - Thomas Fritz
  - Gail C. Murphy
  - Thomas Zimmermann
  title: Software developers' perceptions of productivity
  booktitle: Proc. FSE'14
  year: 2014
  doi: 10.1145/2635868.2635892
  publisher: ACM Press
  url: https://doi.org/10.1145/2635868.2635892
  abstract: >
    The better the software development community becomes at creating software,
    the more software the world seems to demand. Although there is a large body
    of research about measuring and investigating productivity from an
    organizational point of view, there is a paucity of research about how
    software developers, those at the front-line of software construction, think
    about, assess and try to improve their productivity. To investigate software
    developers' perceptions of software development productivity, we conducted
    two studies: a survey with 379 professional software developers to help
    elicit themes and an observational study with 11 professional software
    developers to investigate emergent themes in more detail. In both studies,
    we found that developers perceive their days as productive when they
    complete many or big tasks without significant interruptions or context
    switches. Yet, the observational data we collected shows our participants
    performed significant task and activity switching while still feeling
    productive. We analyze such apparent contradictions in our findings and use
    the analysis to propose ways to better support software developers in a
    retrospection and improvement of their productivity through the development
    of new tools and the sharing of best practices.

- key: Meyvis2021
  kind: article
  author:
  - Tom Meyvis
  - Heeyoung Yoon
  title: Adding is Favoured Over Subtracting in Problem Solving
  journal: Nature
  volume: 592
  number: 7853
  year: 2021
  doi: 10.1038/d41586-021-00592-0

- key: Michaelson2004
  kind: book
  title: "Team-Based Learning: A Transformative Use of Small Groups in College Teaching"
  editor:
  - Larry K. Michaelson
  - Arletta Bauman Knight
  - L. Dee Fink
  year: 2004
  isbn: 978-1579220860
  publisher: Stylus Publishing

- key: Mildenberger2019
  kind: link
  author:
  - Matto Mildenberger
  title: The Tragedy of 'The Tragedy of the Commons'
  year: 2019
  url: https://blogs.scientificamerican.com/voices/the-tragedy-of-the-tragedy-of-the-commons/

- key: Miller1956
  kind: article
  author:
  - George A. Miller
  title: "The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information"
  journal: Psychological Review"
  volume: 63
  number: 2
  year: 1956
  doi: 10.1037/h0043158
  abstract: >
    First, the span of absolute judgment and the span of immediate memory impose
    severe limitations on the amount of information that we are able to receive,
    process, and remember. By organizing the stimulus input simultaneously into
    several dimensions and successively into a sequence or chunks, we manage to
    break (or at least stretch) this informational bottleneck. Second, the
    process of recoding is a very important one in human psychology and deserves
    much more explicit attention than it has received. In particular, the kind
    of linguistic recoding that people do seems to me to be the very lifeblood
    of the thought processes. Recoding procedures are a constant concern to
    clinicians, social psychologists, linguists, and anthropologists and yet,
    probably because recoding is less accessible to experimental manipulation
    than nonsense syllables or T mazes, the traditional experimental
    psychologist has contributed little or nothing to their
    analysis. Nevertheless, experimental techniques can be used, methods of
    recoding can be specified, behavioral indicants can be found. And I
    anticipate that we will find a very orderly set of relations describing what
    now seems an uncharted wilderness of individual differences. Third, the
    concepts and measures provided by the theory of information provide a
    quantitative way of getting at some of these questions. The theory provides
    us with a yardstick for calibrating our stimulus materials and for measuring
    the performance of our subjects. In the interests of communication I have
    suppressed the technical details of information measurement and have tried
    to express the ideas in more familiar terms; I hope this paraphrase will not
    lead you to think they are not useful in research. Informational concepts
    have already proved valuable in the study of discrimination and of language;
    they promise a great deal in the study of learning and memory; and it has
    even been proposed that they can be useful in the study of concept
    formation. A lot of questions that seemed fruitless twenty or thirty years
    ago may now be worth another look. In fact, I feel that my story here must
    stop just as it begins to get really interesting. And finally, what about
    the magical number seven? What about the seven wonders of the world, the
    seven seas, the seven deadly sins, the seven daughters of Atlas in the
    Pleiades, the seven ages of man, the seven levels of hell, the seven primary
    colors, the seven notes of the musical scale, and the seven days of the
    week? What about the seven-point rating scale, the seven categories for
    absolute judgment, the seven objects in the span of attention, and the seven
    digits in the span of immediate memory? For the present I propose to
    withhold judgment. Perhaps there is something deep and profound behind all
    these sevens, something just calling out for us to discover it. But I
    suspect that it is only a pernicious, Pythagorean coincidence.

- key: Miller1990
  kind: article
  author:
  - Barton P. Miller
  - Louis Fredriksen
  - Bryan So
  title: An Empirical Study of the Reliability of UNIX Utilities
  journal: CACM
  volume: 33
  number: 12
  year: 1990
  doi: 10.1145/96267.96279
  abstract: >
    The following section describes the tools we built to test the
    utilities. These tools include the fuzz (random character) generator, ptyjig
    (to test interactive utilities), and scripts to automate the testing
    process. Next, we will describe the tests we performed, giving the types of
    input we presented to the utilities. Results from the tests will follow
    along with an analysis of the results, including identification and
    classification of the program bugs that caused the crashes. The final
    section presents concluding remarks, including suggestions for avoiding the
    types of problems detected by our study and some commentary on the bugs we
    found. We include an Appendix with the user manual pages for fuzz and
    ptyjig.

- key: Miller2020
  kind: article
  author:
  - Barton Miller
  - Mengxiao Zhang
  - Elisa Heymann
  title: "The Relevance of Classic Fuzz Testing: Have We Solved This One?"
  journal: TSE
  year: 2020
  doi: 10.1109/tse.2020.3047766
  abstract: >
    As fuzz testing has passed its 30th anniversary, and in the face of the
    incredible progress in fuzz testing techniques and tools, the question
    arises if the classic, basic fuzz technique is still useful and applicable?
    In that tradition, we have updated the basic fuzz tools and testing scripts
    and applied them to a large collection of Unix utilities on Linux, FreeBSD,
    and MacOS. As before, our failure criteria was whether the program crashed
    or hung. We found that 9 crash or hang out of 74 utilities on Linux, 15 out
    of 78 utilities on FreeBSD, and 12 out of 76 utilities on MacOS. A total of
    24 different utilities failed across the three platforms. We note that these
    failure rates are somewhat higher than our in previous 1995, 2000, and 2006
    studies of the reliability of command line utilities. In the basic fuzz
    tradition, we debugged each failed utility and categorized the causes the
    failures. Classic categories of failures, such as pointer and array errors
    and not checking return codes, were still broadly present in the current
    results. In addition, we found a couple of new categories of failures
    appearing. We present examples of these failures to illustrate the
    programming practices that allowed them to happen. As a side note, we tested
    the limited number of utilities available in a modern programming language
    (Rust) and found them to be of no better reliability than the standard ones.

- key: Minahan1986
  kind: article
  author:
  - Anne Minahan
  title: Martha's Rules
  journal: Affilia
  volume: 1
  number: 2
  pages: 53--56
  month: '6'
  year: 1986
  doi: 10.1177/088610998600100206
  publisher: SAGE Publications
  url: https://doi.org/10.1177/088610998600100206
  abstract: >
    For several years I have been teaching a social work course on how social
    workers can make their employing organization more responsive to
    consumers. We'study how decisions are made in organizations. The students
    and I have become intrigued with the use of consensus decision-making in
    some organizations. Many—but by no means all—of these organizations are
    feminist organizations that wish to put feminist beliefs and philosophy into
    practice within organizations and to avoid structured administrative
    hierarchies for decision making. In their study of "consensus
    decision-making" organizations, students have reported that some
    participants are impatient with the time required by the use of consensus
    decision-making for all decisions—large and small. One student brought to
    class a copy of "Martha's Rules" that were developed by Martha's Housing
    Co-op for families in Madison, Wisconsin. "Martha's Rules" are not only an
    alternative to Robert's Rules, but provide ideas for people in organizations
    who are committed to consensus decision-making and who want to make it work
    well.

- key: Mishra2013
  kind: book
  author:
  - Pankaj Mishra
  title: "From the Ruins of Empire: The Revolt Against the West and the Remaking of Asia"
  year: 2013
  isbn: 978-1250037718
  publisher: Picador

- key: Mockus2010
  kind: inproceedings
  author:
  - Audris Mockus
  title: Organizational Volatility and Its Effects on Software Defects
  booktitle: Proc. FSE'10
  year: 2010
  doi: 10.1145/1882291.1882311
  abstract: >
    The key premise of an organization is to allow more efficient production,
    including production of high quality software. To achieve that, an
    organization defines roles and reporting relationships. Therefore, changes
    in organization's structure are likely to affect product's quality. We
    propose and investigate a relationship between developer-centric measures of
    organizational change and the probability of customer-reported defects in
    the context of a large software project. We find that the proximity to an
    organizational change is significantly associated with reductions in
    software quality. We also replicate results of several prior studies of
    software quality supporting findings that code, change, and developer
    characteristics affect fault-proneness. In contrast to prior studies we find
    that distributed development decreases quality. Furthermore, recent
    departures from an organization were associated with increased probability
    of customer-reported defects, thus demonstrating that in the observed
    context the organizational change reduces product quality.

- key: Monperrus2018
  kind: article
  author:
  - Martin Monperrus
  title: Automatic Software Repair
  journal: ACM Computing Surveys
  volume: 51
  number: 1
  year: 2018
  doi: 10.1145/3105906
  abstract: >
    This article presents a survey on automatic software repair. Automatic
    software repair consists of automatically finding a solution to software
    bugs without human intervention. This article considers all kinds of
    repairs. First, it discusses behavioral repair where test suites, contracts,
    models, and crashing inputs are taken as oracle. Second, it discusses state
    repair, also known as runtime repair or runtime recovery, with techniques
    such as checkpoint and restart, reconfiguration, and invariant
    restoration. The uniqueness of this article is that it spans the research
    communities that contribute to this body of knowledge: software engineering,
    dependability, operating systems, programming languages, and security. It
    provides a novel and structured overview of the diversity of bug oracles and
    repair operators used in the literature.

- key: Morrison2013
  kind: inproceedings
  author:
  - Patrick Morrison
  - Emerson Murphy-Hill
  title: Is programming knowledge related to age? An exploration of stack overflow
  booktitle: Proc. MSR'13
  month: '5'
  year: 2013
  doi: 10.1109/msr.2013.6624008
  publisher: IEEE
  url: https://doi.org/10.1109/msr.2013.6624008
  abstract: >
    Becoming an expert at programming is thought to take an estimated 10,000
    hours of deliberate practice. But what happens after that? Do programming
    experts continue to develop, do they plateau, or is there a decline at some
    point? A diversity of opinion exists on this matter, but many seem to think
    that aging brings a decline in adoption and absorption of new programming
    knowledge. We develop several research questions on this theme, and draw on
    data from StackOverflow (SO) to address these questions. The goal of this
    research is to support career planning and staff development for programmers
    by identifying age-related trends in SO data. We observe that programmer
    reputation scores increase relative to age well into the 50's, that
    programmers in their 30's tend to focus on fewer areas relative to those
    younger or older in age, and that there is not a strong correlation between
    age and scores in specific knowledge areas.

- key: Mosher2013
  kind: inproceedings
  author:
  - Gretchen A. Mosher
  title: Formation and Development of Effective Student Teams to Facilitate Team-Based Learning
  booktitle: Proc. ASEE North Midwest Section Conference
  year: 2013

- key: Moyn2010
  kind: book
  author:
  - Samuel Moyn
  title: "The Last Utopia: Human Rights in History"
  year: 2010
  isbn: 978-0674048720
  publisher: Belknap Press

- key: Mukherjee2011
  kind: book
  author:
  - Siddhartha Mukherjee
  title: "The Emperor of All Maladies: A Biography of Cancer"
  year: 2011
  isbn: 978-1439170915
  publisher: Scribner

- key: Nagappan2015
  kind: inproceedings
  author:
  - Meiyappan Nagappan
  - Romain Robbes
  - Yasutaka Kamei
  - Éric Tanter
  - Shane McIntosh
  - Audris Mockus
  - Ahmed E. Hassan
  title: An empirical study of goto in C code from GitHub repositories
  booktitle: Proc. FSE'15
  year: 2015
  doi: 10.1145/2786805.2786834
  publisher: ACM Press
  url: https://doi.org/10.1145/2786805.2786834
  abstract: >
    It is nearly 50 years since Dijkstra argued that goto obscures the flow of
    control in program execution and urged programmers to abandon the goto
    statement. While past research has shown that goto is still in use, little
    is known about whether goto is used in the unrestricted manner that Dijkstra
    feared, and if it is ‘harmful’ enough to be a part of a post-release
    bug. We, therefore, conduct a two part empirical study - (1) qualitatively
    analyze a statistically rep- resentative sample of 384 files from a
    population of almost 250K C programming language files collected from over
    11K GitHub repositories and find that developers use goto in C files for
    error handling (80.21±5%) and cleaning up resources at the end of a
    procedure (40.36 ± 5%); and (2) quantitatively analyze the commit history
    from the release branches of six OSS projects and find that no goto
    statement was re- moved/modified in the post-release phase of four of the
    six projects. We conclude that developers limit themselves to using goto
    appropriately in most cases, and not in an unrestricted manner like Dijkstra
    feared, thus suggesting that goto does not appear to be harmful in practice.

- key: Nakshatri2016
  kind: inproceedings
  author:
  - Suman Nakshatri
  - Maithri Hegde
  - Sahithi Thandra
  title: Analysis of exception handling patterns in Java projects
  booktitle: Proc. MSR'16
  year: 2016
  doi: 10.1145/2901739.2903499
  publisher: ACM Press
  url: https://doi.org/10.1145/2901739.2903499
  abstract: >
    Exception handling is a powerful tool provided by many pro- gramming
    languages to help developers deal with unforeseen conditions. Java is one of
    the few programming languages to enforce an additional compilation check on
    certain sub- classes of the Exception class through checked exceptions. As
    part of this study, empirical data was extracted from soft- ware projects
    developed in Java. The intent is to explore how developers respond to
    checked exceptions and identify common patterns used by them to deal with
    exceptions, checked or otherwise. Bloch’s book - “Effective Java” [1] was
    used as reference for best practices in exception handling - these
    recommendations were compared against results from the empirical
    data. Results of this study indicate that most programmers ignore checked
    exceptions and leave them un- noticed. Additionally, it is observed that
    classes higher in the exception class hierarchy are more frequently used as
    compared to specific exception subclasses.

- key: Nielebock2018
  kind: article
  author:
  - Sebastian Nielebock
  - Dariusz Krolikowski
  - "Jacob Krüger"
  - Thomas Leich
  - Frank Ortmeier
  title: "Commenting source code: is it worth it for small programming tasks?"
  journal: ESE
  volume: 24
  number: 3
  pages: 1418--1457
  month: '11'
  year: 2018
  doi: 10.1007/s10664-018-9664-z
  publisher: Springer Science and Business Media LLC
  url: https://doi.org/10.1007/s10664-018-9664-z
  abstract: >
    Maintaining a program is a time-consuming and expensive task in software
    engineering. Consequently, several approaches have been proposed to improve
    the comprehensibility of source code. One of such approaches are comments in
    the code that enable developers to explain the program with their own words
    or predefined tags. Some empirical studies indicate benefits of comments in
    certain situations, while others find no benefits at all. Thus, the real
    effect of comments on software development remains uncertain. In this
    article, we describe an experiment in which 277 participants, mainly
    professional software developers, performed small programming tasks on
    differently commented code. Based on quantitative and qualitative feedback,
    we i) partly replicate previous studies, ii) investigate performances of
    differently experienced participants when confronted with varying types of
    comments, and iii) discuss the opinions of developers on comments. Our
    results indicate that comments seem to be considered more important in
    previous studies and by our participants than they are for small programming
    tasks. While other mechanisms, such as proper identifiers, are considered
    more helpful by our participants, they also emphasize the necessity of
    comments in certain situations.

- key: Noble2009
  kind: article
  author:
  - William Stafford Noble
  title: A Quick Guide to Organizing Computational Biology Projects
  editor:
  - Fran Lewitter
  journal: PLoS Comp Bio
  volume: 5
  number: 7
  pages: e1000424
  month: '7'
  year: 2009
  doi: 10.1371/journal.pcbi.1000424
  publisher: Public Library of Science (PLoS)
  url: https://doi.org/10.1371/journal.pcbi.1000424
  abstract: >
    Most bioinformatics coursework focuses on algorithms, with perhaps some
    components devoted to learning programming skills and learning how to use
    existing bioinformatics software. Unfortunately, for students who are
    preparing for a research career, this type of curriculum fails to address
    many of the day-to-day organizational challenges associated with performing
    computational experiments. In practice, the principles behind organizing and
    documenting computational experiments are often learned on the fly, and this
    learning is strongly influenced by personal predilections as well as by
    chance interactions with collaborators or colleagues.
    The purpose of this article is to describe one good strategy for carrying
    out computational experiments. I will not describe profound issues such as
    how to formulate hypotheses, design experiments, or draw
    conclusions. Rather, I will focus on relatively mundane issues such as
    organizing files and directories and documenting progress. These issues are
    important because poor organizational choices can lead to significantly
    slower research progress. I do not claim that the strategies I outline here
    are optimal. These are simply the principles and practices that I have
    developed over 12 years of bioinformatics research, augmented with various
    suggestions from other researchers with whom I have discussed these issues.

- key: Noble2018
  kind: book
  author:
  - Safiya Umoja Noble
  title: "Algorithms of Oppression: How Search Engines Reinforce Racism"
  year: 2018
  isbn: 978-1479837243
  publisher: NYU Press

- key: Nystrom2014
  kind: book
  author:
  - Robert Nystrom
  title: Game Programming Patterns
  year: 2014
  isbn: 978-0990582908
  publisher: Genever Benning

- key: ONeil2017
  kind: book
  author: Cathy O'Neil
  title: "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy"
  year: 2017
  isbn: 978-0553418835
  publisher: Crown

- key: Oakley2004
  kind: article
  author:
  - Barbara Oakley
  - Richard M. Felder
  - Rebecca Brent
  - Imad Elhajj
  title: Turning Student Groups into Effective Teams
  journal: Journal of Student Centered Learning
  volume: 2
  number: 1
  year: 2004

- key: Olsen2007
  kind: book
  author:
  - Russ Olsen
  title: Design Patterns in Ruby
  year: 2007
  isbn: 978-0321490452
  publisher: Addison-Wesley Professional

- key: Oluo2019
  kind: book
  author:
  - Ijeoma Oluo
  title: So You Want to Talk About Race
  year: 2019
  isbn: 978-1580058827
  publisher: Seal Press

- key: Oram2007
  kind: book
  title: "Beautiful Code: Leading Programmers Explain How They Think"
  editor:
  - Andy Oram
  - Greg Wilson
  year: 2007
  isbn: 978-0596510046
  publisher: O'Reilly
  note: Chapter-length descriptions and analyses of programmers' favorite pieces of software.

- key: Oram2010
  kind: book
  title: "Making Software: What Really Works, and Why We Believe It"
  editor:
  - Andy Oram
  - Greg Wilson
  year: 2010
  isbn: 978-0596808327
  publisher: O'Reilly Media

- key: Osmani2017
  kind: link
  author:
  - Addy Osmani
  title: Learning JavaScript Design Patterns
  year: 2017
  url: https://addyosmani.com/resources/essentialjsdesignpatterns/book/
  note: A guide to design patterns using JavaScript.

- key: Ostrom2015
  kind: book
  author:
  - Elinor Ostrom
  title: "Governing the Commons: The Evolution of Institutions for Collective Action"
  year: 2015
  isbn: 978-1107569782
  publisher: Cambridge University Press

- key: Owhadikareshk2019
  kind: inproceedings
  author:
  - Moein Owhadi-Kareshk
  - Sarah Nadi
  - Julia Rubin
  title: Predicting Merge Conflicts in Collaborative Software Development
  booktitle: Proc. ESEM'19
  month: '9'
  year: 2019
  doi: 10.1109/esem.2019.8870173
  publisher: IEEE
  url: https://doi.org/10.1109/esem.2019.8870173
  abstract: >
    Background. During collaborative software development, developers often use
    branches to add features or fix bugs. When merging changes from two
    branches, conflicts may occur if the changes are inconsistent. Developers
    need to resolve these conflicts before completing the merge, which is an
    error-prone and time-consuming process. Early detection of merge conflicts,
    which warns developers about resolving conflicts before they become large
    and complicated, is among the ways of dealing with this problem. Existing
    techniques do this by continuously pulling and merging all combinations of
    branches in the background to notify developers as soon as a conflict
    occurs, which is a computationally expensive process. One potential way for
    reducing this cost is to use a machine-learning based conflict predictor
    that filters out the merge scenarios that are not likely to have conflicts,
    i.e.safe merge scenarios.Aims. In this paper, we assess if conflict
    prediction is feasible.Method. We design a classifier for predicting merge
    conflicts, based on 9 light-weight Git feature sets. To evaluate our
    predictor, we perform a large-scale study on 267,657 merge scenarios from
    744 GitHub repositories in seven programming languages.Results. Our results
    show that we achieve high f1-scores, varying from 0.95 to 0.97 for different
    programming languages, when predicting safe merge scenarios. The f1-score is
    between 0.57 and 0.68 for the conflicting merge
    scenarios.Conclusions. Predicting merge conflicts is feasible in practice,
    especially in the context of predicting safe merge scenarios as a
    pre-filtering step for speculative merging.

- key: Pan2008
  kind: article
  author:
  - Kai Pan
  - Sunghun Kim
  - E. James Whitehead
  title: Toward an Understanding of Bug Fix Patterns
  journal: ESE
  volume: 14
  number: 3
  year: 2008
  doi: 10.1007/s10664-008-9077-5
  abstract: >
    Twenty-seven automatically extractable bug fix patterns are defined using
    the syntax components and context of the source code involved in bug fix
    changes. Bug fix patterns are extracted from the configuration management
    repositories of seven open source projects, all written in Java (Eclipse,
    Columba, JEdit, Scarab, ArgoUML, Lucene, and MegaMek). Defined bug fix
    patterns cover 45.7% to 63.3% of the total bug fix hunk pairs in these
    projects. The frequency of occurrence of each bug fix pattern is computed
    across all projects. The most common individual patterns are MC-DAP (method
    call with different actual parameter values) at 14.9–25.5%, IF-CC (change in
    if conditional) at 5.6–18.6%, and AS-CE (change of assignment expression) at
    6.0–14.2%. A correlation analysis on the extracted pattern instances on the
    seven projects shows that six have very similar bug fix pattern
    frequencies. Analysis of if conditional bug fix sub-patterns shows a trend
    towards increasing conditional complexity in if conditional fixes. Analysis
    of five developers in the Eclipse projects shows overall consistency with
    project-level bug fix pattern frequencies, as well as distinct variations
    among developers in their rates of producing various bug patterns. Overall,
    data in the paper suggest that developers have difficulty with specific code
    situations at surprisingly consistent rates. There appear to be broad
    mechanisms causing the injection of bugs that are largely independent of the
    type of software being produced.

- key: Park2012
  kind: inproceedings
  author:
  - Jihun Park
  - Miryung Kim
  - Baishakhi Ray
  - Doo-Hwan Bae
  title: An Empirical Study of Supplementary Bug Fixes
  booktitle: Proc. MSR'12
  year: 2012
  doi: 10.1109/msr.2012.6224298
  abstract: >
    A recent study finds that errors of omission are harder for programmers to
    detect than errors of commission. While several change recommendation
    systems already exist to prevent or reduce omission errors during software
    development, there have been very few studies on why errors of omission
    occur in practice and how such errors could be prevented. In order to
    understand the characteristics of omission errors, this paper investigates a
    group of bugs that were fixed more than once in open source projects - those
    bugs whose initial patches were later considered incomplete and to which
    programmers applied supplementary patches. Our study on Eclipse JDT core,
    Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs
    (22% to 33%) involves more than one fix attempt. Our manual inspection shows
    that the causes of omission errors are diverse, including missed porting
    changes, incorrect handling of conditional statements, or incomplete
    refactorings, etc. While many consider that missed updates to code clones
    often lead to omission errors, only a very small portion of supplementary
    patches (12% in JDT, 25% in SWT, and 9% in Mozilla) have a content similar
    to their initial patches. This implies that supplementary change locations
    cannot be predicted by code clone analysis alone. Furthermore, 14% to 15% of
    files in supplementary patches are beyond the scope of immediate neighbors
    of their initial patch locations - they did not overlap with the initial
    patch locations nor had direct structural dependencies on them (e.g. calls,
    accesses, subtyping relations, etc.). These results call for new types of
    omission error prevention approaches that complement existing change
    recommendation systems.

- key: Parnin2010
  kind: article
  author:
  - Chris Parnin
  - Spencer Rugaber
  title: Resumption Strategies for Interrupted Programming Tasks
  journal: Software Quality Journal
  volume: 19
  number: 1
  year: 2010
  doi: 10.1007/s11219-010-9104-9
  abstract: >
    Interrupted and blocked tasks are a daily reality for professional
    programmers. Unfortunately, the strategies programmers use to recover lost
    knowledge and rebuild context when resuming work have not yet been well
    studied. In this paper, we describe an exploratory analysis performed on
    10,000 recorded sessions of 86 programmers and a survey of 414 programmers
    to understand the various strategies and coping mechanisms developers use to
    manage interrupted programming tasks. Based on the analysis, we propose a
    framework for understanding these strategies and suggest how task resumption
    might be better supported in future development tools. The results suggest
    that task resumption is a frequent and persistent problem for
    developers. For example, we find that only 10% of the sessions have
    programming activity resume in less than 1 min after an interruption, only
    7% of the programming sessions involve no navigation to other locations
    prior to editing. We also found that programmers use multiple coping
    mechanisms to recover task context when resuming work.

- key: Patitsas2016
  kind: inproceedings
  author:
  - Elizabeth Patitsas
  - Jesse Berlin
  - Michelle Craig
  - Steve Easterbrook
  title: Evidence That Computer Science Grades Are Not Bimodal
  booktitle: Proc. ICER'16
  year: 2016
  doi: 10.1145/2960310.2960312
  abstract: >
    It is commonly thought that CS grades are bimodal. We statistically analyzed
    778 distributions of final course grades from a large research university,
    and found only 5.8% of the distributions passed tests of multimodality. We
    then devised a psychology experiment to understand why CS educators believe
    their grades to be bimodal. We showed 53 CS professors a series of
    histograms displaying ambiguous distributions and asked them to categorize
    the distributions. A random half of participants were primed to think about
    the fact that CS grades are commonly thought to be bimodal; these
    participants were more likely to label ambiguous distributions as
    "bimodal". Participants were also more likely to label distributions as
    bimodal if they believed that some students are innately predisposed to do
    better at CS. These results suggest that bimodal grades are instructional
    folklore in CS, caused by confirmation bias and instructor beliefs about
    their students.

- key: Patterson2017
  kind: book
  author:
  - David A. Patterson
  - John L. Hennessy
  title: "Computer Organization and Design: The Hardware/Software Interface"
  year: 2017
  isbn: 978-0128122754
  publisher: Morgan Kaufmann
  note: The latest in a series of classic textbooks describing a methodical approach to hardware design.

- key: Peitek2021
  kind: inproceedings
  author:
  - Norman Peitek
  - Sven Apel
  - Chris Parning
  - "André Brechmann"
  - Janet Siegmund
  title: "Program Comprehension and Code Complexity Metrics: An fMRI Study"
  booktitle: Proc. ICSE'21
  year: 2021

- key: PerezDeRosso2013
  kind: inproceedings
  author:
  - Santiago Perez De Rosso
  - Daniel Jackson
  title: What's Wrong With Git?
  booktitle: Proc. Onward!'13
  year: 2013
  doi: 10.1145/2509578.2509584
  abstract: >
    It is commonly asserted that the success of a software development project,
    and the usability of the final product, depend on the quality of the
    concepts that underlie its design. Yet this hypothesis has not been
    systematically explored by researchers, and conceptual design has not played
    the central role in the research and teaching of software engineering that
    one might expect.
    As part of a new research project to explore conceptual design, we are
    engaging in a series of case studies. This paper reports on the early stages
    of our first study, on the Git version control system. Despite its
    widespread adoption, Git puzzles even experienced developers and is not
    regarded as easy to use. In an attempt to understand the root causes of its
    complexity, we analyze its conceptual model and identify some undesirable
    properties; we then propose a reworking of the conceptual model that forms
    the basis of (the first version of) Gitless, an ongoing effort to redesign
    Git and experiment with the effects of conceptual simplifications.

- key: PerezDeRosso2016
  kind: inproceedings
  author:
  - Santiago Perez De Rosso
  - Daniel Jackson
  title: Purposes,  Concepts,  Misfits,  and a Redesign of Git
  booktitle: Proc. OOPSLA'16
  year: 2016
  doi: 10.1145/2983990.2984018
  abstract: >
    Git is a widely used version control system that is powerful but
    complicated. Its complexity may not be an inevitable consequence of its
    power but rather evidence of flaws in its design. To explore this
    hypothesis, we analyzed the design of Git using a theory that identifies
    concepts, purposes, and misfits. Some well-known difficulties with Git are
    described, and explained as misfits in which underlying concepts fail to
    meet their intended purpose. Based on this analysis, we designed a reworking
    of Git (called Gitless) that attempts to remedy these flaws. To correlate
    misfits with issues reported by users, we conducted a study of Stack
    Overflow questions. And to determine whether users experienced fewer
    complications using Gitless in place of Git, we conducted a small user
    study. Results suggest our approach can be profitable in identifying,
    analyzing, and fixing design problems.

- key: Perri2018
  kind: book
  author:
  - Melissa Perri
  title: "Escaping the Build Trap: How Effective Product Management Creates Real Value"
  year: 2018
  isbn: 978-1491973790
  publisher: O'Reilly Media

- key: Petre2013
  kind: inproceedings
  author:
  - Marian Petre
  title: UML in Practice
  booktitle: Proc. ICSE'13
  year: 2013
  doi: 10.1109/icse.2013.6606618
  abstract: >
    UML has been described by some as “the lingua franca of software
    engineering”. Evidence from industry does not necessarily support such
    endorsements. How exactly is UML being used in industry - if it is? This
    paper presents a corpus of interviews with 50 professional software
    engineers in 50 companies and identifies 5 patterns of UML use.

- key: Petre2016
  kind: book
  author:
  - Marian Petre
  - "André van der Hoek"
  title: "Software Design Decoded: 66 Ways Experts Think"
  year: 2016
  isbn: 978-0262035187
  publisher: MIT Press

- key: Petzold2008
  kind: book
  author:
  - Charles Petzold
  title: The Annotated Turing
  year: 2008
  isbn: 978-0470229057
  publisher: Wiley
  note: A fascinating look at one of the pivotal moments in the history of computing.

- key: Phillips2019
  kind: book
  author:
  - Leigh Phillips
  - Michal Rozworski
  title: The People's Republic of Walmart
  year: 2019
  isbn: 978-1786635167
  publisher: Verso

- key: Pinzger2008
  kind: inproceedings
  author:
  - Martin Pinzger
  - Nachiappan Nagappan
  - Brendan Murphy
  title: Can developer-module networks predict failures?
  booktitle: Proc. FSE'08
  year: 2008
  doi: 10.1145/1453101.1453105
  publisher: ACM Press
  url: https://doi.org/10.1145/1453101.1453105
  abstract: >
    Software teams should follow a well defined goal and keep their work
    focused. Work fragmentation is bad for efficiency and quality. In this paper
    we empirically investigate the relationship between the fragmentation of
    developer contributions and the number of post-release failures. Our
    approach is to represent developer contributions with a developer-module
    network that we call contribution network. We use network centrality
    measures to measure the degree of fragmentation of developer
    contributions. Fragmentation is determined by the centrality of software
    modules in the contribution network. Our claim is that central software
    modules are more likely to be failure-prone than modules located in
    surrounding areas of the network. We analyze this hypothesis by exploring
    the network centrality of Microsoft Windows Vista binaries using several
    network centrality measures as well as linear and logistic regression
    analysis. In particular, we investigate which centrality measures are
    significant to predict the probability and number of post-release
    failures. Results of our experiments show that central modules are more
    failure-prone than modules located in surrounding areas of the
    network. Results further confirm that number of authors and number of
    commits are significant predictors for the probability of post-release
    failures. For predicting the number of post-release failures the closeness
    centrality measure is most significant.

- key: Porter2013
  kind: inproceedings
  author:
  - Leo Porter
  - Cynthia Bailey Lee
  - Beth Simon
  title: Halving fail rates using peer instruction
  booktitle: Proc. SIGCSE'13
  year: 2013
  doi: 10.1145/2445196.2445250
  publisher: ACM Press
  url: https://doi.org/10.1145/2445196.2445250
  abstract: >
    Peer Instruction (PI) is a teaching method that supports student-centric
    classrooms, where students construct their own understanding through a
    structured approach featuring questions with peer discussions. PI has been
    shown to increase learning in STEM disciplines such as physics and
    biology. In this report we look at another indicator of student success the
    rate at which students pass the course or, conversely, the rate at which
    they fail. Evaluating 10 years of instruction of 4 different courses
    spanning 16 PI course instances, we find that adoption of the PI methodology
    in the classroom reduces fail rates by a per-course average of 61% (20%
    reduced to 7%) compared to standard instruction (SI). Moreover, we also find
    statistically significant improvements within-instructor. For the same
    instructor teaching the same course, we find PI decreases the fail rate, on
    average, by 67% (from 23% to 8%) compared to SI. As an in-situ study, we
    discuss the various threats to the validity of this work and consider
    implications of wide-spread adoption of PI in computing programs.

- key: Posnett2011
  kind: inproceedings
  author:
  - Daryl Posnett
  - Abram Hindle
  - Prem Devanbu
  title: Got Issues? Do New Features and Code Improvements Affect Defects?
  booktitle: Proc. WCRE'11
  month: '10'
  year: 2011
  doi: 10.1109/wcre.2011.33
  publisher: IEEE
  url: https://doi.org/10.1109/wcre.2011.33
  abstract: >
    There is a perception that when new features are added to a system that
    those added and modified parts of the source-code are more fault prone. Many
    have argued that new code and new features are defect prone due to
    immaturity, lack of testing, as well unstable requirements. Unfortunately
    most previous work does not investigate the link between a concrete
    requirement or new feature and the defects it causes, in particular the
    feature, the changed code and the subsequent defects are rarely
    investigated. In this paper we investigate the relationship between
    improvements, new features and defects recorded within an issue tracker. A
    manual case study is performed to validate the accuracy of these issue
    types. We combine defect issues and new feature issues with the code from
    version-control systems that introduces these features, we then explore the
    relationship of new features with the fault-proneness of their
    implementations. We describe properties and produce models of the
    relationship between new features and fault proneness, based on the analysis
    of issue trackers and version-control systems. We find, surprisingly, that
    neither improvements nor new features have any significant effect on later
    defect counts, when controlling for size and total number of changes.

- key: Post2020
  kind: article
  author:
  - Martiqua L Post
  - Anthony Barrett
  - Marlyse Williams
  - Lauren Scharff
  title: Impact of Team Formation Method on Student Performance, Attitudes, and Behaviors
  journal: JSTL
  volume: 20
  number: 1
  year: 2020
  doi: 10.14434/josotl.v20i1.24128
  abstract: >
    This project examined the effects of two team selection methods
    (self-selected and instructorformed based on matched academic performance)
    on team and individual student performance and on self-reported attitudes
    and team behaviors in a freshman-level core-required introductory
    course. The data included mid and end-of-semester
    self-reports. Matched-performance groups had significantly higher grades on
    several performance measures, with a larger effect on the team grades than
    on the individual grades; however, overall the effect sizes were
    small. Although there were no group differences for most self-reported
    items, a key finding was that self-selected teams were significantly more
    likely to already have friends on their team, and a significant correlation
    showed that already having friends on a team was negatively correlated with
    many of the performance measures. In contrast, members of both types of
    teams reported equally high likelihood to make new friends, which was
    positively correlated with performance. Understanding the impact of
    different approaches to team formation may guide instructors and lead to
    more well-functioning teams, higher student learning, and greater student
    satisfaction.

- key: Prechelt2000
  kind: article
  author:
  - Lutz Prechelt
  title: An Empirical Comparison of Seven Programming Languages
  journal: IEEE Computer
  volume: 33
  number: 10
  year: 2000
  doi: 10.1109/2.876288
  abstract: >
    Often heated, debates regarding different programming languages'
    effectiveness remain inconclusive because of scarce data and a lack of
    direct comparisons. The author addresses that challenge, comparatively
    analyzing 80 implementations of the phone-code program in seven different
    languages (C, C++, Java, Perl, Python, Rexx and Tcl). Further, for each
    language, the author analyzes several separate implementations by different
    programmers. The comparison investigates several aspects of each language,
    including program length, programming effort, runtime efficiency, memory
    consumption, and reliability. The author uses comparisons to present insight
    into program language performance.

- key: Prechelt2019
  kind: incollection
  author: Lutz Prechelt
  title: The Mythical 10x Programmer
  booktitle: Rethinking Productivity in Software Engineering
  editor:
  - Caitlin Sadowski
  - Thomas Zimmermann
  year: 2019
  isbn: 978-1484242209
  publisher: Apress

- key: Prioleau2021
  kind: inproceedings
  author:
  - Diandra Prioleau
  - Brianna Richardson
  - Emma Drobina
  - Rua Williams
  - Joshua Martin
  - Juan E. Gilbert
  title: How Students in Computing-Related Majors Distinguish Social Implications of Technology
  booktitle: Proc. ICSE'21
  year: 2021
  doi: 10.1145/3408877.3432360
  abstract: >
    The demand for machine learning and data science has grown exponentially in
    recent years. Yet, as the influence of these fields reach farther into daily
    life, the disparate impacts of these algorithms and models on more
    marginalized populations have also begun to surface rapidly. To address this
    emerging crisis, it is necessary to equip the next generation of computer
    scientists with the ethical tools needed to tackle these issues. Thus, an
    exploratory study was conducted to investigate how students who are
    currently enrolled in computing-related programs evaluate and understand the
    ethical and social impact of technology. 43 students in computing majors
    were presented with 5 scenarios of different technologies that utilizes
    machine learning to address potentially sensitive areas (e.g. policing,
    medical diagnosing). The long-format responses to these scenarios were
    qualitatively analyzed. Additionally, quantitative analysis was conducted
    after qualitatively coding the long-format responses into four
    sentiments. Ultimately, we found that participants were able to decipher the
    social implications of technology. However, many issues of systemic
    discrimination were missing from participants' analysis. Alarmingly, our
    findings also indicated that 50% or more of participants were not exposed to
    most of the technologies highlighted in the scenarios, which highlights a
    potential gap in computing curriculum of connecting ethics as well as
    racial, cultural, and socioeconomic understanding to computer science. Based
    on these results, we suggest that computing-related curriculum be
    reevaluated with ethical training in mind.

- key: Pritchard2015
  kind: inproceedings
  author:
  - David Pritchard
  title: Frequency distribution of error messages
  booktitle: Proc. PLATEAU'15
  year: 2015
  doi: 10.1145/2846680.2846681
  publisher: ACM Press
  url: https://doi.org/10.1145/2846680.2846681
  abstract: >
    Which programming error messages are the most common? We investigate this
    question, motivated by writing error explanations for novices. We consider
    large data sets in Python and Java that include both syntax and run-time
    errors. In both data sets, after grouping essentially identical messages,
    the error message frequencies empirically resemble Zipf-Mandelbrot
    distributions. We use a maximum-likelihood approach to fit the distribution
    parameters. This gives one possible way to contrast languages or compilers
    quantitatively.

- key: Proksch2018
  kind: inproceedings
  author:
  - Sebastian Proksch
  - Sven Amann
  - Sarah Nadi
  title: Enriched event streams
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196400
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196400
  abstract: >
    Developers have been the subject of many empirical studies over the
    years. To assist developers in their everyday work, an understanding of
    their activities is necessary, especially how they develop source
    code. Unfortunately, conducting such studies is very expensive and
    researchers often resort to studying artifacts after the fact. To pave the
    road for future empirical studies on developer activities, we built FeedBaG,
    a general-purpose interaction tracker for Visual Studio that monitors
    development activities. The observations are stored in enriched event
    streams that encode a holistic picture of the in-IDE development
    process. Enriched event streams capture all commands invoked in the IDE with
    additional context information, such as the test being run or the
    accompanying fine-grained code edits. We used FeedBaG to collect enriched
    event streams from 81 developers. Over 1,527 days, we collected more than
    11M events that correspond to 15K hours of working time.

- key: Rabbani2018
  kind: inproceedings
  author:
  - Noam Rabbani
  - Michael S. Harvey
  - Sadnan Saquif
  - Keheliya Gallaba
  - Shane McIntosh
  title: Revisiting "programmers' build errors" in the visual studio context
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196469
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196469
  abstract: >
    Build systems translate sources into deliverables. Developers execute builds
    on a regular basis in order to integrate their personal code changes into
    testable deliverables. Prior studies have evaluated the rate at which builds
    in large organizations fail. A recent study at Google has analyzed (among
    other things) the rate at which builds in developer workspaces fail. In this
    paper, we replicate the Google study in the Visual Studio context of the MSR
    challenge. We extract and analyze 13,300 build events, observing that builds
    are failing 67%-76% less frequently and are fixed 46%-78% faster in our
    study context. Our results suggest that build failure rates are highly
    sensitive to contextual factors. Given the large number of factors by which
    our study contexts differ (e.g., system size, team size, IDE tooling,
    programming languages), it is not possible to trace the root cause for the
    large differences in our results. Additional data is needed to arrive at
    more complete conclusions.

- key: Rahman2013
  kind: inproceedings
  author:
  - Foyzur Rahman
  - Premkumar Devanbu
  title: How, and why, process metrics are better
  booktitle: Proc. ICSE'13
  month: '5'
  year: 2013
  doi: 10.1109/icse.2013.6606589
  publisher: IEEE
  url: https://doi.org/10.1109/icse.2013.6606589
  abstract: >
    Defect prediction techniques could potentially help us to focus
    quality-assurance efforts on the most defect-prone files. Modern statistical
    tools make it very easy to quickly build and deploy prediction
    models. Software metrics are at the heart of prediction models;
    understanding how and especially why different types of metrics are
    effective is very important for successful model deployment. In this paper
    we analyze the applicability and efficacy of process and code metrics from
    several different perspectives. We build many prediction models across 85
    releases of 12 large open source projects to address the performance,
    stability, portability and stasis of different sets of metrics. Our results
    suggest that code metrics, despite widespread use in the defect prediction
    literature, are generally less useful than process metrics for
    prediction. Second, we find that code metrics have high stasis; they don't
    change very much from release to release. This leads to stagnation in the
    prediction models, leading to the same files being repeatedly predicted as
    defective; unfortunately, these recurringly defective files turn out to be
    comparatively less defect-dense.

- key: Rahman2018
  kind: inproceedings
  author:
  - Akond Rahman
  title: Comprehension effort and programming activities
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196470
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196470
  abstract: >
    Researchers have observed programmers to allocate considerable amount of
    effort in program comprehension. But, how does program comprehension effort
    relate with programming activities? We answer this question by conducting an
    empirical study using the MSR 2018 Mining Challenge Dataset. We quantify
    programmers' comprehension effort, and investigate the relationship between
    program comprehension effort and four programming activities: navigating,
    editing, building projects, and debugging. We observe when programmers are
    involved in high comprehension effort they navigate and make edits at a
    significantly slower rate. However, we do not observe any significant
    differences in programmers' build and debugging behavior, when programmers
    are involved in high comprehension effort. Our findings suggest that the
    relationship between program comprehension effort and programming activities
    is nuanced, as not all programming activities associate with program
    comprehension effort.

- key: Rajagopalan2018
  kind: link
  author:
  - Megha Rajagopalan
  - Lam Thuy Vo
  - Aung Naing Soe
  title: How Facebook Failed The Rohingya In Myanmar
  year: 2018
  url: https://www.buzzfeednews.com/article/meghara/facebook-myanmar-rohingya-genocide

- key: Rankin2021
  kind: inproceedings
  author:
  - Yolanda A. Rankin
  - Jakita O. Thomas
  - Sheena Erete
  title: "Real Talk: Saturated Sites of Violence in CS Education"
  booktitle: Proc. ICSE'21
  year: 2021
  doi: 10.1145/3408877.3432432
  abstract: >
    Despite numerous CS education pedagogical interventions, the pipeline of
    Black women in Computing has not increased, which illustrates the need to
    address structural issues (such as racism, sexism, power, and privilege)
    that impact Black women's intersectional identities. Without honest
    conversations about power relations within the field of Computing, one
    cannot expect to engender social change that equates to equity for all CS
    students. Leveraging intersectionality as a critical framework, we interview
    18 Black women about their experiences navigating the computing education
    ecosystem. Intersectional analysis of Black women's experiences reveals that
    CS education consists of saturated sites of violence in which interconnected
    systems of power converge to enact oppression. Findings reveal three
    saturated sites of violence within CS education: 1. traditional K-12
    classrooms; 2. predominantly White institutions; and 3. internships as
    supplementary learning experiences.

- key: Rastogi2018
  kind: inproceedings
  author:
  - Ayushi Rastogi
  - Nachiappan Nagappan
  - Georgios Gousios
  - "André van der Hoek"
  title: Relationship between geographical location and evaluation of developer contributions in github
  booktitle: Proc. ESEM'18
  year: 2018
  doi: 10.1145/3239235.3240504
  publisher: ACM Press
  url: https://doi.org/10.1145/3239235.3240504
  abstract: >
    Background Open source software projects show gender bias suggesting that
    other demographic characteristics of developers, like geographical location,
    can negatively influence evaluation of contributions too. Aim This study
    contributes to this emerging body of knowledge in software development by
    presenting a quantitative analysis of the relationship between the
    geographical location of developers and evaluation of their contributions on
    GitHub. Method We present an analysis of 70,000+ pull requests selected from
    17 most actively participating countries to model the relationship between
    the geographical location of developers and pull request acceptance
    decision. Results and Conclusion We observed structural differences in pull
    request acceptance rates across 17 countries. Countries with no apparent
    similarities such as Switzerland and Japan had one of the highest pull
    request acceptance rates while countries like China and Germany had one of
    the lowest pull request acceptance rates. Notably, higher acceptance rates
    were observed for all but one country when pull requests were evaluated by
    developers from the same country.

- key: Rawson2014
  kind: article
  author:
  - Katherine A. Rawson
  - Ruthann C. Thomas
  - Larry L. Jacoby
  title: "The Power of Examples: Illustrative Examples Enhance Conceptual Learning of Declarative Concepts"
  journal: Educational Psychology Review
  volume: 27
  number: 3
  year: 2014
  doi: 10.1007/s10648-014-9273-3
  abstract: >
    Declarative concepts (i.e., key terms with short definitions of the abstract
    concepts denoted by those terms) are a common kind of information that
    students are expected to learn in many domains. A common pedagogical
    approach for supporting learning of declarative concepts involves presenting
    students with concrete examples that illustrate how the abstract concepts
    can be instantiated in real-world situations. However, minimal prior
    research has examined whether illustrative examples actually enhance
    declarative concept learning, and the available outcomes provide weak
    evidence at best. In the three experiments reported here, students studied
    definitions of declarative concepts followed either by illustrative examples
    of those concepts or by additional study of the definitions. On a subsequent
    classification test in which learners were presented with examples and were
    asked to identify which concept the example illustrated, performance was
    greater for students who had studied illustrative examples during learning
    than for students who only studied definitions (ds from 0.74 to
    1.67). However, the effects of illustrative examples on declarative concept
    learning depended in part on the conditions under which those examples were
    presented. Although performance was similar when examples were presented
    after versus before concept definitions (Experiments 1a–1b), classification
    accuracy depended on the extent to which examples of different concepts were
    interleaved and whether definitions were presented along with the examples
    (Experiment 2).

- key: Ray2017
  kind: article
  author:
  - Baishakhi Ray
  - Daryl Posnett
  - Premkumar Devanbu
  - Vladimir Filkov
  title: A large-scale study of programming languages and code quality in GitHub
  journal: Communications of the ACM
  volume: 60
  number: 10
  pages: 91--100
  month: '9'
  year: 2017
  doi: 10.1145/3126905
  publisher: Association for Computing Machinery (ACM)
  url: https://doi.org/10.1145/3126905
  abstract: >
    What is the effect of programming languages on software quality? This
    question has been a topic of much debate for a very long time. In this
    study, we gather a very large data set from GitHub (728 projects, 63 million
    SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to
    shed some empirical light on this question. This reasonably large sample
    size allows us to use a mixed-methods approach, combining multiple
    regression modeling with visualization and text analytics, to study the
    effect of language features such as static versus dynamic typing and
    allowing versus disallowing type confusion on software quality. By
    triangulating findings from different methods, and controlling for
    confounding effects such as team size, project size, and project history, we
    report that language design does have a significant, but modest effect on
    software quality. Most notably, it does appear that disallowing type
    confusion is modestly better than allowing it, and among functional
    languages, static typing is also somewhat better than dynamic typing. We
    also find that functional languages are somewhat better than procedural
    languages. It is worth noting that these modest effects arising from
    language design are overwhelmingly dominated by the process factors such as
    project size, team size, and commit size. However, we caution the reader
    that even these modest effects might quite possibly be due to other,
    intangible process factors, for example, the preference of certain
    personality types for functional, static languages that disallow type
    confusion.

- key: Raymond2001
  kind: book
  author:
  - Eric S. Raymond
  title: The Cathedral and the Bazaar
  year: 2001
  isbn: 978-0596001087
  publisher: O'Reilly Media

- key: Reekie2006
  kind: book
  author:
  - John Reekie
  - Rohan McAdam
  title: A Software Architecture Primer
  year: 2006
  isbn: 978-0646458410
  publisher: Angophora Press

- key: Restakis2010
  kind: book
  author:
  - John Restakis
  title: "Humanizing the Economy: Co-operatives in the Age of Capital"
  year: 2010
  isbn: 978-0865716513
  publisher: New Society Publishers

- key: Rigby2016
  kind: inproceedings
  author:
  - Peter C. Rigby
  - Yue Cai Zhu
  - Samuel M. Donadelli
  - Audris Mockus
  title: Quantifying and mitigating turnover-induced knowledge loss
  booktitle: Proc. ICSE'16
  year: 2016
  doi: 10.1145/2884781.2884851
  publisher: ACM Press
  url: https://doi.org/10.1145/2884781.2884851
  abstract: >
    The utility of source code, as of other knowledge artifacts, is predicated
    on the existence of individuals skilled enough to derive value by using or
    improving it. Developers leaving a software project deprive the project of
    the knowledge of the decisions they have made. Previous research shows that
    the survivors and newcomers maintaining abandoned code have reduced
    productivity and are more likely to make mistakes. We focus on quantifying
    the extent of abandoned source files and adapt methods from financial risk
    analysis to assess the susceptibility of the project to developer
    turnover. In particular, we measure the historical loss distribution and
    find (1) that projects are susceptible to losses that are more than three
    times larger than the expected loss. Using historical simulations we find
    (2) that projects are susceptible to large losses that are over five times
    larger than the expected loss. We use Monte Carlo simulations of disaster
    loss scenarios and find (3) that simplistic estimates of the `truck factor'
    exaggerate the potential for loss. To mitigate loss from developer turnover,
    we modify Cataldo et al's coordination requirements matrices. We find (4)
    that we can recommend the correct successor 34% to 48% of the time. We also
    find that having successors reduces the expected loss by as much as 15%. Our
    approach helps large projects assess the risk of turnover thereby making
    risk more transparent and manageable.

- key: Robinson2005
  kind: link
  author:
  - Evan Robinson
  title: "Why Crunch Mode Doesn''t Work: 6 Lessons"
  year: 2005
  url: https://igda.org/resources-archive/why-crunch-mode-doesnt-work-six-lessons-2005/

- key: Robinson2021
  kind: link
  author:
  - Nathan J. Robinson
  title: Surely We Can Do Better Than Elon Musk
  url: https://www.currentaffairs.org/2021/04/surely-we-can-do-better-than-elon-musk/

- key: Rodriguez2018
  kind: inproceedings
  author:
  - Ariel Rodriguez
  - Fumiya Tanaka
  - Yasutaka Kamei
  title: Empirical study on the relationship between developer's working habits and efficiency
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196458
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196458
  abstract: >
    Software developers can have a reputation for frequently working long and
    irregular hours which are widely considered to inhibit mental capacity and
    negatively affect work quality. This paper analyzes the working habits of
    software developers and the effects these habits have on efciency based on a
    large amount of data extracted from the actions of developers in the IDE
    (Integrated Development Environment), Visual Studio. We use events that
    recorded the times at which all developer actions were performed along with
    the numbers of successful and failed build and test events. Due to the high
    level of detail of the events provided by KaVE project's tool, we were able
    to analyze the data in a way that previous studies have not been able to. We
    structure our study along three dimensions: (1) days of the week, (2) time
    of the day, and (3) continuous work. Our findings will help software
    developers and team leaders to appropriately allocate working times and to
    maximise work quality.

- key: Rohrer2015
  kind: article
  author:
  - Doug Rohrer
  - Robert F. Dedrick
  - Sandra Stershic
  title: Interleaved Practice Improves Mathematics Learning
  journal: Journal of Educational Psychology
  volume: 107
  number: 3
  year: 2015
  doi: 10.1037/edu0000001
  abstract: >
    A typical mathematics assignment consists primarily of practice problems
    requiring the strategy introduced in the immediately preceding lesson (e.g.,
    a dozen problems that are solved by using the Pythagorean theorem). This
    means that students know which strategy is needed to solve each problem
    before they read the problem. In an alternative approach known as
    interleaved practice, problems from the course are rearranged so that a
    portion of each assignment includes different kinds of problems in an
    interleaved order. Interleaved practice requires students to choose a
    strategy on the basis of the problem itself, as they must do when they
    encounter a problem during a comprehensive examination or subsequent
    course. In the experiment reported here, 126 seventh-grade students received
    the same practice problems over a 3-month period, but the problems were
    arranged so that skills were learned by interleaved practice or by the usual
    blocked approach. The practice phase concluded with a review session,
    followed 1 or 30 days later by an unannounced test. Compared with blocked
    practice, interleaved practice produced higher scores on both the immediate
    and delayed tests (Cohen’s ds 0.42 and 0.79, respectively).

- key: Romano2018
  kind: inproceedings
  author:
  - Simone Romano
  - Giuseppe Scanniello
  - Davide Fucci
  - Natalia Juristo
  - Burak Turhan
  title: The effect of noise on software engineers' performance
  booktitle: Proc. ESEM'18
  year: 2018
  doi: 10.1145/3239235.3240496
  publisher: ACM Press
  url: https://doi.org/10.1145/3239235.3240496
  abstract: >
    Background: Noise, defined as an unwanted sound, is one of the commonest
    factors that could affect people's performance in their daily work
    activities. The software engineering research community has marginally
    investigated the effects of noise on software engineers' performance. Aims:
    We studied if noise affects software engineers' performance in: (i)
    comprehending functional requirements and (ii) fixing faults in source
    code. Method: We conducted two experiments with final-year undergraduate
    students in Computer Science. In the first experiment, we asked 55 students
    to comprehend functional requirements exposing them or not to noise, while
    in the second experiment 42 students were asked to fix faults in Java
    code. Results: The participants in the second experiment, when exposed to
    noise, had significantly worse performance in fixing faults in source
    code. On the other hand, we did not observe any statistically significant
    difference in the first experiment. Conclusions: Fixing faults in source
    code seems to be more vulnerable to noise than comprehending functional
    requirements.

- key: Rosen2015
  kind: inproceedings
  author:
  - Christoffer Rosen
  - Ben Grawi
  - Emad Shihab
  title: "Commit guru: analytics and risk prediction of software commits"
  booktitle: Proc. ESEC/FSE'15
  year: 2015
  doi: 10.1145/2786805.2803183
  publisher: ACM Press
  url: https://doi.org/10.1145/2786805.2803183
  abstract: >
    Software quality is one of the most important research sub-areas of software
    engineering. Hence, a plethora of research has focused on the prediction of
    software quality. Much of the software analytics and prediction work has
    proposed metrics, models and novel approaches that can predict quality with
    high levels of accuracy. However, adoption of such techniques remain low;
    one of the reasons for this low adoption of the current analytics and
    prediction technique is the lack of actionable and publicly available
    tools. We present Commit Guru, a language agnostic analytics and prediction
    tool that identifies and predicts risky software commits. Commit Guru is
    publicly available and is able to mine any GIT SCM repository. Analytics are
    generated at both, the project and commit levels. In addition, Commit Guru
    automatically identifies risky (i.e., bug-inducing) commits and builds a
    prediction model that assess the likelihood of a recent commit introducing a
    bug in the future. Finally, to facilitate future research in the area, users
    of Commit Guru can download the data for any project that is processed by
    Commit Guru with a single click. Several large open source projects have
    been successfully processed using Commit Guru. Commit Guru is available
    online at commit.guru. Our source code is also released freely under the MIT
    license.

- key: Rossbach2010
  kind: article
  author:
  - Christopher J. Rossbach
  - Owen S. Hofmann
  - Emmett Witchel
  title: Is transactional programming actually easier?
  journal: SIGPLAN
  volume: 45
  number: 5
  pages: '47'
  month: '5'
  year: 2010
  doi: 10.1145/1837853.1693462
  publisher: Association for Computing Machinery (ACM)
  url: https://doi.org/10.1145/1837853.1693462

- key: Royce1970
  kind: inproceedings
  author:
  - Winston Royce
  title: Managing the Development of Large Software Systems
  booktitle: Proc. IEEE WESCON
  year: 1970

- key: Sadowski2019
  kind: book
  title: Rethinking Productivity in Software Engineering
  editor:
  - Caitlin Sadowski
  - Thomas Zimmermann
  year: 2019
  isbn: 978-1484242209
  publisher: Apress

- key: Sadowski2019a
  kind: book
  title: Rethinking Productivity in Software Engineering
  editor:
  - Caitlin Sadowski
  - Thomas Zimmermann
  year: 2019
  isbn: 978-1484242209
  publisher: Apress

- key: Sadowski2019b
  kind: incollection
  author:
  - Caitlin Sadowski
  - Margaret-Anne Storey
  - Robert Feldt
  title: A Software Development Productivity Framework
  booktitle: Rethinking Productivity in Software Engineering
  editor:
  - Caitlin Sadowski
  - Thomas Zimmermann
  year: 2019
  isbn: 978-1484242209
  publisher: Apress

- key: Sajaniemi2006
  kind: article
  author:
  - Jorma Sajaniemi
  - Mordechai Ben-Ari
  - Pauli Byckling
  - Petri Gerdt
  - Yevgeniya Kulikova
  title: Roles of Variables in Three Programming Paradigms
  journal: Computer Science Education
  volume: 16
  number: 4
  year: 2006
  doi: 10.1080/08993400600874584
  abstract: >
    Roles can be assigned to occurrences of variables in programs according to a
    small number of stereotypical patterns of use. Studies on explicitly
    teaching roles to novices learning programming have shown that roles are an
    excellent pedagogical tool for clarifying the structure and meaning of
    programs and that their use improves students' programming skills. This
    paper describes how roles can be applied in various programming paradigms
    and presents the results of three studies designed to test the
    understandability and acceptability of the role concept and of the
    individual roles in procedural, object-oriented, and functional
    programming. Based on the results, two new roles and small modifications to
    the definitions of the original roles are suggested.

- key: Sarker2019
  kind: inproceedings
  author:
  - Farhana Sarker
  - Bogdan Vasilescu
  - Kelly Blincoe
  - Vladimir Filkov
  title: Socio-Technical Work-Rate Increase Associates With Changes in Work Patterns in Online Projects
  booktitle: Proc. ICSE'19
  month: '5'
  year: 2019
  doi: 10.1109/icse.2019.00099
  publisher: IEEE
  url: https://doi.org/10.1109/icse.2019.00099
  abstract: >
    Software developers work on a variety of tasks ranging from the technical,
    e.g., writing code, to the social, e.g., participating in issue resolution
    discussions. The amount of work developers perform per week (their
    work-rate) also varies and depends on project needs and developer
    schedules. Prior work has shown that while moderate levels of increased
    technical work and multitasking lead to higher productivity, beyond a
    certain threshold, they can lead to lowered performance. Here, we study how
    increases in the short-term work-rate along both the technical and social
    dimensions are associated with changes in developers' work patterns, in
    particular communication sentiment, technical productivity, and social
    productivity. We surveyed active and prolific developers on GitHub to
    understand the causes and impacts of increased work-rates. Guided by the
    responses, we developed regression models to study how communication and
    committing patterns change with increased work-rates and fit those models to
    large-scale data gathered from traces left by thousands of GitHub
    developers. From our survey and models, we find that most developers do
    experience work-rate-increase-related changes in behavior. Most notably, our
    models show that there is a sizable effect when developers comment much more
    than their average: the negative sentiment in their comments increases,
    suggesting an increased level of stress. Our models also show that
    committing patterns do not change with increased commenting, and vice versa,
    suggesting that technical and social activities tend not to be multitasked.

- key: Scalzi2012
  kind: link
  author:
  - John Scalzi
  title: "Straight White Male: The Lowest Difficulty Setting There Is"
  year: 2012
  url: https://whatever.scalzi.com/2012/05/15/straight-white-male-the-lowest-difficulty-setting-there-is/

- key: Scanlan1989
  kind: article
  author:
  - David A. Scanlan
  title: "Structured Flowcharts Outperform Pseudocode: An Experimental Comparison"
  journal: IEEE Software
  volume: 6
  number: 5
  year: 1989
  doi: 10.1109/52.35587
  abstract: >
    The author discovered, while teaching a course on data structures, that his
    students overwhelmingly preferred structured flowcharts over pseudocode for
    comprehending the algorithms presented. He describes an experiment that he
    designed to find out if real differences in comprehension exist between
    structured flowcharts and pseudocode when used to describe conditional
    logic. He hypothesized that structured flowcharts (1) take less time to
    comprehend, (2) produce fewer errors in understanding, (3) give students
    more confidence in their understanding of an algorithm, (4) reduce the time
    spent answering questions about an algorithm, and (5) reduce the number of
    times students need to look at an algorithm. These hypotheses were tested on
    three algorithms of varying complexity. The results strongly indicate that
    structured flowcharts do indeed aid algorithm comprehension. A large
    difference was found even for the simplest algorithm.<<ETX>>

- key: Scanniello2017
  kind: article
  author:
  - Giuseppe Scanniello
  - Michele Risi
  - Porfirio Tramontana
  - Simone Romano
  title: Fixing Faults in C and Java Source Code
  journal: ACM Trans. Software Engineering and Methodology
  volume: 26
  number: 2
  pages: 1--43
  month: '7'
  year: 2017
  doi: 10.1145/3104029
  publisher: Association for Computing Machinery (ACM)
  url: https://doi.org/10.1145/3104029
  abstract: >
    We carried out a family of controlled experiments to investigate whether the
    use of abbreviated identifier names, with respect to full-word identifier
    names, affects fault fixing in C and Java source code. This family consists
    of an original (or baseline) controlled experiment and three
    replications. We involved 100 participants with different backgrounds and
    experiences in total. Overall results suggested that there is no difference
    in terms of effort, effectiveness, and efficiency to fix faults, when source
    code contains either only abbreviated or only full-word identifier names. We
    also conducted a qualitative study to understand the values, beliefs, and
    assumptions that inform and shape fault fixing when identifier names are
    either abbreviated or full-word. We involved in this qualitative study six
    professional developers with 1--3 years of work experience. A number of
    insights emerged from this qualitative study and can be considered a useful
    complement to the quantitative results from our family of experiments. One
    of the most interesting insights is that developers, when working on source
    code with abbreviated identifier names, adopt a more methodical approach to
    identify and fix faults by extending their focus point and only in a few
    cases do they expand abbreviated identifiers.

- key: Schankin2018
  kind: inproceedings
  author:
  - Andrea Schankin
  - Annika Berger
  - Daniel V. Holt
  - Johannes C. Hofmeister
  - Till Riedel
  - Michael Beigl
  title: Descriptive Compound Identifier Names Improve Source Code Comprehension
  booktitle: ICPC'18
  year: 2018
  doi: 10.1145/3196321.3196332
  abstract: >
    Reading and understanding source code is a major task in software
    development. Code comprehension depends on the quality of code, which is
    impacted by code structure and identifier naming. In this paper we
    empirically investigated whether longer but more descriptive identifier
    names improve code comprehension compared to short names, as they represent
    useful information in more detail. In a web-based study 88 Java developers
    were asked to locate a semantic defect in source code snippets. With
    descriptive identifier names, developers spent more time in the lines of
    code before the actual defect occurred and changed their reading direction
    less often, finding the semantic defect about 14% faster than with shorter
    but less descriptive identifier names. These effects disappeared when
    developers searched for a syntax error, i.e., when no in-depth understanding
    of the code was required. Interestingly, the style of identifier names had a
    clear impact on program comprehension for more experienced developers but
    not for less experienced developers.

- key: Schneier2019
  kind: book
  author:
  - Bruce Schneier
  title: "Click Here to Kill Everybody: Security and Survival in a Hyper-connected World"
  year: 2019
  isbn: 978-0393357448
  publisher: WW Norton

- key: Schneier2021
  kind: link
  author:
  - Bruce Schneier
  title: National Security Risks of Late-Stage Capitalism
  year: 2021
  url: https://www.schneier.com/blog/archives/2021/03/national-security-risks-of-late-stage-capitalism.html

- key: Schon1984
  kind: book
  author:
  - Donald A. Schon
  title: "The Reflective Practitioner: How Professionals Think in Action"
  year: 1984
  isbn: 978-0465068784
  publisher: Basic Books

- key: Scott1999
  kind: book
  author:
  - James C. Scott
  title: "Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed"
  year: 1999
  isbn: 978-0300078152
  publisher: Yale University Press

- key: Sedano2017
  kind: inproceedings
  author:
  - Todd Sedano
  - Paul Ralph
  - "Cécile Péraire"
  title: Software Development Waste
  booktitle: Proc. ICSE'17
  year: 2017
  doi: 10.1109/ICSE.2017.20
  abstract: >
    Context: Since software development is a complex socio-technical activity
    that involves coordinating different disciplines and skill sets, it provides
    ample opportunities for waste to emerge. Waste is any activity that produces
    no value for the customer or user. Objective: The purpose of this paper is
    to identify and describe different types of waste in software
    development. Method: Following Constructivist Grounded Theory, we conducted
    a two-year five-month participant-observation study of eight software
    development projects at Pivotal, a software development consultancy. We also
    interviewed 33 software engineers, interaction designers, and product
    managers, and analyzed one year of retrospection topics. We iterated between
    analysis and theoretical sampling until achieving theoretical
    saturation. Results: This paper introduces the first empirical waste
    taxonomy. It identifies nine wastes and explores their causes, underlying
    tensions, and overall relationship to the waste taxonomy found in Lean
    Software Development. Limitations: Grounded Theory does not support
    statistical generalization. While the proposed taxonomy appears widely
    applicable, organizations with different software development cultures may
    experience different waste types. Conclusion: Software development projects
    manifest nine types of waste: building the wrong feature or product,
    mismanaging the backlog, rework, unnecessarily complex solutions, extraneous
    cognitive load, psychological distress, waiting/multitasking, knowledge
    loss, and ineffective communication.

- key: Seitz2021
  kind: book
  author:
  - Justin Seitz
  - Tim Arnold
  title: "Black Hat Python: Python Programming for Hackers and Pentesters"
  edition: 2nd
  year: 2021
  isbn: 978-1718501126
  publisher: No Starch Press

- key: Sharma2021
  kind: link
  author:
  - Ax Sharma
  title: PHP's Git server hacked to add backdoors to PHP source code
  year: 2021
  url: https://www.bleepingcomputer.com/news/security/phps-git-server-hacked-to-add-backdoors-to-php-source-code/

- key: Sharp2016
  kind: article
  author:
  - Helen Sharp
  - Yvonne Dittrich
  - Cleidson R. B. de Souza
  title: The Role of Ethnographic Studies in Empirical Software Engineering
  journal: TSE
  volume: 42
  number: 8
  year: 2016
  doi: 10.1109/tse.2016.2519887
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  abstract: >
    Ethnography is a qualitative research method used to study people and
    cultures. It is largely adopted in disciplines outside software engineering,
    including different areas of computer science. Ethnography can provide an
    in-depth understanding of the socio-technological realities surrounding
    everyday software development practice, i.e., it can help to uncover not
    only what practitioners do, but also why they do it. Despite its potential,
    ethnography has not been widely adopted by empirical software engineering
    researchers, and receives little attention in the related literature. The
    main goal of this paper is to explain how empirical software engineering
    researchers would benefit from adopting ethnography. This is achieved by
    explicating four roles that ethnography can play in furthering the goals of
    empirical software engineering: to strengthen investigations into the social
    and human aspects of software engineering; to inform the design of software
    engineering tools; to improve method and process development; and to inform
    research programmes. This article introduces ethnography, explains its
    origin, context, strengths and weaknesses, and presents a set of dimensions
    that position ethnography as a useful and usable approach to empirical
    software engineering research. Throughout the paper, relevant examples of
    ethnographic studies of software practice are used to illustrate the points
    being made.

- key: Sholler2019
  kind: article
  author:
  - Dan Sholler
  - Igor Steinmacher
  - Denae Ford
  - Mara Averick
  - Mike Hoye
  - Greg Wilson
  title: Ten Simple Rules for Helping Newcomers Become Contributors to Open Projects
  journal: PLOS Computational Biology
  volume: 15
  number: 9
  year: 2019
  doi: 10.1371/journal.pcbi.1007296
  abstract: >
    To survive and thrive, a community must attract new members, retain them,
    and help them be productive [1]. As openness becomes the norm in research,
    software development, and education, knowing how to do this has become an
    essential skill for principal investigators and community managers alike. A
    growing body of knowledge in sociology, anthropology, education, and
    software engineering can guide decisions about how to facilitate this.  What
    exactly do we mean by "community"? In the case of open source and open
    science, the most usual meaning is a "community of practice." As defined by
    Lave and Wenger [2, 3], groups as diverse as knitting circles, oncology
    researchers, and web designers share three key characteristics:
    1. Participants have a common product or purpose that they work on or
    toward.  2. They are mutually engaged, i.e., they assist and mentor each
    another.  3. They develop shared resources and domain knowledge.  Brown [4]
    specializes this to define a "community of effort" as …a community formed in
    pursuit of a common goal. The goal can be definite or indefinite in time,
    and may not be clearly defined, but it is something that (generally
    speaking) the community is aligned on. People working to preserve coral
    reefs in the face of global climate change are an example of such a
    community. No central organization coordinates their work, but the
    scientists who study coral reefs, the environmentalists who work to protect
    them, and the citizens who support them financially and politically are
    aware of each other’s efforts, collaborate in ad hoc ways, and are conscious
    of contributing toward a shared purpose.  Open-source software projects are
    also communities of effort. E.g., the Mozilla Firefox [5] community includes
    a mix of paid professionals, highly involved volunteers, and occasional
    contributors who not only create software, documentation, and tutorials but
    also organize events, answer questions in online forums, mentor newcomers,
    and advocate for open standards.  Every community of effort has unique
    features, but they have enough in common to profit from one another’s
    experience. The 10 rules laid out below are based on studies of such
    communities and on the authors’ experience as members, leaders, and
    observers. Our focus is on small and medium-sized projects, i.e., ones that
    have a handful of to a few hundred participants and are a few months to a
    few years old but may not (yet) have any formal legal standing, such as
    incorporation as a nonprofit.

- key: Singh2009
  kind: book
  author:
  - Simon Singh
  - Edzard Ernst
  title: "Trick or Treatment: The Undeniable Facts about Alternative Medicine"
  year: 2009
  isbn: 978-0393337785
  publisher: W. W. Norton

- key: Sjoberg2018
  kind: inproceedings
  author:
  - Dag I. K. Sjøberg
  title: An Empirical Study of WIP in Kanban Teams
  booktitle: Proc. ESEM'18
  year: 2018
  doi: 10.1145/3239235.3239238
  publisher: ACM Press
  url: https://doi.org/10.1145/3239235.3239238
  abstract: >
    Background: Limiting the amount of Work-In-Progress (WIP) is considered a
    fundamental principle in Kanban software development. However, no published
    studies from real cases exist that indicate what an optimal WIP limit should
    be. Aims: The primary aim is to study the effect of WIP on the performance
    of a Kanban team. The secondary aim is to illustrate methodological
    challenges when attempting to identify an optimal or appropriate WIP
    limit. Method: A quantitative case study was conducted in a software company
    that provided information about more than 8,000 work items developed over
    four years by five teams. Relationships between WIP, lead time and
    productivity were analyzed. Results: WIP correlates with lead time; that is,
    lower WIP indicates shorter lead times, which is consistent with claims in
    the literature. However, WIP also correlates with productivity, which is
    inconsistent with the claim in the literature that a low WIP (still above a
    certain threshold) will improve productivity. The collected data set did not
    include sufficient information to measure aspects of quality. There are
    several threats to the way productivity was measured. Conclusions:
    Indicating an optimal WIP limit is difficult in the studied company because
    a changing WIP gives contrasting results on different team performance
    variables. Because the effect of WIP has not been quantitatively examined
    before, this study clearly needs to be replicated in other contexts. In
    addition, studies that include other team performance variables, such as
    various aspects of quality, are requested. The methodological challenges
    illustrated in this paper need to be addressed.

- key: Smalls2021
  kind: article
  author:
  - Danielle Smalls
  - Greg Wilson
  title: Ten Quick Tips for Staying Safe Online
  journal: PLOS Computational Biology
  volume: 17
  number: 3
  year: 2021
  doi: 10.1371/journal.pcbi.1008563
  abstract: >
    Researchers studying everything from sexual health to the Coronavirus
    Disease 2019 (COVID-19) to gun violence are increasingly likely to be
    targeted because of their work. While research institutions have rules and
    guidelines for safeguarding sensitive information, these usually do not
    address the problem of keeping individuals safe from either targeted attacks
    like Climategate [1] or the kinds of “drive-by” threats that everyone now
    faces regardless of their occupation. Hollywood depictions of everyday
    threats are as far from reality as their portrayals of scientists, but more
    realistic guidance for personal digital security is now freely available
    [2–4]. The 10 quick tips in this paper are a starting point: While they
    apply to everyone, they were developed with researchers in mind. While
    researchers expect their work to be scrutinized by the academic community,
    they should not expect to endure harassment due to the visibility of their
    published works. These rules do not guarantee complete safety, any more than
    seatbelts guarantee safe driving, but following them greatly reduces the
    likelihood of harm.

- key: Smith2011
  kind: book
  author:
  - Peter Smith
  title: "Software Build Systems: Principles and Experience"
  year: 2011
  isbn: 978-0134185965
  publisher: Addison-Wesley Professional
  note: A practical overview of the build process and tools to support it.

- key: Snowden2019
  kind: book
  author:
  - Edward Snowden
  title: Permanent Record
  year: 2019
  isbn: 978-1250237231
  publisher: Metropolitan Books

- key: Solnit2010
  kind: book
  author:
  - Rebecca Solnit
  title: "A Paradise Built in Hell: The Extraordinary Communities That Arise in Disaster"
  year: 2010
  isbn: 978-0143118077
  publisher: Penguin Books

- key: Soto2018
  kind: inproceedings
  author:
  - Mauricio Soto
  - Claire Le Goues
  title: Common statement kind changes to inform automatic program repair
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196472
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196472
  abstract: >
    The search space for automatic program repair approaches is vast and the
    search for mechanisms to help restrict this search are increasing. We make a
    granular analysis based on statement kinds to find which statements are more
    likely to be modified than others when fixing an error. We construct a
    corpus for analysis by delimiting debugging regions in the provided dataset
    and recursively analyze the differences between the Simplified Syntax Trees
    associated with EditEvent's. We build a distribution of statement kinds with
    their corresponding likelihood of being modified and we validate the usage
    of this distribution to guide the statement selection. We then build
    association rules with different confidence thresholds to describe statement
    kinds commonly modified together for multi-edit patch creation. Finally we
    evaluate association rule coverage over a held out test set and find that
    when using a 95% confidence threshold we can create less and more accurate
    rules that fully cover 93.8% of the testing instances.

- key: Sotovalero2018
  kind: inproceedings
  author:
  - "César Soto-Valero"
  - Johann Bourcier
  - Benoit Baudry
  title: Detection and analysis of behavioral T-patterns in debugging activities
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196452
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196452
  abstract: >
    A growing body of research in empirical software engineering applies
    recurrent patterns analysis in order to make sense of the developers'
    behavior during their interactions with IDEs. However, the exploration of
    hidden real-time structures of programming behavior remains a challenging
    task. In this paper, we investigate the presence of temporal behavioral
    patterns (T-patterns) in debugging activities using the THEME software. Our
    preliminary exploratory results show that debugging activities are strongly
    correlated with code editing, file handling, window interactions and other
    general types of programming activities. The validation of our T-patterns
    detection approach demonstrates that debugging activities are performed on
    the basis of repetitive and well-organized behavioral events. Furthermore,
    we identify a large set of T-patterns that associate debugging activities
    with build success, which corroborates the positive impact of debugging
    practices on software development.

- key: Spadini2019
  kind: inproceedings
  author:
  - Davide Spadini
  - Fabio Palomba
  - Tobias Baum
  - Stefan Hanenberg
  - Magiel Bruntink
  - Alberto Bacchelli
  title: "Test-Driven Code Review: An Empirical Study"
  booktitle: Proc. ICSE'19
  month: '5'
  year: 2019
  doi: 10.1109/icse.2019.00110
  publisher: IEEE
  url: https://doi.org/10.1109/icse.2019.00110
  abstract: >
    Test-Driven Code Review (TDR) is a code review practice in which a reviewer
    inspects a patch by examining the changed test code before the changed
    production code. Although this practice has been mentioned positively by
    practitioners in informal literature and interviews, there is no systematic
    knowledge of its effects, prevalence, problems, and advantages. In this
    paper, we aim at empirically understanding whether this practice has an
    effect on code review effectiveness and how developers' perceive TDR. We
    conduct (i) a controlled experiment with 93 developers that perform more
    than 150 reviews, and (ii) 9 semi-structured interviews and a survey with
    103 respondents to gather information on how TDR is perceived. Key results
    from the experiment show that developers adopting TDR find the same
    proportion of defects in production code, but more in test code, at the
    expenses of fewer maintainability issues in production code. Furthermore, we
    found that most developers prefer to review production code as they deem it
    more critical and tests should follow from it. Moreover, general poor test
    code quality and no tool support hinder the adoption of TDR. Public
    preprint: https://doi.org/10.5281/zenodo.2551217, data and materials:
    https://doi.org/10.5281/zenodo.2553139

- key: Spinellis2003
  kind: book
  author:
  - Diomidis Spinellis
  title: "Code Reading: The Open Source Perspective"
  year: 2003
  isbn: 978-0201799408
  publisher: Addison-Wesley Professional

- key: Spinellis2007
  kind: article
  author:
  - Diomidis Spinellis
  title: The Tools We Use
  journal: IEEE Software
  volume: 24
  number: 4
  year: 2007

- key: Spinellis2016
  kind: inproceedings
  author:
  - Diomidis Spinellis
  - Panos Louridas
  - Maria Kechagia
  title: The evolution of C programming practices
  booktitle: Proc. ICSE'16
  year: 2016
  doi: 10.1145/2884781.2884799
  publisher: ACM Press
  url: https://doi.org/10.1145/2884781.2884799
  abstract: >
    Tracking long-term progress in engineering and applied science allows us to
    take stock of things we have achieved, appreciate the factors that led to
    them, and set realistic goals for where we want to go. We formulate seven
    hypotheses associated with the long term evolution of C programming in the
    Unix operating system, and examine them by extracting, aggregating, and
    synthesising metrics from 66 snapshots obtained from a synthetic software
    configuration management repository covering a period of four decades. We
    found that over the years developers of the Unix operating system appear to
    have evolved their coding style in tandem with advancements in hardware
    technology, promoted modularity to tame rising complexity, adopted valuable
    new language features, allowed compilers to allocate registers on their
    behalf, and reached broad agreement regarding code formatting. The progress
    we have observed appears to be slowing or even reversing prompting the need
    for new sources of innovation to be discovered and followed.

- key: Stefik2013
  kind: article
  author:
  - Andreas Stefik
  - Susanna Siebert
  title: An Empirical Investigation into Programming Language Syntax
  journal: ACM Trans. Computing Education
  volume: 13
  number: 4
  pages: 1--40
  month: '11'
  year: 2013
  doi: 10.1145/2534973
  publisher: Association for Computing Machinery (ACM)
  url: https://doi.org/10.1145/2534973
  abstract: >
    Recent studies in the literature have shown that syntax remains a
    significant barrier to novice computer science students in the field. While
    this syntax barrier is known to exist, whether and how it varies across
    programming languages has not been carefully investigated. For this article,
    we conducted four empirical studies on programming language syntax as part
    of a larger analysis into the, so called, programming language wars. We
    first present two surveys conducted with students on the intuitiveness of
    syntax, which we used to garner formative clues on what words and symbols
    might be easy for novices to understand. We followed up with two studies on
    the accuracy rates of novices using a total of six programming languages:
    Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by
    randomly choosing some keywords from the ASCII table (a metaphorical
    placebo). To our surprise, we found that languages using a more traditional
    C-style syntax (both Perl and Java) did not afford accuracy rates
    significantly higher than a language with randomly generated keywords, but
    that languages which deviate (Quorum, Python, and Ruby) did. These results,
    including the specifics of syntax that are particularly problematic for
    novices, may help teachers of introductory programming courses in choosing
    appropriate first languages and in helping students to overcome the
    challenges they face with syntax.

- key: Stefik2017
  kind: link
  author:
  - Andreas Stefik
  - Patrick Daleiden
  - Diana Franklin
  - Stefan Hanenberg
  - Antti-Juhani Kaijanaho
  - Walter Tichy
  - Brett A. Becker
  title: Programming Languages and Learning
  year: 2017
  url: https://quorumlanguage.com/evidence.html

- key: Stegeman2014
  kind: inproceedings
  author:
  - Martijn Stegeman
  - Erik Barendsen
  - Sjaak Smetsers
  title: Towards an Empirically Validated Model for Assessment of Code Quality
  booktitle: Proc. Koli'14
  year: 2014
  doi: 10.1145/2674683.2674702
  abstract: >
    We present a pilot study into developing a model of feedback on code quality
    in introductory programming courses. To devise such a model, we analyzed
    professional standards of code quality embedded in three popular software
    engineering handbooks and found 401 suggestions that we categorized into
    twenty topics. We recorded three instructors who performed a think-aloud
    judgment of student-submitted programs, and we interviewed them on the
    topics from the books, leading to 178 statements about code quality. The
    statements from the instructor interviews allowed us to generate a set of
    topics relevant to their practice of giving feedback, which we used to
    create criteria for the model. We used the instructor statements as well as
    the book suggestions to distinguish three levels of achievement for each
    criterion. This resulted in a total of 9 criteria for code quality. The
    interviews with the instructors generated a view of code quality that is
    very comparable to what was found in the handbooks, while the handbooks
    provide detailed suggestions that make our results richer than previously
    published grading schemes. As such, this process leads to an overview of
    code quality criteria and levels that can be very useful for constructing a
    standards-based rubric for introductory programming courses.

- key: Stegeman2016
  kind: inproceedings
  author:
  - Martijn Stegeman
  - Erik Barendsen
  - Sjaak Smetsers
  title: Designing a Rubric for Feedback on Code Quality in Programming Courses
  booktitle: Proc. Koli'16
  year: 2016
  doi: 10.1145/2999541.2999555
  abstract: >
    We investigate how to create a rubric that can be used to give feedback on
    code quality to students in introductory programming courses. Based on an
    existing model of code quality and a set of preliminary design rules, we
    constructed a rubric and put it through several design iterations. Each
    iteration focused on different aspects of the rubric, and solutions to
    various programming assignments were used to evaluate. The rubric appears to
    be complete for the assignments it was tested on. We articulate additional
    design aspects that can be used when drafting new feedback rubrics for
    programming courses.

- key: Steinmacher2014
  kind: inproceedings
  author:
  - Igor Steinmacher
  - Igor Scaliante Wiese
  - Tayana Conte
  - "Marco Aurélio Gerosa"
  - David Redmiles
  title: The Hard Life of Open Source Software Project Newcomers
  booktitle: Proc. CHASE'14
  year: 2014
  doi: 10.1145/2593702.2593704
  abstract: >
    While onboarding an open source software (OSS) project, contributors face
    many different barriers that hinder their contribution, leading in many
    cases to dropouts. Many projects leverage the contribution of outsiders and
    the sustainability of the project relies on retaining some of these
    newcomers. In this paper, we discuss some barriers faced by newcomers to
    OSS. The barriers were identified using a qualitative analysis on data
    obtained from newcomers and members of OSS projects. We organize the results
    in a conceptual model composed of 38 barriers, grouped into seven different
    categories. These barriers may motivate new studies and the development of
    appropriate tooling to better support the onboarding of new contributors.

- key: Stylos2007
  kind: inproceedings
  author:
  - Jeffrey Stylos
  - Steven Clarke
  title: Usability Implications of Requiring Parameters in Objects' Constructors
  booktitle: Proc. ICSE'07
  year: 2007
  doi: 10.1109/icse.2007.92
  abstract: >
    The usability of APIs is increasingly important to programmer
    productivity. Based on experience with usability studies of specific APIs,
    techniques were explored for studying the usability of design choices common
    to many APIs. A comparative study was performed to assess how professional
    programmers use APIs with required parameters in objects' constructors as
    opposed to parameterless "default" constructors. It was hypothesized that
    required parameters would create more usable and self- documenting APIs by
    guiding programmers toward the correct use of objects and preventing
    errors. However, in the study, it was found that, contrary to expectations,
    programmers strongly preferred and were more effective with APIs that did
    not require constructor parameters. Participants' behavior was analyzed
    using the cognitive dimensions framework, and revealing that required
    constructor parameters interfere with common learning strategies, causing
    undesirable premature commitment.

- key: Sulistya2019
  kind: article
  author:
  - Agus Sulistya
  - Gede Artha Azriadi Prana
  - Abhishek Sharma
  - David Lo
  - Christoph Treude
  title: "SIEVE: Helping developers sift wheat from chaff via cross-platform analysis"
  journal: ESE
  month: '10'
  year: 2019
  doi: 10.1007/s10664-019-09775-w
  publisher: Springer Science and Business Media LLC
  url: https://doi.org/10.1007/s10664-019-09775-w
  abstract: >
    Software developers have benefited from various sources of knowledge such as
    forums, question-and-answer sites, and social media platforms to help them
    in various tasks. Extracting software-related knowledge from different
    platforms involves many challenges. In this paper, we propose an approach to
    improve the effectiveness of knowledge extraction tasks by performing
    cross-platform analysis. Our approach is based on transfer representation
    learning and word embedding, leveraging information extracted from a source
    platform which contains rich domain-related content. The information
    extracted is then used to solve tasks in another platform (considered as
    target platform) with less domain-related content. We first build a word
    embedding model as a representation learned from the source platform, and
    use the model to improve the performance of knowledge extraction tasks in
    the target platform. We experiment with Software Engineering Stack Exchange
    and Stack Overflow as source platforms, and two different target platforms,
    i.e., Twitter and YouTube. Our experiments show that our approach improves
    performance of existing work for the tasks of identifying software-related
    tweets and helpful YouTube comments.

- key: Taschuk2017
  kind: article
  author:
  - Morgan Taschuk
  - Greg Wilson
  title: Ten Simple Rules for Making Research Software More Robust
  journal: PLoS Comp Bio
  volume: 13
  number: 4
  month: '4'
  year: 2017
  doi: 10.1371/journal.pcbi.1005412
  publisher: Public Library of Science (PLoS)
  abstract: >
    Software produced for research, published and otherwise, suffers from a
    number of common problems that make it difficult or impossible to run
    outside the original institution or even off the primary developer’s
    computer. We present ten simple rules to make such software robust enough to
    be run by anyone, anywhere, and thereby delight your users and
    collaborators.

- key: Tedre2008
  kind: article
  author:
  - Matti Tedre
  - Erkki Sutinen
  title: "Three Traditions of Computing: What Educators Should Know"
  journal: CSE
  volume: 18
  number: 3
  year: 2008
  doi: 10.1080/08993400802332332
  abstract: >
    Educators in the computing fields are often familiar with the
    characterization of computing as a combination of theoretical, scientific,
    and engineering traditions. That distinction is often used to guide the work
    and disciplinary self-identity of computing professionals. But the
    distinction is, by no means, an easy one. The three traditions of computing
    are based on different principles, they have different aims, they employ
    different methods, and their products are very different. Educators in the
    field of computing should be aware of the fundamental differences between
    the traditions of computing so that they can offer their students a truthful
    and balanced view about computing branches. In this article the three
    traditions of computing are presented and some of their underlying
    assumptions, principles, application areas, restrictions, and weaknesses are
    portrayed. Also, some of the landmark arguments in the debates about the
    identity of computing disciplines are discussed.

- key: Thomas2019
  kind: book
  author:
  - David Thomas
  - Andrew Hunt
  title: "The Pragmatic Programmer: Your Journey to Mastery"
  year: 2019
  isbn: 978-0135957059
  publisher: Addison-Wesley Professional

- key: Thompson2019
  kind: link
  author:
  - Clive Thompson
  title: When Workers Control the Code
  year: 2019
  url: https://www.wired.com/story/when-workers-control-gig-economy/

- key: Tichy2010
  kind: incollection
  author:
  - Walter Tichy
  title: The Evidence for Design Patterns
  booktitle: Making Software
  editor:
  - Andy Oram
  - Greg Wilson
  year: 2010
  isbn: 978-0596808327
  publisher: O'Reilly

- key: Tiku2021
  kind: link
  author:
  - Nitasha Tiku
  title: Google's approach to historically Black schools helps explain why there are few Black engineers in Big Tech
  url: https://www.washingtonpost.com/technology/2021/03/04/google-hbcu-recruiting/

- key: Tourani2016
  kind: inproceedings
  author:
  - Parastou Tourani
  - Bram Adams
  title: "The Impact of Human Discussions on Just-in-Time Quality Assurance: An Empirical Study on OpenStack and Eclipse"
  booktitle: Proc. SANER'16
  month: '3'
  year: 2016
  doi: 10.1109/saner.2016.113
  publisher: IEEE
  url: https://doi.org/10.1109/saner.2016.113
  abstract: >
    In order to spot defect-introducing code changes during review before they
    are integrated into a project's version control system, a variety of defect
    prediction models have been designed. Most of these models focus exclusively
    on source code properties, like the number of added or deleted lines, or
    developer-related measures like experience. However, a code change is only
    the outcome of a much longer process, involving discussions on an issue
    report and review discussions on (different versions of) a patch. % Ignoring
    the characteristics of these activities during prediction is unfortunate,
    since Similar to how body language implicitly can reveal a person's real
    feelings, the length, intensity or positivity of these discussions can
    provide important additional clues about how risky a particular patch is or
    how confident developers and reviewers are about the patch. In this paper,
    we build logistic regression models to study the impact of the
    characteristics of issue and review discussions on the defect-proneness of a
    patch. Comparison of these models to conventional source code-based models
    shows that issue and review metrics combined improve precision and recall of
    the explanatory models up to 10%. Review time and issue discussion lag are
    amongst the most important metrics, having a positive (i.e., increasing)
    relation with defect-proneness.

- key: Tregubov2017
  kind: inproceedings
  author:
  - Alexey Tregubov
  - Barry Boehm
  - Natalia Rodchenko
  - Jo Ann Lane
  title: Impact of Task Switching and Work Interruptions on Software Development Processes
  booktitle: Proc. ICSSP'17
  year: 2017
  doi: 10.1145/3084100.3084116
  abstract: >
    Software developers often work on multiple projects and tasks throughout a
    work day, which may affect their productivity and quality of work. Knowing
    how working on several projects at a time affects productivity can improve
    cost and schedule estimations. It also can provide additional insights for
    better work scheduling and the development process. We want to achieve a
    better productivity without losing the benefits of work interruptions and
    multitasking for developers involved in the process. To understand how the
    development process can be improved, first, we identify work interruptions
    that mostly have a negative effect on productivity, second, we need to
    quantitatively evaluate impact of multitasking (task switching, work context
    switching) and work interruptions on productivity. In this research we study
    cross-project multitasking among the developers working on multiple projects
    in an educational setting. We propose a way to evaluate the number of
    cross-project interruptions among software developers using self-reported
    work logs. This paper describes the research that found: a) software
    developers involved in two or more projects on average spend 17% of their
    development effort on cross-project interruptions, b) the amount of effort
    spent on interruptions is overestimated by the G. Weinberg’s heuristic, c)
    the correlation between the number of projects and effort spent by
    developers on cross-project interruptions is relatively weak, and d) there
    is strong correlation between the number of projects and the number of
    interruptions developers reported.

- key: Treude2011
  kind: inproceedings
  author:
  - Christoph Treude
  - Margaret-Anne Storey
  title: Effective Communication of Software Development Knowledge Through Community Portals
  booktitle: FSE'11
  year: 2011
  doi: 10.1145/2025113.2025129

- key: Troy2018
  kind: link
  author:
  - Chelsea Troy
  title: Why do Remote Meetings Suck so Much?
  year: 2018
  url: https://chelseatroy.com/2018/03/29/why-do-remote-meetings-suck-so-much/

- key: Tudose2020
  kind: book
  author:
  - "Cătălin Tudose"
  title: JUnit in Action
  edition: 3rd
  year: 2020
  isbn: 978-1617297045
  publisher: Manning
  note: A guide to the most popular unit testing framework for Java.

- key: Tufekci2018
  kind: book
  author:
  - Zeynep Tufekci
  title: "Twitter and Tear Gas: The Power and Fragility of Networked Protest"
  year: 2018
  isbn: 978-0300234176
  publisher: Yale University Press

- key: Vigen2015
  kind: book
  author:
  - Tyler Vigen
  title: Spurious Correlations
  year: 2015
  isbn: 978-0316339438
  publisher: Hachette Books

- key: WachterBoettcher2017
  kind: book
  author:
  - Sara Wachter-Boettcher
  title: "Technically Wrong: Why Digital Products Are Designed to Fail You"
  year: 2017
  isbn: 978-0393634631
  publisher: WW Norton

- key: Walkinshaw2018
  kind: inproceedings
  author:
  - Neil Walkinshaw
  - Leandro Minku
  title: Are 20% of files responsible for 80% of defects?
  booktitle: Proc. ESEM'18
  year: 2018
  doi: 10.1145/3239235.3239244
  publisher: ACM Press
  url: https://doi.org/10.1145/3239235.3239244
  abstract: >
    Background: Over the past two decades a mixture of anecdote from the
    industry and empirical studies from academia have suggested that the 80:20
    rule (otherwise known as the Pareto Principle) applies to the relationship
    between source code files and the number of defects in the system: a small
    minority of files (roughly 20%) are responsible for a majority of defects
    (roughly 80%). Aims: This paper aims to establish how widespread the
    phenomenon is by analysing 100 systems (previous studies have focussed on
    between one and three systems), with the goal of whether and under what
    circumstances this relationship does hold, and whether the key files can be
    readily identified from basic metrics. Method: We devised a search criterion
    to identify defect fixes from commit messages and used this to analyse 100
    active Github repositories, spanning a variety of languages and domains. We
    then studied the relationship between files, basic metrics (churn and LOC),
    and defect fixes. Results: We found that the Pareto principle does hold, but
    only if defects that incur fixes to multiple files count as multiple
    defects. When we investigated multi-file fixes, we found that key files
    (belonging to the top 20%) are commonly fixed alongside other much less
    frequently-fixed files. We found LOC to be poorly correlated with defect
    proneness, Code Churn was a more reliable indicator, but only for extremely
    high values of Churn. Conclusions: It is difficult to reliably identify the
    "most fixed" 20% of files from basic metrics. However, even if they could
    be reliably predicted, focussing on them would probably be
    misguided. Although fixes will naturally involve files that are often
    involved in other fixes too, they also tend to include other less
    frequently-fixed files.

- key: Washburn2016
  kind: inproceedings
  author:
  - Michael Washburn
  - Pavithra Sathiyanarayanan
  - Meiyappan Nagappan
  - Thomas Zimmermann
  - Christian Bird
  title: "What Went Right and What Went Wrong: An Analysis of 155 Postmortems from Game Development"
  booktitle: Proc. ICSE'16
  year: 2016
  doi: 10.1145/2889160.2889253
  abstract: >
    In game development, software teams often conduct postmortems to reflect on
    what went well and what went wrong in a project. The postmortems are shared
    publicly on gaming sites or at developer conferences. In this paper, we
    present an analysis of 155 postmortems published on the gaming site
    Gamasutra.com. We identify characteristics of game development, link the
    characteristics to positive and negative experiences in the postmortems and
    distill a set of best practices and pitfalls for game development.

- key: Washington2020
  kind: inproceedings
  author:
  - Alicia Nicki Washington
  title: "When Twice as Good Isn''t Enough: The Case for Cultural Competence in Computing"
  booktitle: Proc. SIGCSE'20
  year: 2020
  doi: 10.1145/3328778.3366792
  abstract: >
    The commonly documented diversity, equity, and inclusion (DEI) issues in the
    computing workforce are the direct result of corporate cultures that benefit
    specific groups and marginalize others. This culture usually begins in
    undergraduate computing departments, where the demographic representation
    mirrors that of industry. With no formal courses that focus on the
    non-technical issues affecting marginalized groups and how to address and
    eradicate them, students are indirectly taught that the current status quo
    in computing departments and industry is not only acceptable, but also
    unproblematic. This directly affects students from marginalized groups (as
    the reasons for attrition are similar in both higher education and
    industry), as well as faculty (as biased student evaluations directly affect
    hiring, promotion, and tenure decisions). This position paper presents the
    need for cultural competence as a required focus for university computing
    departments nationwide. By improving these issues before students complete
    baccalaureate computing degrees, companies will have talent pools that
    better understand the importance and necessity of DEI and also work to
    ensure they help foster a more diverse, equitable, and inclusive
    environment. In addition, more students from marginalized groups will be
    retained in the major through degree completion.

- key: Wayne2018
  kind: book
  author:
  - Hillel Wayne
  title: "Practical TLA+: Planning Driven Development"
  year: 2018
  isbn: 978-1484238288
  publisher: Apress

- key: Wayner2009
  kind: book
  author:
  - Peter Wayner
  title: Translucent Databases
  edition: 2nd
  year: 2009
  isbn: 978-1441421340
  publisher: CreateSpace

- key: Weinstein2018
  kind: book
  author:
  - Yana Weinstein
  - Megan Sumeracki
  - Oliver Caviglioli
  title: "Understanding How We Learn: A Visual Guide"
  year: 2018
  isbn: 978-1138561724
  publisher: Routledge

- key: Wenger1999
  kind: book
  author:
  - Etienne Wenger
  title: "Communities of Practice: Learning, Meaning, and Identity"
  year: 1999
  isbn: 978-0521663632
  publisher: Cambridge University Press

- key: WeziakBialowolska2018
  kind: article
  author:
  - Dorota Węziak-Białowolska
  - Zhao Dong
  - Eileen McNeely
  title: "Turning the Mirror on the Architects: A Study of the Open-Plan Office and Work Behaviors at an Architectural Company"
  journal: Frontiers in Psychology
  volume: 9
  year: 2018
  doi: 10.3389/fpsyg.2018.02178

- key: Wilkie2018
  kind: inproceedings
  author:
  - John Wilkie
  - Ziad Al Halabi
  - Alperen Karaoglu
  - Jiafeng Liao
  - George Ndungu
  - Chaiyong Ragkhitwetsagul
  - Matheus Paixao
  - Jens Krinke
  title: Who's this?
  booktitle: Proc. MSR'18
  year: 2018
  doi: 10.1145/3196398.3196461
  publisher: ACM Press
  url: https://doi.org/10.1145/3196398.3196461
  abstract: >
    This paper presents a technique to identify a developer based on their IDE
    event data. We exploited the KaVE data set which recorded IDE activities
    from 85 developers with 11M events. We found that using an SVM with a linear
    kernel on raw event count outperformed k-NN in identifying developers with
    an accuracy of 0.52. Moreover, after setting the optimal number of events
    and sessions to train the classifier, we achieved a higher accuracy of 0.69
    and 0.71 respectively. The findings shows that we can identify developers
    based on their IDE event data. The technique can be expanded further to
    group similar developers for IDE feature recommendations.

- key: Wilkinson2011
  kind: book
  author:
  - Richard Wilkinson
  - Kate Pickett
  title: "The Spirit Level: Why Greater Equality Makes Societies Stronger"
  year: 2011
  isbn: 978-1608193417
  publisher: Bloomsbury Press

- key: Williams2001
  kind: inproceedings
  author:
  - Laurie Williams
  - Richard L. Upchurch
  title: In Support of Student Pair-Programming
  booktitle: Proc. SIGCSE'01
  year: 2001
  doi: 10.1145/364447.364614
  abstract: >
    Industry, particularly those following the eXtreme Programming (XP)
    methodology [2], has popularized the use of pair-programming. The
    pair-programming model has also been found to be beneficial for student
    programmers. Initial quantitative and qualitative results, which will be
    discussed in this paper, demonstrate that the use of pair-programming in the
    computer science classroom enhances student learning and satisfaction and
    reduces the frustration common among students. Additionally, the use of
    pair-programming relieves the burden on the educators because students no
    longer view the teaching staff as their sole form of technical
    information. We explore the nature of pair-programming, then examine the
    ways such a practice may enhance teaching and learning in computer science
    education.

- key: Wilson2019
  kind: book
  author:
  - Greg Wilson
  title: Teaching Tech Together
  year: 2019
  isbn: 978-0367353285
  publisher: Taylor & Francis
  url: http://teachtogether.tech

- key: Wlodkowski2017
  kind: book
  author:
  - Raymond J. Wlodkowski
  - Margery B. Ginsberg
  title: "Enhancing Adult Motivation to Learn: A Comprehensive Guide for Teaching All Adults"
  year: 2017
  isbn: 978-1119077992
  publisher: Jossey-Bass

- key: Xu2015
  kind: inproceedings
  author:
  - Tianyin Xu
  - Long Jin
  - Xuepeng Fan
  - Yuanyuan Zhou
  - Shankar Pasupathy
  - Rukma Talwadker
  title: "Hey, you have given me too many knobs!: understanding and dealing with over-designed configuration in system software"
  booktitle: Proc. FSE'15
  year: 2015
  doi: 10.1145/2786805.2786852
  publisher: ACM Press
  url: https://doi.org/10.1145/2786805.2786852
  abstract: >
    Configuration problems are not only prevalent, but also severely impair the
    reliability of today's system software. One fundamental reason is the
    ever-increasing complexity of configuration, reflected by the large number
    of configuration parameters ("knobs"). With hundreds of knobs, configuring
    system software to ensure high reliability and performance becomes a
    daunting, error-prone task. This paper makes a first step in understanding a
    fundamental question of configuration design: "do users really need so many
    knobs?" To provide the quantitatively answer, we study the configuration
    settings of real-world users, including thousands of customers of a
    commercial storage system (Storage-A), and hundreds of users of two
    widely-used open-source system software projects. Our study reveals a series
    of interesting findings to motivate software architects and developers to be
    more cautious and disciplined in configuration design. Motivated by these
    findings, we provide a few concrete, practical guidelines which can
    significantly reduce the configuration space. Take Storage-A as an example,
    the guidelines can remove 51.9% of its parameters and simplify 19.7% of the
    remaining ones with little impact on existing users. Also, we study the
    existing configuration navigation methods in the context of "too many
    knobs" to understand their effectiveness in dealing with the over-designed
    configuration, and to provide practices for building navigation support in
    system software.

- key: Yin2011
  kind: inproceedings
  author:
  - Zuoning Yin
  - Ding Yuan
  - Yuanyuan Zhou
  - Shankar Pasupathy
  - Lakshmi Bairavasundaram
  title: How do Fixes Become Bugs?
  booktitle: Proc. SIGSOFT/FSE'11
  year: 2011
  doi: 10.1145/2025113.2025121

- key: Yuan2014
  kind: inproceedings
  author:
  - Ding Yuan
  - Yu Luo
  - Xin Zhuang
  - Guilherme Renna Rodrigues
  - Xu Zhao
  - Yongle Zhang
  - Pranay U. Jain
  - Michael Stumm
  title: "Simple testing can prevent most critical failures: an analysis of production failures in distributed data-intensive systems"
  booktitle: Proc. OSDI'14
  month: '10'
  year: 2014
  doi: 10.13140/2.1.2044.2889
  url: https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf

- key: Yuan2015
  kind: article
  author:
  - Ding Yuan
  - Yu Luo
  - Xin Zhuang
  - Guilherme Renna Rodrigues
  - Xu Zhao
  - Yongle Zhang
  - Pranay Jain
  - Michael Stumm
  title: "Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems"
  journal: login Usenix Magazine
  volume: 40
  number: 1
  year: 2015
  url: https://www.usenix.org/publications/login/feb15/yuan

- key: Zampetti2020
  kind: article
  author:
  - Fiorella Zampetti
  - Carmine Vassallo
  - Sebastiano Panichella
  - Gerardo Canfora
  - Harald Gall
  - Massimiliano Di Penta
  title: An Empirical Characterization of Bad Practices in Continuous Integration
  journal: ESE
  volume: 25
  number: 2
  year: 2020
  doi: 10.1007/s10664-019-09785-8
  abstract: >
    Continuous Integration (CI) has been claimed to introduce several benefits
    in software development, including high software quality and
    reliability. However, recent work pointed out challenges, barriers and bad
    practices characterizing its adoption. This paper empirically investigates
    what are the bad practices experienced by developers applying CI. The
    investigation has been conducted by leveraging semi-structured interviews of
    13 experts and mining more than 2,300 Stack Overflow posts. As a result, we
    compiled a catalog of 79 CI bad smells belonging to 7 categories related to
    different dimensions of a CI pipeline management and process. We have also
    investigated the perceived importance of the identified bad smells through a
    survey involving 26 professional developers, and discussed how the results
    of our study relate to existing knowledge about CI bad practices. Whilst
    some results, such as the poor usage of branches, confirm existing
    literature, the study also highlights uncovered bad practices, e.g., related
    to static analysis tools or the abuse of shell scripts, and contradict
    knowledge from existing literature, e.g., about avoiding nightly builds. We
    discuss the implications of our catalog of CI bad smells for (i)
    practitioners, e.g., favor specific, portable tools over hacking, and do not
    ignore nor hide build failures, (ii) educators, e.g., teach CI culture, not
    just technology, and teach CI by providing examples of what not to do, and
    (iii) researchers, e.g., developing support for failure analysis, as well as
    automated CI bad smell detectors.

- key: Zeller2009
  kind: book
  author:
  - Andreas Zeller
  title: "Why Programs Fail: A Guide to Systematic Debugging"
  edition: 2nd
  year: 2009
  isbn: 978-0080923000
  publisher: Morgan Kaufmann

- key: Zeller2011
  kind: inproceedings
  author:
  - Andreas Zeller
  - Thomas Zimmermann
  - Christian Bird
  title: Failure is a four-letter word
  booktitle: Proc. PROMISE'11
  year: 2011
  doi: 10.1145/2020390.2020395
  publisher: ACM Press
  url: https://doi.org/10.1145/2020390.2020395
  abstract: >
    Background: The past years have seen a surge of techniques predicting
    failure-prone locations based on more or less complex metrics. Few of these
    metrics are actionable, though. Aims: This paper explores a simple,
    easy-to-implement method to predict and avoid failures in software
    systems. The IROP method links elementary source code features to known
    software failures in a lightweight, easy-to-implement fashion. Method: We
    sampled the Eclipse data set mapping defects to files in three Eclipse
    releases. We used logistic regression to associate programmer actions with
    defects, tested the predictive power of the resulting classifier in terms of
    precision and recall, and isolated the most defect-prone actions. We also
    collected initial feedback on possible remedies. Results: In our sample
    set, IROP correctly predicted up to 74% of the failure-prone modules, which
    is on par with the most elaborate predictors available. We isolated a set of
    four easy-to-remember recommendations, telling programmers precisely what to
    do to avoid errors. Initial feedback from developers suggests that these
    recommendations are straightforward to follow in practice. Conclusions:
    With the abundance of software development data, even the simplest methods
    can produce "actionable" results.

- key: Zeller2019
  kind: link
  author:
  - Andreas Zeller
  - Rahul Gopinath
  - "Marcel Böhme"
  - Gordon Fraser
  - Christian Holler
  title: The Fuzzing Book
  year: 2019
  url: https://www.fuzzingbook.org/

- key: Zeller2021
  kind: link
  author:
  - Andreas Zeller
  title: The Debugging Book
  year: 2021
  url: https://www.debuggingbook.org/

- key: Zhan2020
  kind: article
  author:
  - Letian Zhang
  title: An Institutional Approach to Gender Diversity and Firm Performance
  journal: Organization Science
  volume: 31
  number: 2
  year: 2020
  doi: 10.1287/orsc.2019.1297
  abstract: >
    This study examines data from 35 countries and 24 industries to understand
    the relationship between gender diversity and firm performance. Previous
    studies report conflicting evidence: some find th...

- key: Zimmermann2007
  kind: inproceedings
  author:
  - Thomas Zimmermann
  - Rahul Premraj
  - Andreas Zeller
  title: Predicting Defects for Eclipse
  booktitle: Proc. PROMISE'07
  month: '5'
  year: 2007
  doi: 10.1109/promise.2007.10
  publisher: IEEE
  url: https://doi.org/10.1109/promise.2007.10
  abstract: >
    We have mapped defects from the bug database of eclipse (one of the largest
    open-source projects) to source code locations. The resulting data set lists
    the number of pre- and post-release defects for every package and file in
    the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data
    with common complexity metrics. All data is publicly available and can serve
    as a benchmark for defect prediction models.

- key: Zou2019
  kind: article
  author:
  - Weiqin Zou
  - Jifeng Xuan
  - Xiaoyuan Xie
  - Zhenyu Chen
  - Baowen Xu
  title: How does code style inconsistency affect pull request integration? An exploratory study on 117 GitHub projects
  journal: ESE
  volume: 24
  number: 6
  pages: 3871--3903
  month: '6'
  year: 2019
  doi: 10.1007/s10664-019-09720-x
  publisher: Springer Science and Business Media LLC
  url: https://doi.org/10.1007/s10664-019-09720-x
